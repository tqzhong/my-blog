[{"content":"PPO PPO（Proximal Policy Optimization）算法出自Schulman et al.，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：\n$$ \\mathcal J_{PPO}(\\theta)=\\mathbb E_{[q\\sim P(Q),o\\sim \\pi_{\\theta_{old}}(O\\vert q)]}\\frac{1}{\\vert o\\vert}\\sum_{t=1}^{\\vert o\\vert}\\min\\left[\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t\\vert q,o_{\u003c t})}A_t,\\text{clip}\\left(\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t\\vert q,o_{\u003c t})},1-\\epsilon,1+\\epsilon\\right)A_t\\right] $$ 其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：\n$$ r_t=r_\\phi(q,o_{1:\\vert o\\vert}) - \\beta \\log\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{ref}(o_t\\vert q,o_{\u003c t})} $$ $$ A_t=\\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2}+\\cdots=\\sum_{l=0}^\\infty (\\gamma\\lambda)^l\\delta_{t+l} $$ $$ \\delta_t=r_t+\\gamma V(s_{t+1}) - V(s_t) $$ 针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\\vert o\\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\\vert o\\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\\phi(q,o_{1:\\vert o\\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\\log\\frac{\\pi_\\theta(\\cdot)}{\\pi_{ref}(\\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 def get_advantages_and_returns(self, values, rewards): lastgaelam = 0 advantages_reversed = [] max_seq_len = rewards.shape[-1] for t in reversed(range(max_seq_len)): nextvalues = values[:, t + 1] if t \u0026lt; max_seq_len - 1 else 0.0 delta = rewards[:, t] + self.gamma * nextvalues - values[:, t] lastgaelam = delta + self.gamma * self.lam * lastgaelam advantages_reversed.append(lastgaelam) advantages = torch.stack(advantages_reversed[::-1], dim=1) returns = advantages + values return advantages, returns 经过一次for循环得到的分别是（令max_seq_len为$\\vert o\\vert$）：\n$$ A_{t=\\vert o\\vert - 1}=\\delta_{\\vert o\\vert - 1}\\\\\\\\ A_{t=\\vert o\\vert - 2}=(\\gamma\\lambda)\\delta_{\\vert o\\vert - 1} + \\delta_{\\vert o\\vert -2}\\\\\\\\ A_{t=\\vert o\\vert -3}=(\\gamma\\lambda)^2\\delta_{\\vert o\\vert - 1} + (\\gamma\\lambda)\\delta_{\\vert o\\vert -2} + \\delta_{\\vert o\\vert - 3}\\\\\\\\ \\cdots\\\\\\\\ A_{t=0}=(\\gamma\\lambda)^{\\vert o\\vert -1}\\delta_{\\vert o\\vert - 1} + (\\gamma\\lambda)^{\\vert o\\vert -2}\\delta_{\\vert o\\vert -2} + \\cdots + (\\gamma\\lambda)\\delta_{1} + \\delta_0 $$ 经过翻转后，得到优势向量$A_t=[A_{t=0}, A_{t=1},\\cdots, A_{t=\\vert o\\vert - 1}]$，向量维度为(bs, max_seq_len)\nGRPO GRPO（Group Relative Policy Optimization）算法出自Shao et al.，其优化目标如下：\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right] - \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\theta} \\| \\pi_{\\text{ref}}] \\right\\}\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G\\frac{1}{\\vert o_i\\vert}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\} \\end{align*} $$ $$ D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - 1, $$ $$ \\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}. $$ DAPO Yu et al.提出了DAPO（Decouple Clip and Dynamic sAmpling Policy Optimization）算法，该算法基于GRPO算法提出了四点改进，其优化目标如下：\n$$ \\begin{align*} \\mathcal J_{DAPO}(\\theta)\u0026=\\mathbb E_{(q,a)\\sim\\mathcal D,\\{o_i\\}_{i=1}^G\\sim\\pi_{\\theta_{old}}(\\cdot\\vert q)}\\left\\{\\frac{1}{\\sum_{i=1}^G\\vert o_i\\vert}\\sum_{i=1}^G\\sum_{t=1}^{\\vert o_i\\vert}\\min\\left[r_{i,t}(\\theta)\\hat A_{i,t},\\text{clip}\\left(r_{i,t}(\\theta),1-\\epsilon_{\\text{low}},1+\\epsilon_{\\text{high}}\\right)\\hat A_{i,t}\\right]\\right\\}\\\\ % \u0026 \\text{s.t.}\\quad 0 \u003c \\vert \\{o_i\\vert \\text{is\\_equivalent}\\} \u0026 \\text{s.t.}\\quad 0 \u003c \\left\\vert \\{o_i\\ \\vert\\ \\text{is\\_equivalent}(a, o_i)\\}\\right\\vert \u003c G, \\end{align*} $$ $$ r_{i,t}(\\theta)=\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},\\quad \\hat A_{i,t}=\\frac{R_i-\\text{mean}(\\{R_i\\}_{i=1}^G)}{\\text{std}(\\{R_i\\}_{i=1}^G)} $$ 首先作者移除了GRPO算法中的KL散度惩罚，作者认为对于训练long-CoT推理模型，actor model的输出分布与ref model的输出分布自然存在较大差异，没有必要设置KL散度限制。其次对于DAPO，作者采用基于规则的奖励模型，对于可验证任务（automated throrem proving、computer programming、mathematics competition），作者使用如下奖励函数，其中$\\hat y$是预测答案，$y$是标准答案。\n$$ R(\\hat y, y)= \\begin{cases} 1,\u0026 \\text{is\\_equivalent}(\\hat y, y)\\\\ -1,\u0026 \\text{otherwise} \\end{cases} $$ Insights 接着，作者针对GRPO的理论缺陷提出了四点比较有意思的insights，每个insight对原本算法的改动都很小，但存在一定效果的提升。\nClip-Higher TLDR：将原先的clip函数的上下界单独设置，而不是统一设置。\nMotivation：对于生成的sentences，其中大部分token的概率值都较低，因此使用一个较低的$\\epsilon$（一般算法设置$\\epsilon=0.2$）对于这些低概率token的提升非常有限，比如$\\pi_{\\theta_{old}}(o_i\\vert q)=0.01$，当$\\epsilon=0.2$时，$\\pi_{\\theta}(o_i\\vert q)$最大值只能为0.012。简单来说就是大部分token的概率值均偏低（\u0026lt; 0.2），而低概率值的token更容易被clip（原因如上所述），作者认为这限制了模型对低概率token的提升，从而限制了整个模型的生成多样性。作者论文中实验设置了$\\epsilon_{\\text{low}}=0.2$，$\\epsilon_{\\text{high}}=0.28$。\nDynamic Sampling TLDR：让每批采样的answer不能全部正确也不能全部错误。\nMotivation：当每批采样的answer全部正确或全部错误时，计算得到的优势$\\hat A_{i,t}=0$，这导致梯度值为零，那导致模型在这一步上等价于没有学习，降低了采样效率。因此作者在每个step会多次采样（理解为对同一批prompt生成answer），直到answer的平均准确率介于0和1之间。\nToken-Level Policy Gradient Loss TLDR：对一批生成样本中的每个token采用相同的损失贡献比例，而不是每个样本各自先按长度归一化损失再平均每个样本的损失。\nMotivation：作者认为GRPO中的损失归一方式对long-CoT RL场景不友好，在GRPO中，每批样本中长样本的每个token贡献的损失比重会低于短样本的每个token贡献的损失比重，这会导致两个问题：1）对于高质量的长样本，这会阻碍模型学习这类样本的推理模式，2）对于低质量的长样本（出现重复，垃圾话），样本层级的损失计算也无法有效对这些样本进行惩罚。\nOverlong Reward Shaping TLDR：设置一个最大生成长度，对超出长度的样本进行惩罚。\nMotivation：传统RL训练，对于过长样本会直接截断，但这种直接截断会带来噪音影响训练过程，因为一个合理但过长的样本被截断显然会影响模型训练。作者首先尝试将每批数据中被截断的损失mask掉，发现这会提升训练稳定性且提升模型性能。进一步地，作者设计了SoftOverlongPunishment，计算方式如下。这个惩罚性奖励被添加到原始基于规则的正确性奖励中一起计算总奖励。\n$$ R_{\\text{length}}(y)= \\begin{cases} 0,\u0026 \\vert y\\vert \\le L_{\\text{max}} - L_{\\text{cache}} \\\\\\\\ \\frac{(L_{\\text{max}}-L_{\\text{cache}})-\\vert y\\vert}{L_{\\text{cache}}},\u0026 L_{\\text{max}}-L_{\\text{cache}} \u003c \\vert y\\vert\\le L_{\\text{max}}\\\\\\\\ -1,\u0026 L_{\\text{max}} \u003c \\vert y\\vert \\end{cases} $$ Dr. GRPO Liu et al.提出Dr. GRPO，该工作指出GRPO算法存在的一些bias，并且这些bias可能导致了GRPO算法随着训练步数增加，生成answer长度不断增加的现象（包括出现aha moment）。该工作提出了两个bias：\nResponse-level length bias：源于对损失除了$\\vert o_i\\vert$，这样，对于正优势（$\\hat A_{i,t}\u0026gt; 0$，correct response），该偏差使得较短的response的梯度更大（$\\vert o_i\\vert$更小），从而导致策略倾向更简洁的正确回答。相反，对于负优势（$\\hat A_{i,t}\u0026lt; 0$，incorrect response），该偏差使得较长的response的梯度更大（$\\vert o_i\\vert$更大），从而导致策略倾向更复杂的错误回答。\nQuestion-level difficulty bias：源于计算优势时对奖励偏差除了$\\text{std}(\\lbrace r_1,\\cdots,r_G\\rbrace)$。因此对于某个特定question，如果其answers容易得到较低的variance，那么这批answer的梯度会更大。通常来说优势归一化在常规RL算法中是在一整个batch上进行，而GRPO在每个question上进行归一化，导致对于不同question的answer，其梯度值可能会有较大差异。\n对此，作者移除了$\\frac{1}{\\vert o_i\\vert}$和$\\text{std}(\\lbrace r_1,\\cdots,r_G\\rbrace)$，并将$\\vert o_i\\vert$替换为一个固定值。\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G\\textcolor{red}{\\frac{1}{\\vert o_i\\vert}}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\} \\end{align*} $$ $$ \\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\textcolor{red}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}}. $$ Insights 作者对比了GRPO与Dr. GRPO，发现随着训练进行，Dr. GRPO的平均生成长度不会一直增加而是收敛。两者回答正确的answer长度均收敛，但不正确的answer长度中，GRPO不断增加而Dr. GRPO收敛甚至有所下降。两者最终性能表现相当。这证明了Dr. GRPO有更高的token efficiency。 图1：GRPO vs. Dr. GRPO Skywork Open Reasoner Series Blog链接：He et al.\n7B数学模型在AIME24上取得69.8%准确率（avg@8）\n图2：Skywork-OR1-Math-7B Performance on AIME24（avg@8） Data Preparation Multi-stage GRPO with Adaptive Entropy Control 优化目标函数如下：\n$$ \\mathcal J(\\theta)=\\frac{1}{T_k}\\sum_{i\\in\\mathcal T_k}\\sum_{j=1}^M\\left\\{\\sum_{t=0}^{\\vert y_{ij}-1\\vert}\\min\\{\\rho_t^{ij}(\\theta)\\hat A_{ij},\\text{clip }(\\rho_t^{ij}(\\theta),1-\\epsilon,1+\\epsilon)\\hat A_{ij}\\}-\\alpha_k\\mathbb H_t^{ij}(\\theta)\\right\\} $$ $$ \\hat A_{ij}=\\frac{r_{ij}-\\text{mean }(\\textbf{r}_i)}{\\text{std }(\\textbf{r}_i)} $$ $$ \\rho_{t}^{ij}(\\theta)=\\frac{\\pi_\\theta(a_t^{(ij)}\\vert s_t^{(ij)})}{\\pi_{\\theta_{k}}(a_t^{(ij)}\\vert s_t^{(ij)})} $$ $$ s_t^{(ij)}=(x_i,a_0^{(ij)},\\cdots,a_{t-1}^{(ij)}) $$ $$ T_k=\\sum_{i\\in\\mathcal T_k}\\sum_{j=1}^M\\vert y_{ij}\\vert $$ $$ \\mathbb H_{t}^{ij}(\\theta)=\\mathcal H(\\pi_\\theta(\\cdot\\vert s_t^{(ij)}))=-\\sum_{a_t^{(ij)}\\in\\mathcal V}\\pi_\\theta(a_t^{(ij)}\\vert s_t^{(ij)})\\log\\pi_\\theta(a_t^{(ij)}\\vert s_t^{(ij)}) $$ 相较于GRPO的主要改动：\n去除KL散度惩罚，这与DAPO，Dr. GRPO中的做法相一致。 增加生成熵的约束，防止模型熵爆炸。 归一化采用batch内所有数据归一化，即最后损失项乘了 $\\frac{1}{\\sum_{i\\in\\mathcal T_k}\\sum_{j=1}^M}$，其中$\\mathcal T_k$为当前batch中所有prompt的集合，$M$为每个prompt的生成answer数量。 此外在训练数据处理上，作者针对long cot模型的训练加入了以下优化：\nOffline \u0026amp; online filtering：训练前，使用base model对每条prompt生成一批answer（M条），滤除answer完全正确或者完全错误的prompt；训练时，在每个epoch开始前，用上一轮epoch结束后的actor model对训练prompt生成一批answer（M条），滤除answer完全正确的prompt。 Rejection Sampling：每个训练step时，当前batch的所有prompt $x_i$，模型生成得到的结果并计算每条样本的优势$\\hat A_{ij}$，要求每个prompt对应的所有优势中，至少有一个优势不为0，否则将滤除（理解是这样），公式表达如下： $$ \\mathcal T_k:=\\left\\{i\\in[N]:\\exists\\ j\\in[M]\\quad\\hat A_{ij}\\neq 0 \\right\\} $$ Multi-Stage Training 作者训练分为3个阶段，逐渐增大最大生成长度，主要用来减少训练时间，同时保证最终模型的性能。当前一个阶段性能收敛时进入下一个阶段（但感觉得通过实验来选取经验值，如第一阶段训练多少个step这种）。\nStage1 Stage2 Stage3 8K 16K 32K On the Issue of Truncated Samples 作者针对最大生成长度限制内，出现生成过长导致截断的问题，提出了Advantage Mask For Truncated方法，并给出了两种方案：\nAdv-Mask-Before: $$ \\hat A_{ij}= \\begin{cases} \\frac{r(x_i,y_{ij})-\\text{mean}(\\hat {\\mathbb R}_i)}{\\text{std}(\\hat{\\mathbb R}_i)}\u0026 \\vert y_{ij}\\vert \\le T_{\\text{max}}\\\\\\\\ 0,\u0026 \\vert y_{ij}\\vert \u003e T_{\\text{max}} \\end{cases} $$ 其中$\\hat{\\mathbb R}_i$为没被阶段的answer的奖励集合。 Adv-Mask-After: $$ \\hat A_{ij}= \\begin{cases} \\frac{r(x_i,y_{ij})-\\text{mean}({\\mathbb R}_i)}{\\text{std}({\\mathbb R}_i)}\u0026 \\vert y_{ij}\\vert \\le T_{\\text{max}}\\\\\\\\ 0,\u0026 \\vert y_{ij}\\vert \u003e T_{\\text{max}} \\end{cases} $$ 其中$\\mathbb R_i$为所有answer的奖励集合。 实验结果发现两个mask效果都不如不加mask，因此作者没有使用这两种mask。\nAdaptive Entropy Control 作者发现生成熵损失对超参数$\\alpha_k$和训练数据分布都非常敏感，因此提出adaptive entropy control的方法。具体来说，作者设置一个熵阈值tgt-ent（即想要模型保持的生成熵水平）以及一个变化量$\\vartriangle$。设置$\\alpha_k$初始值为0，每个step前，用当前actor模型计算当前batch的平均生成熵e，如果e小于tgt-ent，那么增加$\\alpha_k: \\alpha_k=\\alpha_k+\\vartriangle$。考虑到增加熵损失会带来训练不稳定，因此当e大于tgt-ent时，该step不会启用熵损失，即理解为$\\alpha_k=0$。实验中，作者设置tgt-ent=0.2，$\\vartriangle$=0.005。\n图3：Adaptive entropy control, tgt-ent=0.2, $\\vartriangle$=0.005 这部分的大概作用就是，不设置生成熵损失或者超参数$\\alpha_k$比较小的时候，在较少的训练step后，模型的生成熵便会降到很低接近0的值，这对模型的探索能力起到负面影响，同时作者发现，当$\\alpha_k$设置较大时（\u0026gt; 1e-3），在较少训练step后，模型的生成熵就爆炸了，因此可以理解，提高$\\alpha_k$会在一定程度上扰乱模型的生成熵防止其快速收敛。训练的最终目的是让模型的生成熵稳定在一个较低水平但不是接近0的值，因此这种Adaptive Entropy Control方法可以很好地解决这个问题。当前估计的熵值e较小则提高$\\alpha_k$，当前估计的熵值e较大则不启用熵损失，让模型自然训练降低生成熵即可。\nReferences [1] Yu et al. “DAPO: An Open-Source LLM Reinforcement Learning System at Scale” arXiv preprint arXiv:2503.14476 (2025).\n[2] Shao et al. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models” arXiv preprint arXiv:2402.03300 (2024).\n[3] Schulman et al. “Proximal Policy Optimization Algorithms” arXiv preprint arXiv:1707.06347 (2017).\n[4] Liu et al. “Understanding R1-Zero-Like Training: A Critical Perspective” Github 2025.\n[5] He et al. “Skywork Open Reasoner Series” Notion Blog 2025.\n","permalink":"https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/","summary":"\u003ch3 id=\"ppo\"\u003ePPO\u003c/h3\u003e\n\u003c!-- #### Algorithm --\u003e\n\u003cp\u003ePPO（Proximal Policy Optimization）算法出自\u003ca href=\"https://arxiv.org/abs/1707.06347\" class=\"entityLink\"\u003eSchulman et al.\u003c/a\u003e，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\mathcal J_{PPO}(\\theta)=\\mathbb E_{[q\\sim P(Q),o\\sim \\pi_{\\theta_{old}}(O\\vert q)]}\\frac{1}{\\vert o\\vert}\\sum_{t=1}^{\\vert o\\vert}\\min\\left[\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t\\vert q,o_{\u003c t})}A_t,\\text{clip}\\left(\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{\\theta_{old}}(o_t\\vert q,o_{\u003c t})},1-\\epsilon,1+\\epsilon\\right)A_t\\right]\n$$\n\u003c/div\u003e\n\u003cp\u003e其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nr_t=r_\\phi(q,o_{1:\\vert o\\vert}) - \\beta \\log\\frac{\\pi_\\theta(o_t\\vert q,o_{\u003c t})}{\\pi_{ref}(o_t\\vert q,o_{\u003c t})}\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nA_t=\\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2}+\\cdots=\\sum_{l=0}^\\infty (\\gamma\\lambda)^l\\delta_{t+l}\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\delta_t=r_t+\\gamma V(s_{t+1}) - V(s_t)\n$$\n\u003c/div\u003e\n\u003cp\u003e针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\\vert o\\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\\vert o\\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\\phi(q,o_{1:\\vert o\\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\\log\\frac{\\pi_\\theta(\\cdot)}{\\pi_{ref}(\\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eget_advantages_and_returns\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003emax_seq_len\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003ereversed\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emax_seq_len\u003c/span\u003e\u003cspan class=\"p\"\u003e)):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003enextvalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003emax_seq_len\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erewards\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egamma\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003enextvalues\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egamma\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elam\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elastgaelam\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estack\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadvantages_reversed\u003c/span\u003e\u003cspan class=\"p\"\u003e[::\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eadvantages\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e经过一次for循环得到的分别是（令max_seq_len为$\\vert o\\vert$）：\u003c/p\u003e","title":"大模型post-training方法——强化学习篇"},{"content":"简介 本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。\nGRPO GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right] - \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\theta} \\| \\pi_{\\text{ref}}] \\right\\}\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G\\frac{1}{\\vert o_i\\vert}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\} \\end{align*} $$ $$ D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - 1, $$ $$ \\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}. $$ GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\\mathbb D_{KL}(q\\Vert p)=r-1-\\log r$，其中$r=\\log\\frac{p(x)}{q(x)}$，该估计相比$-\\log r$具有更小的方差，比$\\frac{1}{2}(\\log r)^2$具有更小的偏差（无偏）。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 import random import copy import re import os import numpy as np import wandb import torch import pdb import torch.nn as nn from torch.nn.utils.rnn import pad_sequence from transformers import AutoModelForCausalLM, AutoTokenizer from datasets import load_dataset, load_from_disk from tqdm import tqdm def set_random_seed(seed: int=42): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False set_random_seed(42) os.environ[\u0026#39;WANDB_API_KEY\u0026#39;] = \u0026#34;YOUR_API_KEY\u0026#34; os.environ[\u0026#39;WANDB_PROJECT\u0026#39;] = \u0026#34;GRPO-Qwen-2.5-1.5B-Instruct\u0026#34; # 设置系统prompt SYSTEM_PROMPT = \u0026#34;\u0026#34;\u0026#34; Respond in the following format: \u0026lt;reasoning\u0026gt; ... \u0026lt;/reasoning\u0026gt; \u0026lt;answer\u0026gt; ... \u0026lt;/answer\u0026gt; \u0026#34;\u0026#34;\u0026#34; def extract_answer_from_model_output(text): \u0026#34;\u0026#34;\u0026#34; Extracts the value from the last \u0026lt;answer\u0026gt; tag in the text. Args: text (str): The model-generated text containing XML-style \u0026lt;answer\u0026gt; tags. Returns: str or None: The content inside the \u0026lt;answer\u0026gt; tags, or None if no valid answer is found. Explanation: 1. Splits the text on the \u0026lt;answer\u0026gt; tag to isolate content after the tag. 2. Checks if at least one \u0026lt;answer\u0026gt; tag exists in the text. 3. For the last \u0026lt;answer\u0026gt; segment: - Verifies it contains a closing \u0026lt;/answer\u0026gt; tag. - Extracts only the content between the tags. 4. Returns None if the answer is empty (just \u0026#34;...\u0026#34;) or if tags are missing. \u0026#34;\u0026#34;\u0026#34; parts = text.split(\u0026#39;\u0026lt;answer\u0026gt;\u0026#39;) if len(parts) \u0026lt; 2: # No \u0026lt;answer\u0026gt; tag found return None last_part = parts[-1] if \u0026#39;\u0026lt;/answer\u0026gt;\u0026#39; not in last_part: return None answer = last_part.split(\u0026#39;\u0026lt;/answer\u0026gt;\u0026#39;)[0].strip() return None if answer == \u0026#34;...\u0026#34; else answer def extract_answer_from_dataset(text): \u0026#34;\u0026#34;\u0026#34; Extracts the answer from the GSM8K dataset examples. Args: text (str): The dataset example text containing a question and answer. Returns: str or None: The extracted answer part after the \u0026#39;####\u0026#39; delimiter, or None if not found. Explanation: 1. Checks if the text contains the \u0026#39;####\u0026#39; delimiter that separates question from answer. 2. If found, splits the text at this delimiter and returns the second part (the answer). 3. The answer is stripped of leading/trailing whitespace. 4. Returns None if no delimiter is present. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;####\u0026#34; not in text: return None return text.split(\u0026#34;####\u0026#34;)[1].strip() def prepare_dataset(split=\u0026#34;train\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Load and prepare the GSM8K dataset for training with string prompts. Args: split (str): The dataset split to load (\u0026#34;train\u0026#34; or \u0026#34;test\u0026#34;). Defaults to \u0026#34;train\u0026#34;. Returns: list: A list of formatted examples, each containing a prompt string and answer. Explanation: 1. Loads the GSM8K dataset from the Hugging Face datasets hub. 2. For each example in the dataset: - Creates a list of messages with system prompt and the question. - Converts this list into a single string prompt using build_prompt(). - Extracts the answer from the dataset example. - Creates a formatted example dictionary with prompt and answer. 3. Returns the list of formatted examples ready for model training or evaluation. \u0026#34;\u0026#34;\u0026#34; # 从本地加载，服务器端连接不上huggingface，使用train部分的数据 data = load_from_disk(\u0026#39;/data/ztq147/gsm8k\u0026#39;)[split] # data = load_dataset(\u0026#39;openai/gsm8k\u0026#39;, \u0026#39;main\u0026#39;)[split] # 一个formatted数据包含“prompt”和“answer”，其中“prompt”格式为SYSTEM_PROMPT\\n QUESTION；“answer”格式为ANSWER formatted_data = [] for example in data: prompt_str = build_prompt( [ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: SYSTEM_PROMPT}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: example[\u0026#39;question\u0026#39;]} ] ) formatted_example = { \u0026#34;prompt\u0026#34;: prompt_str, \u0026#34;answer\u0026#34;: extract_answer_from_dataset(example[\u0026#34;answer\u0026#34;]) } formatted_data.append(formatted_example) return formatted_data def build_prompt(messages): \u0026#34;\u0026#34;\u0026#34; Build a single prompt string from a list of messages. Args: messages (list): A list of message dictionaries, each with \u0026#39;role\u0026#39; and \u0026#39;content\u0026#39; keys. Returns: str: A concatenated string of all message contents. Explanation: 1. Takes a list of message dictionaries in the typical chat format. 2. Extracts the \u0026#39;content\u0026#39; field from each message and strips whitespace. 3. Joins all content strings with newlines to create a single prompt. 4. This preserves the training format while converting from structured messages to a string. \u0026#34;\u0026#34;\u0026#34; return \u0026#34;\\n\u0026#34;.join([msg[\u0026#34;content\u0026#34;].strip() for msg in messages]) def extract_last_number(text): \u0026#34;\u0026#34;\u0026#34; Extracts the last number appearing in the text. Args: text (str): The text to extract a number from. Returns: float or None: The last number in the text, or None if no number is found. Explanation: 1. Removes dollar signs and percent symbols from the text. 2. Uses regex to find a number that appears at the end of the text (possibly after whitespace). 3. The pattern matches numbers that appear at the end of the string, with or without decimal points. 4. Returns the found number as a float, or None if no match is found. \u0026#34;\u0026#34;\u0026#34; text = text.replace(\u0026#39;$\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;) pattern = r\u0026#39;(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$\u0026#39; match = re.search(pattern, text) return float(match.group(1)) if match else None def extract_single_number(text): \u0026#34;\u0026#34;\u0026#34; Extracts the last number appearing in the text. Args: text (str): The text to extract a number from. Returns: float or None: The last number in the text, or None if no number is found. Explanation: 1. Removes dollar signs and percent symbols from the text. 2. Uses regex to find a number that appears at the end of the text (possibly after whitespace). 3. The pattern matches numbers that appear at the end of the string, with or without decimal points. 4. Returns the found number as a float, or None if no match is found. \u0026#34;\u0026#34;\u0026#34; text = text.replace(\u0026#39;$\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;) pattern = r\u0026#39;(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$\u0026#39; match = re.search(pattern, text) return float(match.group(1)) if match else None def evaluate_model(model, tokenizer, eval_examples, device): \u0026#34;\u0026#34;\u0026#34; Evaluates the model on a set of examples and prints detailed results. Args: model: The language model to evaluate. tokenizer: The tokenizer for encoding inputs and decoding outputs. eval_examples (list): List of evaluation examples, each containing \u0026#34;prompt\u0026#34; and \u0026#34;answer\u0026#34;. device: The device (CPU or GPU) to run evaluation on. Returns: float: The accuracy percentage (correct predictions / total examples * 100). Explanation: 1. Sets the model to evaluation mode. 2. For each example in the evaluation set: - Encodes the prompt and generates a response using the model. - Extracts the predicted answer from the generated response. - Compares the predicted answer with the expected answer using multiple methods: a. Exact string matching b. Single number extraction and comparison c. Last number extraction and comparison - Prints detailed information about each example. 3. Calculates and returns the overall accuracy. 4. Returns the model to training mode. \u0026#34;\u0026#34;\u0026#34; model.eval() correct = 0 total = len(eval_examples) print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50) print(\u0026#34;EVALUATION ON\u0026#34;, total, \u0026#34;EXAMPLES\u0026#34;) print(\u0026#34;=\u0026#34;*50) for example in eval_examples: full_prompt = example[\u0026#34;prompt\u0026#34;] expected = example[\u0026#34;answer\u0026#34;] inputs = tokenizer.encode(full_prompt, return_tensors=\u0026#34;pt\u0026#34;).to(device) # early_stopping=False，表示模型会一直生直到达到最大新标记数（max_new_tokens）。 # forced_eos_token_id=tokenizer.eos_token_id，当生成到最大长度时，默认将最后一个token换成eos_token_id。 with torch.no_grad(): outputs = model.generate( inputs, max_new_tokens=512, temperature=0.7, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, forced_eos_token_id=tokenizer.eos_token_id, early_stopping=False, ) response = tokenizer.decode(outputs[0], skip_special_tokens=True) # 关于数学类问题评估的方法，这里不太了解，之前没做过 try: predicted = extract_answer_from_model_output(response) if predicted == expected: is_correct = True else: pred_num = extract_single_number(str(predicted)) exp_num = extract_single_number(str(expected)) if pred_num is not None and exp_num is not None and pred_num == exp_num: is_correct = True else: pred_num = extract_last_number(str(predicted)) exp_num = extract_last_number(str(expected)) is_correct = (pred_num is not None and exp_num is not None and pred_num == exp_num) if is_correct: correct += 1 print(\u0026#34;\\nPrompt:\u0026#34;) print(full_prompt) print(\u0026#34;\\nExpected Answer:\u0026#34;) print(expected) print(\u0026#34;\\nExtracted Answer:\u0026#34;) print(predicted) print(\u0026#34;\\nFull Generated Response:\u0026#34;) print(response) print(\u0026#34;\\nCorrect:\u0026#34;, \u0026#34;✓\u0026#34; if is_correct else \u0026#34;✗\u0026#34;) print(\u0026#34;-\u0026#34;*50) except Exception as e: print(\u0026#34;\\nFailed to parse model output for prompt:\u0026#34;) print(full_prompt) print(\u0026#34;Error:\u0026#34;, e) print(\u0026#34;-\u0026#34;*50) accuracy = (correct / total) * 100 print(f\u0026#34;\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\u0026#34;) print(\u0026#34;=\u0026#34;*50) model.train() return accuracy def correctness_reward(prompts, completions, answer, **kwargs): \u0026#34;\u0026#34;\u0026#34; Assigns a reward based on the correctness of the model\u0026#39;s answer. Args: prompts (list): List of input prompts. completions (list): List of model completions, each containing content. answer (list): List of expected answers. **kwargs: Additional keyword arguments. Returns: list: List of numerical rewards for each completion. Explanation: 1. Extracts the content from each completion. 2. Extracts the answer portion from each response using extract_answer_from_model_output. 3. Assigns rewards based on matching criteria: - 2.0 points for an exact match - 1.5 points for numeric equivalence (when values match but format differs) - 0.0 points for incorrect answers 4. Tracks completion lengths for analysis. \u0026#34;\u0026#34;\u0026#34; responses = [completion[0][\u0026#39;content\u0026#39;] for completion in completions] extracted = [extract_answer_from_model_output(r) for r in responses] rewards = [] for r, a in zip(extracted, answer): if r == a: # Exact match case rewards.append(2.0) else: r_num = extract_single_number(str(r)) a_num = extract_single_number(str(a)) if r_num is not None and a_num is not None and r_num == a_num: rewards.append(1.5) else: rewards.append(0.0) completion_lengths = [len(response.split()) for response in responses] return rewards def format_reward(completions, **kwargs): \u0026#34;\u0026#34;\u0026#34; Assigns a reward for adhering to the desired XML format. Args: completions (list): List of model completions, each containing content. **kwargs: Additional keyword arguments. Returns: list: List of format compliance scores for each completion. Explanation: 1. Extracts the content from each completion. 2. Evaluates format compliance by checking for required XML tags: - 0.2 points for each tag present (\u0026lt;reasoning\u0026gt;, \u0026lt;/reasoning\u0026gt;, \u0026lt;answer\u0026gt;, \u0026lt;/answer\u0026gt;) - Maximum score of 0.8 for perfect format compliance 3. Stores and returns the format compliance scores. \u0026#34;\u0026#34;\u0026#34; responses = [completion[0][\u0026#39;content\u0026#39;] for completion in completions] rewards = [] format_scores = [] for response in responses: score = 0.0 if \u0026#34;\u0026lt;reasoning\u0026gt;\u0026#34; in response: score += 0.2 if \u0026#34;\u0026lt;/reasoning\u0026gt;\u0026#34; in response: score += 0.2 if \u0026#34;\u0026lt;answer\u0026gt;\u0026#34; in response: score += 0.2 if \u0026#34;\u0026lt;/answer\u0026gt;\u0026#34; in response: score += 0.2 rewards.append(score) format_scores.append(score) return rewards def combined_reward(prompts, completions, answer): \u0026#34;\u0026#34;\u0026#34; Combines correctness and format rewards. Args: prompts (list[str]): List of prompt texts completions (list[list[dict]]): List of completion dictionaries answer (list[str]): List of expected answers Returns: list[float]: Combined rewards for each prompt-completion pair Explanation: 1. Calculates separate rewards for correctness and format compliance. 2. Combines the rewards with the following weights: - Correctness score range: 0.0 to 2.0 - Format score range: 0.0 to 0.8 - Total possible range: 0.0 to 2.8 3. Returns the combined reward for each example. \u0026#34;\u0026#34;\u0026#34; # Get individual rewards correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer) format_scores = format_reward(completions=completions) # Combine rewards - correctness is weighted more heavily combined_rewards = [] for c_score, f_score in zip(correctness_scores, format_scores): # Correctness score range: 0.0 to 2.0 # Format score range: 0.0 to 0.8 # Total range: 0.0 to 2.8 combined_rewards.append(c_score + f_score) return combined_rewards def selective_log_softmax(logits, input_ids): \u0026#34;\u0026#34;\u0026#34; Computes log probabilities for specific tokens in the vocabulary. Args: logits (torch.Tensor): The raw logits output from the model. input_ids (torch.Tensor): The token IDs for which we want the log probabilities. Returns: torch.Tensor: Log probabilities of the selected tokens. Explanation: 1. Applies log softmax to convert logits to log probabilities over the vocabulary. 2. Uses gather to extract only the log probabilities corresponding to the input_ids. 3. Removes the extra dimension to match the original shape of input_ids. \u0026#34;\u0026#34;\u0026#34; log_probs = nn.functional.log_softmax(logits, dim=-1) return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) def compute_log_probs(model, input_ids, attention_mask, logits_to_keep): \u0026#34;\u0026#34;\u0026#34; Computes the log probabilities for a batch of tokens. Args: model: The language model. input_ids (torch.Tensor): Token IDs for input sequences. attention_mask (torch.Tensor): Attention mask for input sequences. logits_to_keep (int): Number of tokens to keep from the end of the sequence. Returns: torch.Tensor: Log probabilities of the selected tokens. Explanation: 1. Gets logits from the model for the input sequence. 2. Selects logits for all tokens except the last one (as we predict next tokens). 3. Selects only the last \u0026#39;logits_to_keep\u0026#39; tokens from both logits and input_ids. 4. Computes log probabilities for these tokens using selective_log_softmax. \u0026#34;\u0026#34;\u0026#34; logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :] input_ids = input_ids[:, -logits_to_keep:] # select the generation part, remove the prompt part logits = logits[:, -logits_to_keep:, :] # the same as input_ids return selective_log_softmax(logits, input_ids) def create_completion_mask(completion_ids, eos_token_id): \u0026#34;\u0026#34;\u0026#34; Creates a mask for completion tokens that excludes tokens after the EOS token. Args: completion_ids (torch.Tensor): Token IDs of the generated completions. eos_token_id (int): The ID of the end-of-sequence token. Returns: torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token. Explanation: 1. Identifies positions where EOS tokens occur in each sequence. 2. Finds the index of the first EOS token in each sequence. 3. Creates a mask where positions before and including the first EOS are 1, others are 0. 4. If no EOS token is found in a sequence, all positions are set to 1. \u0026#34;\u0026#34;\u0026#34; is_eos = completion_ids == eos_token_id # shape (bs, max_completion_length) eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device) # 表示每个completion的第一个Eos_token的位置，初始化全为max_completion_length. shape (bs, ) mask_exists = is_eos.any(dim=1) # 返回一个布尔向量 shape (bs, )，表示哪些序列包含至少一个Eos token eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists] # 对于包含Eos token的序列，找到第一个Eos token的位置，is_eos.int().argmax(dim=1)返回每个序列中第一个Eos token的索引。 sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1) # shape (bs, max_completion_length) return (sequence_indices \u0026lt;= eos_idx.unsqueeze(1)).int() def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32): \u0026#34;\u0026#34;\u0026#34; Generates multiple completions for each prompt. Args: model: The language model. tokenizer: The tokenizer for encoding and decoding text. prompts (list): List of text prompts. num_generations (int): Number of completions to generate per prompt. max_completion_length (int): Maximum number of tokens to generate. Returns: tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask. Explanation: 1. Encodes the prompts and moves them to the appropriate device. 2. Repeats each prompt num_generations times to generate multiple completions. 3. Generates completions using the model with specified parameters. 4. Extracts the completion IDs (excluding the prompt tokens). 5. Creates a mask for the completions using create_completion_mask. \u0026#34;\u0026#34;\u0026#34; device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) inputs = tokenizer(prompts, return_tensors=\u0026#34;pt\u0026#34;, padding=True, padding_side=\u0026#34;left\u0026#34;) prompt_ids = inputs[\u0026#34;input_ids\u0026#34;].to(device) prompt_mask = inputs[\u0026#34;attention_mask\u0026#34;].to(device) print(f\u0026#34;Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\u0026#34;) prompt_length = prompt_ids.size(1) # .repeat_interleave 沿着dim=0重复num_generations次，使得prompt_ids和prompt_mask的维度都增加了num_generations倍 prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0) # shape (bs * num_generations, max_prompt_length) prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0) # shape (bs * num_generations, max_prompt_length) outputs = model.generate( prompt_ids, attention_mask=prompt_mask, max_new_tokens=max_completion_length, do_sample=True, temperature=1.0, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, early_stopping=False ) print(f\u0026#34;Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\u0026#34;) # completio_ids只包含模型生成的answer部分，长度为max_completion_length completion_ids = outputs[:, prompt_length:] # shape (bs * num_generations, max_completion_length) completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id) # shape (bs * num_generations, max_completion_length) return prompt_ids, prompt_mask, completion_ids, completion_mask def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length): \u0026#34;\u0026#34;\u0026#34; Generates data for GRPO rollouts including completions and log probabilities. Args: model: The policy model being trained. ref_model: The reference model for KL divergence calculation. tokenizer: The tokenizer for encoding and decoding text. batch_samples (list): Batch of training samples. num_generations (int): Number of completions to generate per sample. max_completion_length (int): Maximum completion length. Returns: dict: Dictionary containing all data needed for GRPO updates. Explanation: 1. Extracts prompts and expected answers from the batch samples. 2. Generates completions using the current policy model. 3. Combines prompt and completion tokens. 4. Computes log probabilities from both the policy model and reference model. 5. Formats completions for reward calculation. 6. Repeats prompts and answers to match the number of generated completions. 7. Returns all data needed for GRPO loss calculation. \u0026#34;\u0026#34;\u0026#34; device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) prompts = [sample[\u0026#34;prompt\u0026#34;] if isinstance(sample, dict) else sample[0] for sample in batch_samples] answers = [sample[\u0026#34;answer\u0026#34;] if isinstance(sample, dict) else sample[1] for sample in batch_samples] with torch.no_grad(): prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions( model, tokenizer, prompts, num_generations, max_completion_length ) input_ids = torch.cat([prompt_ids, completion_ids], dim=1) # shape (bs * num_generations, max_prompt_length + max_completion_length) attention_mask = torch.cat([prompt_mask, completion_mask], dim=1) # shape (bs * num_generations, max_prompt_length + max_completion_length) logits_to_keep = completion_ids.size(1) # max_completion_length old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep) # shape (bs * num_generations, max_completion_length) ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep) # shape (bs * num_generations, max_completion_length) formatted_completions = [[{\u0026#39;content\u0026#39;: tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids] repeated_prompts = [p for p in prompts for _ in range(num_generations)] repeated_answers = [a for a in answers for _ in range(num_generations)] return { \u0026#34;input_ids\u0026#34;: input_ids, \u0026#34;attention_mask\u0026#34;: attention_mask, \u0026#34;completion_mask\u0026#34;: completion_mask, \u0026#34;old_log_probs\u0026#34;: old_log_probs, \u0026#34;ref_log_probs\u0026#34;: ref_log_probs, \u0026#34;formatted_completions\u0026#34;: formatted_completions, \u0026#34;repeated_prompts\u0026#34;: repeated_prompts, \u0026#34;repeated_answers\u0026#34;: repeated_answers, \u0026#34;logits_to_keep\u0026#34;: logits_to_keep, \u0026#34;batch_size\u0026#34;: len(prompts), \u0026#34;num_generations\u0026#34;: num_generations } def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2): \u0026#34;\u0026#34;\u0026#34; Computes the GRPO loss for updating the policy model. Args: model: The policy model being trained. ref_model: The reference model for KL divergence calculation. rollout_data (dict): Data generated by generate_rollout_data. tokenizer: The tokenizer for encoding and decoding text. reward_function: Function that calculates rewards for completions. beta (float): KL penalty coefficient. epsilon (float): Clipping parameter for PPO. Returns: torch.Tensor: The GRPO loss to be minimized. Explanation: 1. Computes current token log probabilities using the policy model. 2. Calculates the probability ratio between current and old policies. 3. Computes rewards using the provided reward_function. 4. Calculates advantages by standardizing rewards within each prompt. 5. Computes the PPO surrogate objective with clipping. 6. Calculates the KL divergence between reference and policy models. 7. Combines surrogate loss and KL penalty. 8. Averages the loss across all tokens and batches. \u0026#34;\u0026#34;\u0026#34; device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) input_ids = rollout_data[\u0026#34;input_ids\u0026#34;] attention_mask = rollout_data[\u0026#34;attention_mask\u0026#34;] completion_mask = rollout_data[\u0026#34;completion_mask\u0026#34;] logits_to_keep = rollout_data[\u0026#34;logits_to_keep\u0026#34;] old_log_probs = rollout_data[\u0026#34;old_log_probs\u0026#34;] ref_log_probs = rollout_data[\u0026#34;ref_log_probs\u0026#34;] token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep) ratio = torch.exp(token_log_probs - old_log_probs) # shape (bs * num_generations, max_completion_length) rewards = torch.tensor( reward_function(prompts=rollout_data[\u0026#34;repeated_prompts\u0026#34;], completions=rollout_data[\u0026#34;formatted_completions\u0026#34;], answer=rollout_data[\u0026#34;repeated_answers\u0026#34;]), dtype=torch.float32, device=device ) # shape (bs * num_generations,) #print(f\u0026#34;Rewards: {rewards}\u0026#34;) # Debug rewards batch_size = rollout_data[\u0026#34;batch_size\u0026#34;] num_generations = rollout_data[\u0026#34;num_generations\u0026#34;] rewards = rewards.view(batch_size, num_generations) avg_reward = rewards.mean().item() print(\u0026#34;Average Reward:\u0026#34;, avg_reward) mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations) # shape (bs * num_generations,) std_rewards = rewards.std(dim=1).repeat_interleave(num_generations) # shape (bs * num_generations,) advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1) # shape (bs * num_generations, 1) surr1 = ratio * advantages # shape (bs * num_generations, max_completion_length) surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages # shape (bs * num_generations, max_completion_length) surrogate_loss = torch.min(surr1, surr2) # shape (bs * num_generations, max_completion_length) kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1 # shape (bs * num_generations, max_completion_length) per_token_loss = surrogate_loss - beta * kl # shape (bs * num_generations, max_completion_length) loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean() # completion_mask用于把生成部分中eos token之后的部分mask掉，只保留生成部分的loss，其中每个completion都会对生成有效长度的loss进行平均，然后再对batch求平均，得到batch的loss。这里添加负号因为GRPO优化目标是最大化改值，这里用梯度下降来优化，所以这里加负号 return loss, avg_reward def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4, num_generations=4, gradient_accumulation_steps=4, max_completion_length=128, beta=0.1, learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None): \u0026#34;\u0026#34;\u0026#34; This function is your original working code (train_with_grpo_static) with an added outer loop for iterative GRPO updates per the pseudocode. Args: model: The language model to train. tokenizer: The tokenizer for encoding and decoding text. train_data (list): Training dataset. num_iterations (int): Number of outer iterations (reference model updates). num_steps (int): Number of batch updates per iteration. batch_size (int): Number of prompts per batch. num_generations (int): Number of completions per prompt. max_completion_length (int): Maximum token length for completions. beta (float): KL penalty coefficient. learning_rate (float): Learning rate for optimizer. mu (int): Number of policy updates per batch. epsilon (float): PPO clipping parameter. reward_function: Function that calculates rewards for completions. device_ids (list): List of GPU device IDs for DataParallel. Returns: The trained model. Explanation: 1. For each outer iteration: - Creates a reference model as a deep copy of the current policy model. - Reinitializes the optimizer for the policy model. - For each training step: a. Samples a batch of examples from the training data. b. Generates rollout data including completions and log probabilities. c. For mu iterations: i. Computes the GRPO loss. ii. Updates the policy model using gradient descent. - Monitors GPU memory usage and prints progress information. \u0026#34;\u0026#34;\u0026#34; # assert device_ids is not None and len(device_ids) \u0026gt; 1 # device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # model = nn.DataParallel(model, device_ids=device_ids).cuda() # print(f\u0026#34;Model wrapped with DataParallel across GPUs: {device_ids}\u0026#34;) # num_iterations表示ref_model迭代次数 for iteration in range(num_iterations): print(f\u0026#34;\\nIteration {iteration+1}/{num_iterations}\u0026#34;) # Create a reference model (deep copy) and set it to eval mode. ref_model = copy.deepcopy(model) ref_model.eval() for param in ref_model.parameters(): param.requires_grad = False print(\u0026#34;Reference model created.\u0026#34;) optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) model.train() # pdb.set_trace() # Inner loop: your original training steps. for step in tqdm(range(num_steps)): batch_samples = random.sample(train_data, batch_size) with torch.no_grad(): # 生成经验数据 rollout_data = generate_rollout_data( model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length, ) # 对每批经验池数据学习的次数，一般设置mu=1 for grpo_iter in range(mu): loss, avg_reward = grpo_loss( model, ref_model, rollout_data, tokenizer, reward_function, beta=beta, epsilon=epsilon, ) loss.backward() if (step + 1) % gradient_accumulation_steps == 0: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1) optimizer.step() optimizer.zero_grad() wandb.log({ \u0026#34;loss\u0026#34;: loss.item(), \u0026#34;average_reward\u0026#34;: avg_reward, \u0026#34;iteration\u0026#34;: iteration + 1, \u0026#34;step\u0026#34;: step + 1, \u0026#34;grpo_iter\u0026#34;: grpo_iter + 1 }) print(f\u0026#34;Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \u0026#34; f\u0026#34;GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\u0026#34;) return model def optimize_model_memory(model): \u0026#34;\u0026#34;\u0026#34; Optimizes the model to use less memory during training. Args: model: The language model to optimize. Returns: The optimized model. Explanation: 1. Sets the model to training mode. 2. Disables KV caching to save memory. 3. Enables gradient checkpointing to trade computation for memory. 4. Ensures that input embeddings require gradients: - Either uses the built-in method if available. - Or adds a forward hook to the input embeddings layer. 5. Returns the optimized model ready for memory-efficient training. \u0026#34;\u0026#34;\u0026#34; model.train() model.config.use_cache = False # 不使用kv-cache缓存，减小显存消耗 # First ensure inputs will require gradients if hasattr(model, \u0026#34;enable_input_require_grads\u0026#34;): model.enable_input_require_grads() else: def make_inputs_require_grad(module, input, output): output.requires_grad_(True) model.get_input_embeddings().register_forward_hook(make_inputs_require_grad) # Then enable gradient checkpointing model.gradient_checkpointing_enable() return model if __name__ == \u0026#34;__main__\u0026#34;: # Main execution device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model_name = \u0026#34;/model/ztq147/Qwen/Qwen2.5-1.5B-Instruct\u0026#34; output_dir = \u0026#34;/data/ztq147/temp_models/math_solver_model\u0026#34; if not os.path.exists(output_dir): os.makedirs(output_dir) print(\u0026#34;Downloading model...\u0026#34;) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.bfloat16, device_map=\u0026#39;auto\u0026#39; ) print(\u0026#34;Model downloaded\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\u0026#34;left\u0026#34;) tokenizer.pad_token = tokenizer.eos_token model.config.pad_token_id = tokenizer.eos_token_id model.config.eos_token_id = tokenizer.eos_token_id # setting the number of device # device_ids = list(range(8)) all_data = prepare_dataset(\u0026#34;train\u0026#34;) random.shuffle(all_data) size_of_eval_data = 30 eval_data = all_data[:size_of_eval_data] train_data = all_data[size_of_eval_data:] # print(\u0026#34;\\nInital model evaluation before finetuning:\u0026#34;) # pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) # print(f\u0026#34;Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\u0026#34;) model = optimize_model_memory(model) print(\u0026#34;\\nStarting RL fine-tuning using GRPO...\u0026#34;) training_config = { \u0026#39;num_iterations\u0026#39;: 1, \u0026#39;num_steps\u0026#39;: 500, \u0026#39;batch_size\u0026#39;: 2, \u0026#39;num_generations\u0026#39;: 8, \u0026#39;gradient_accumulation_steps\u0026#39;: 4, \u0026#39;max_completion_length\u0026#39;: 256, \u0026#39;beta\u0026#39;: 0.04, \u0026#39;learning_rate\u0026#39;: 5e-6, \u0026#39;mu\u0026#39;: 1, \u0026#39;epsilon\u0026#39;: 0.1 } wandb.init(project=os.environ[\u0026#34;WANDB_PROJECT\u0026#34;], reinit=True) print(\u0026#34;Weights \u0026amp; Biases initialized.\u0026#34;) # pdb.set_trace() model = train_with_grpo( model=model, tokenizer=tokenizer, train_data=train_data, reward_function=combined_reward, # device_ids=device_ids, **training_config ) wandb.finish() print(\u0026#34;Training completed and wandb run finished.\u0026#34;) print(\u0026#34;\\nFinalmodel evaluation after GTPO RL fine-tuning:\u0026#34;) post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) print(f\u0026#34;Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\u0026#34;) print(\u0026#34;\\nSaving GTPO fine-tuned model...\u0026#34;) model.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) 单卡实验结果 作者使用一张3090GPU训练，实验结果如下：\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=3, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 23.33% (7/30)\n这里横轴是1500步因为设置了mu=3，所以每个step被记录了三次，这个实验主要想看看mu的影响，个人理解mu=1的情况下，ratio的计算结果应该永远是1。\nbatch_size=1, num_generations=8, max_completion_length=400, gradient_accumulation_steps=12, mu=1, num_steps=1500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 36.67% (11/30)\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=6, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 40.00% (12/30)\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 46.67% (14/30)\n单卡实验小结 从上面简单的几次实验结果来看，最好的是batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1，准确率达到了46.67%，但是这个结果与原作者的90%的准确率有较大差距，原作者用8张80G显存的A100，batch_size设置为7（一张卡放1条数据，原文用了数据并行nn.DataParallel），最大生成长度设置为400，num_generations设置为12，mu=1，gradient_accumulation_steps=1，一共训练500步，其他参数与本文这个设置的其他参数一致。\n整体看对结果影响较大的参数有max_completino_length和num_generations。相反，设置gradient_accumulation_steps并没能带来大batch_size的效果（这个不确定是不是代码写的有问题），反而降低性能，mu的设置大于1也降低了性能。\n实验的loss曲线也没有原文那种稳定的上升趋势，虽然不理解为啥loss是上升的，本文贴的所有loss图均是设置了最大Y值，实际上会存在很多loss很高的脉冲（十几到几百），这个脉冲存在的原因也不太清楚。reward图和原文的有一定相似性。\nReferences [1] Shao et al. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models” arXiv preprint arXiv:2402.03300 (2024).\n[2] John Schulman. “Approximating KL Divergence” John Schulman\u0026rsquo;s Homepage 2020.\n[3] Andriy Burkov. “Coding GRPO from Scratch: A Guide to Distributed Implementation with Qwen2.5-1.5B-Instruct” github 2025.\n","permalink":"https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/","summary":"\u003ch3 id=\"简介\"\u003e简介\u003c/h3\u003e\n\u003cp\u003e本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：\u003ca href=\"https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb\" class=\"entityLink\"\u003eGRPO_From-Scratch\u003c/a\u003e以及\u003ca href=\"https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb\" class=\"entityLink\"\u003eGRPO_Qwen-0_5_Instruct\u003c/a\u003e。注：原作者使用8张80G A100完成实验。\u003c/p\u003e\n\u003ch3 id=\"grpo\"\u003eGRPO\u003c/h3\u003e\n\u003cp\u003eGRPO算法原理见\u003ca href=\"https://matrixai.online/my-blog/posts/2025-01-27-deepseek-r1/#311-reinforcement-learning-algorithm\" class=\"entityLink\"\u003ealg-grpo\u003c/a\u003e，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\begin{align*}\n\\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\\n\u0026=\\frac{1}{G} \\sum_{i=1}^G \\left\\{\n\\min \\left[ \n\\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \n\\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \n\\right] \n- \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\theta} \\| \\pi_{\\text{ref}}]\n\\right\\}\\\\\n\u0026=\\frac{1}{G} \\sum_{i=1}^G\\frac{1}{\\vert o_i\\vert}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\}\n\\end{align*}\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nD_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \n\\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} \n- \\log \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - 1,\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}.\n$$\n\u003c/div\u003e\n\u003cp\u003eGRPO算法出自文章\u003ca href=\"https://arxiv.org/abs/2402.03300\" class=\"entityLink\"\u003eDeepSeekMath (2024)\u003c/a\u003e，其中KL散度的计算采用了\u003ca href=\"http://joschu.net/blog/kl-approx.html\" class=\"entityLink\"\u003eApproximating KL Divergence\u003c/a\u003e中的无偏估计方法，即$\\mathbb D_{KL}(q\\Vert p)=r-1-\\log r$，其中$r=\\log\\frac{p(x)}{q(x)}$，该估计相比$-\\log r$具有更小的方差，比$\\frac{1}{2}(\\log r)^2$具有更小的偏差（无偏）。\u003c/p\u003e","title":"GRPO From Scratch"},{"content":"1. 摘要 DeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：DeepSeek-V3\n2. DeepSeek-V3模型结构 2.1 Basic Architecture 图1: DeepSeek-V3基础结构图 DeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。\n2.1.1 Multi-Head Latent Attention 定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\\bold{h}_t\\in\\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：\n$$ \\textcolor{blue}{\\bold{c}_t^{KV}}=W^{DKV}\\bold{h}_t,\\\\ $$ $$ \\left[\\mathbf{k}_{t,1}; \\mathbf{k}^C_{t,2}; \\dots; \\mathbf{k}^C_{t,n_h} \\right] = \\mathbf{k}^C_t = W^{UK} \\mathbf{c}^{KV}_t, $$ $$ \\textcolor{blue}{\\mathbf{k}^R_t} = \\mathrm{RoPE}(W^{KR} \\mathbf{h}_t), $$ $$ \\mathbf{k}_{t,i} = \\left[\\mathbf{k}^C_{t,i}; \\mathbf{k}^R_t \\right], $$ $$ \\left[\\mathbf{v}^C_{t,1}; \\mathbf{v}^C_{t,2}; \\dots; \\mathbf{v}^C_{t,n_h} \\right] = \\mathbf{v}^C_t = W^{UV} \\mathbf{c}^{KV}_t. $$ 其中$\\bold{c}_t^{KV}\\in\\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\\in\\mathbb R^{d_c\\times d}$为下投影矩阵，$W^{UK},W^{UV}\\in\\mathbb R^{d_hn_h\\times d_c}$为key和value的上投影矩阵。$W^{KR}\\in\\mathbb R^{d_h^R\\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\\textcolor{blue}{\\bold{c}_t^{KV}}$和$\\textcolor{blue}{\\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。\n对于注意力中的query，团队同样执行low-rank compression，这可以减少训练时的激活缓存的开销：\n$$ \\begin{align*} \\bold c_t^Q \u0026= W^{DQ} \\bold h_t, \\\\ \\left[\\bold q_{t,1}^{C}; \\bold q_{t,2}^{C}; \\dots; \\bold q_{t,n_h}^{C} \\right] \u0026= \\bold q_t^C = W^{UQ} \\bold c_t^Q, \\\\ \\left[\\bold q_{t,1}^{R}; \\bold q_{t,2}^{R}; \\dots; \\bold q_{t,n_h}^{R} \\right] \u0026= \\bold q_t^R = \\text{RoPE}(W^{QR} \\bold c_t^Q), \\\\ \\bold q_{t,i} \u0026= \\left[\\bold q_{t,i}^C; \\bold q_{t,i}^R \\right], \\end{align*} $$ 其中$\\bold c_t^Q\\in\\mathbb R^{d_c^\\prime}$代表query压缩后的隐藏向量；$d_c^\\prime(\\ll d_hn_h)$为query压缩向量，$W^{DQ}\\in\\mathbb R^{d_c^\\prime\\times d},W^{UQ}\\in\\mathbb R^{d_hn_h\\times d_c^\\prime}$分别为query的下投影和上投影矩阵，$W^{QR}\\in\\mathbb R^{d_h^Rn_h\\times d_c^\\prime}$用于生成carry RoPE query向量的矩阵。\n最终，query($\\bold q_{t,i}$)，key($\\bold k_{j,i}$)，value($\\bold v_{j,i}^C$)被用于计算注意力输出$\\bold{u}_t$：\n$$ \\mathbf{o}_{t,i} = \\sum_{j=1}^t \\text{Softmax}_j \\left( \\frac{\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h + d_h^R}} \\right) \\mathbf{v}_{j,i}^C,\\\\ \\mathbf{u}_t = W^O [\\mathbf{o}_{t,1}; \\mathbf{o}_{t,2}; \\ldots; \\mathbf{o}_{t,n_h}], $$ 其中$W^O\\in\\mathbb R^{d\\times d_hn_h}$表示输出投影矩阵。\n常规Multi-Head Attention的参数量计算，$W^K\\in\\mathbb R^{d_hn_h\\times d}$，$W^Q\\in\\mathbb R^{d_hn_h\\times d}$，$W^V\\in\\mathbb R^{d_hn_h\\times d}$，$W^O\\in\\mathbb R^{d\\times d_hn_h}$\n$$ \\begin{align*} \\bold{k}_t\u0026=W^K\\bold{h}_t\\\\ [\\bold{k}_{t,1},\\bold k_{t,2},\\cdots,\\bold k_{t,n_h}]\u0026=\\bold{k}_t\\\\ \\bold v_t\u0026=W^V\\bold h_t\\\\ [\\bold v_{t,1},\\bold v_{t,2},\\cdots,\\bold v_{t,n_h}]\u0026=\\bold{v}_t\\\\ \\bold q_t\u0026=W^Q\\bold h_t\\\\ [\\bold q_{t,1},\\bold q_{t,2},\\cdots,\\bold q_{t,n_h}]\u0026=\\bold q_t\\\\ \\bold o_{t,i}\u0026=\\sum_{j=1}^t\\text{Softmax}_j(\\frac{\\bold q_{t,i}^T\\bold k_{j,i}}{\\sqrt{d_h}})\\bold v_{j,i}\\\\ \\bold u_t\u0026=W^O[\\bold o_{t,1},\\bold o_{t,2},\\cdots,\\bold o_{t,n_h}] \\end{align*} $$ 因此对于Multi-Head Attention，一层的总参数量为：\n$$ 3\\times(d_hn_h\\times d + d) + d\\times d_hn_h + d_hn_h = 4d_hn_hd+3d + d_hn_h $$ 对于Multi-Head Latent Attention，一层的总参数量为（不计算RoPE相关的参数，假设$d_c=d_c^\\prime$）：\n$$ \\begin{align*} \u0026(d_c\\times d+d) + 2\\times(d_hn_h\\times d_c+d_c)+(d_c^\\prime\\times d+d)+(d_hn_h\\times d_c^\\prime+d_c^\\prime)\\\\ \u0026=2\\times(d_c\\times d+d)+3\\times(d_hn_h\\times d_c+d_c)\\\\ \u0026=d_c(3d_hn_h+2d)+2d+3d_c \\end{align*} $$ 忽略bias项，MLA参数量与MHA参数量之比：\n$$ \\delta=\\frac{d_c(3d_hn_h+2d)}{4d_hn_hd}=\\frac{3+\\frac{2d}{d_hn_h}}{4\\frac{d}{d_c}} $$ 由于$d_c\\ll d_hn_h, d_c\\ll d$，所以$\\delta\\ll 1$。\n2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing Basic Architecture of DeepSeekMoE. DeepSeekMoE同时使用了finer-grained experts和shared experts，即部分专家是所有token共享，部分是通过路由决定。令$\\bold u_t$表示第$t$个token的FFN层输入向量，通过如下公式计算FFN层的输出向量$\\bold h_t^\\prime$：\n$$ \\begin{align*} \\mathbf{h}_t' \u0026= \\mathbf{u}_t + \\sum_{i=1}^{N_s} \\text{FFN}_i^{(s)} (\\mathbf{u}_t) + \\sum_{i=1}^{N_r} g_{i,t} \\text{FFN}_i^{(r)} (\\mathbf{u}_t),\\\\ g_{i,t} \u0026= \\frac{g_{i,t}'}{\\sum_{j=1}^{N_r} g_{j,t}'},\\\\ g_{i,t}' \u0026= \\begin{cases} s_{i,t}, \u0026 s_{i,t} \\in \\text{Topk}(\\{s_{j,t}|1 \\leq j \\leq N_r\\}, K_r), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases}\\\\ s_{i,t} \u0026= \\text{Sigmoid} (\\mathbf{u}_t^T e_i), \\end{align*} $$ 其中$N_s$和$N_r$分别为共享专家数目和路由专家数目，$\\text{FFN}_i^{(s)}(\\cdot)$和$\\text{FFN}_i^{(r)}(\\cdot)$分别为第$i$个共享专家网络和第$i$个路由专家网络，$K_r$表示每个token输入会被激活的路由专家数目，$g_{i,t}$表示第$i$个路由专家的门控值，$s_{i,t}$表示每个路由专家对该token的分数，$\\bold e_i$为第$i$个路由专家的重心向量。与DeepSeek-V2不同的是，DeepSeek-V3使用sigmoid函数来计算每个路由专家对token的分数，并使用一个归一化处理来得到门控值。\nAuxiliary-Loss-Free Load Balancing. 团队为了均衡每个路由专家的负载量，提出了一个无额外损失函数的负载均衡方法，具体来说，为每个路由专家引入一个偏置项$b_i$：\n$$ \\begin{align*} g_{i,t}' = \\begin{cases} s_{i,t}, \u0026 s_{i,t} + b_i \\in \\mathrm{Topk}(\\{s_{j,t} + b_j | 1 \\leq j \\leq N_r\\}, K_r), \\\\ 0, \u0026 \\text{otherwise.} \\end{cases} \\end{align*} $$ 注意偏置项只用于路由选取时，而不影响最终的门控值。对于具体控制方式，团队在训练过程中，以每个训练step为单位，在每个step结束后，如果当前路由专家过载，则会对该专家的偏置降低$\\gamma$，如果欠载，则偏置增加$\\gamma$，其中$\\gamma$为超参数（bias update speed），通过这种动态调整方式，DeepSeek-V3能在训练中保持每个专家负载均衡。\nComplementary Sequence-Wise Auxiliary Loss. 为了防止单个sequence出现极端不均衡的情况，团队还采用了一个balance loss：\n$$ \\mathcal{L}_{Bal} = \\alpha \\sum_{i=1}^{N_r} f_i P_i, \\\\ f_i = \\frac{N_r}{K_r T} \\sum_{t=1}^T \\mathbb 1 (s_{i,t} \\in \\text{Topk}( \\{ s_{j,t} | 1 \\leq j \\leq N_r \\}, K_r ) ), \\\\ s'_{i,t} = \\frac{s_{i,t}}{\\sum_{j=1}^{N_r} s_{j,t}}, \\\\ P_i = \\frac{1}{T} \\sum_{t=1}^T s'_{i,t}, $$ 其中balance factor $\\alpha$为超参数，会被赋予一个很小的值。$\\mathbb 1(\\cdot)$是示性函数，$T$表示一个句子中的token数量，损失$\\mathcal L_{Bal}$能够鼓励路由专家在句子层级上负载均衡。对于上面的损失计算过程，具体分析如下（不是很明白这个优化目标为啥能让负载均衡？？）：\n$$ \\sum_{i=1}^{N_r}P_i=\\frac{1}{T}\\sum_{t=1}^T\\sum_{i=1}^{N_r}\\frac{s_{i,t}}{\\sum_{j=1}^{N_r}s_{j,t}}=1 $$ $$ \\begin{align*} \\sum_{i=1}^{N_r}f_i\u0026=\\frac{N_r}{K_rT}\\sum_{t=1}^{T}\\sum_{i=1}^{N_r} 1(s_{i,t} \\in \\text{Topk}( \\{ s_{j,t} | 1 \\leq j \\leq N_r \\}, K_r ) )\\\\ \u0026=\\frac{N_r}{K_rT}\\sum_{t=1}^TK_r=N_r \\end{align*} $$ $$ \\mathcal L_{Bal}=\\alpha\\sum_{i=1}^{N_r}f_iP_i\\in[0,\\alpha N_r] $$ No Token-Dropping. 由于良好的专家负载均衡，DeepSeek-V3训练阶段不会丢弃任何tokens，此外团队对推理阶段的负载均衡也采取了相应策略，因此在推理阶段也不会丢弃任何tokens。\n2.2 Multi-Token Prediction 图2: DeepSeek-V3 Multi-Token Prediction结构示意图 DeepSeek-V3中的MTP结构如图2所示，其使用了$D$个MTP模块（不包含主模型）来同时预测额外的$D$个tokens（相当于每个token预测后面1+D个tokens）。其中第$k$（$1\\le k\\le D$）个MTP模块包含一个与主模型共享的词嵌入向量层$\\text{Emb}(\\cdot)$，一个与主模型共享的输出头$\\text{OutHead}(\\cdot)$，一层Transformer块$\\text{TRM}_k(\\cdot)$，和一个投影矩阵$M_k\\in\\mathbb R^{d\\times 2d}$，对于一个输入序列中的第$i$个输入token $t_i$，对$t_i$的第$k$个预测深度，团队首先结合第$i$个token $t_i$在第$(k-1)$预测深度的Transformer块输出隐藏表征向量 $\\bold{h}_i^{k-1}\\in\\mathbb R^d$ 和第$(i+k)$个token的词嵌入向量$\\text{Emb}(t_{i+k})\\in\\mathbb R^d$，并对结合后的向量进行投影：\n$$ \\bold{h}_i^{\\prime k}=M_k[\\text{RMSNorm}(\\bold{h}_i^{k-1});\\text{RMSNorm}(\\text{Emb}(t_{i+k}))], $$ 特别的，当$k=1$时，$\\bold{h}_i^{k-1}$为主模型输出的表征。结合后的向量$\\bold{h}_i^{\\prime k}$作为第$i$个token的第$k$预测深度的MTP模块中的Transformer块的输入，并输出第$i$个token在第$k$预测深度的Transformer块输出的隐藏表征向量$\\bold{h}_i^k$：\n$$ \\bold{h}^k_{i}=\\text{TRM}_k(\\bold{h}_{i}^{\\prime k})\\quad 1\\le i\\le T-k, $$ 其中$T$代表输入序列的长度，最终，将$\\bold{h}_i^k$作为输入，共享输出头计算第$i$个token在第$k$预测深度的预测token的词表维度分布$P_{i+1+k}^k\\in\\mathbb R^V$，其中$V$为词表大小：\n$$ P^k_{i+k+1}=\\text{OutHead}(\\bold{h}_i^k). $$ 经过$\\text{OutHead}(\\cdot)$后再过一个$\\text{Softmax}(\\cdot)$得到第$i$个token在第$k$预测深度的预测token的概率分布。\n结合图2中的示意图，假设输入序列长度$T=8$，最大额外预测深度$D=4$，即总的MTP结构包含一个主模型和4个MTP块。定义输入序列token为{${t_1,t_2,t_3,t_4,t_5,t_6,t_7,t_8}$}，下面来拆解MTP的损失函数计算过程：\n主模型：token {${t_1,t_2,t_3,t_4,t_5,t_6,t_7}$}经过主模型Embedding层和Transformer块输出得到隐藏表征{${\\bold{h}_1^0,\\bold{h}_2^0,\\bold{h}_3^0,\\bold{h}_4^0,\\bold{h}_5^0,\\bold{h}_6^0,\\bold{h}_7^0}$}，再经过Output头得到{${P_2^0,P_3^0,P_4^0,P_5^0,P_6^0,P_7^0,P_8^0 }$}，与Target Tokens {${t_2,t_3,t_4,t_5,t_6,t_7,t_8}$}做交叉熵损失得到$\\mathcal L_{\\text{Main}}$； 第一层MTP模块：输入token {${t_2,t_3,t_4,t_5,t_6,t_7}$}，Transformer块的输入向量为{${\\bold{h}_1^{\\prime 1},\\bold{h}_2^{\\prime 1},\\bold{h}_3^{\\prime 1},\\bold{h}_4^{\\prime 1},\\bold{h}_5^{\\prime 1},\\bold{h}_6^{\\prime 1}, }$}，输出为{${\\bold{h}_1^1,\\bold{h}_2^1,\\bold{h}_3^1,\\bold{h}_4^1,\\bold{h}_5^1,\\bold{h}_6^1 }$}，经过Output头得到{${P_3^1,P_4^1,P_5^1,P_6^1,P_7^1,P_8^1 }$}，与Target Tokens {${t_3,t_4,t_5,t_6,t_7,t_8 }$}做交叉熵损失得到$\\mathcal L^1_{\\text{MTP}}$； $$ \\bold{h}_i^{\\prime 1}=M_1[\\text{RMSNorm}(\\bold{h}_i^0);\\text{RMSNorm}(\\text{Emb}(t_{i+1}))]\\quad 1\\le i\\le 6, $$ $$ \\bold{h}^1_i=\\text{TRM}_1(\\bold{h}_i^{\\prime 1})\\quad 1\\le i\\le 6, $$ $$ P_{i+2}^1=\\text{OutHead}(\\bold{h}_i^1)\\quad 1\\le i\\le 6, $$ 第二层MTP模块：输入token {${t_3,t_4,t_5,t_6,t_7}$}，Transformer块的输入向量为{${\\bold{h}_1^{\\prime 2},\\bold{h}_2^{\\prime 2},\\bold{h}_3^{\\prime 2},\\bold{h}_4^{\\prime 2},\\bold{h}_5^{\\prime 2} }$}，输出为{${\\bold{h}_1^2,\\bold{h}_2^2,\\bold{h}_3^2,\\bold{h}_4^2,\\bold{h}_5^2 }$}，经过Output头得到{${P_4^2,P_5^2,P_6^2,P_7^2,P_8^2 }$}，与Target Tokens {${t_4,t_5,t_6,t_7,t_8 }$}做交叉熵损失得到$\\mathcal L^2_{\\text{MTP}}$； $$ \\bold{h}_i^{\\prime 2}=M_2[\\text{RMSNorm}(\\bold{h}_i^1);\\text{RMSNorm}(\\text{Emb}(t_{i+2}))]\\quad 1\\le i\\le 5, $$ $$ \\bold{h}^2_i=\\text{TRM}_2(\\bold{h}_i^{\\prime 2})\\quad 1\\le i\\le 5, $$ $$ P_{i+3}^2=\\text{OutHead}(\\bold{h}_i^2)\\quad 1\\le i\\le 5, $$ 第三层MTP模块：输入token {${t_4,t_5,t_6,t_7}$}，Transformer块的输入向量为{${\\bold{h}_1^{\\prime 3},\\bold{h}_2^{\\prime 3},\\bold{h}_3^{\\prime 3},\\bold{h}_4^{\\prime 3} }$}，输出为{${\\bold{h}_1^3,\\bold{h}_2^3,\\bold{h}_3^3,\\bold{h}_4^3 }$}，经过Output头得到{${P_5^3,P_6^3,P_7^3,P_8^3 }$}，与Target Tokens {${t_5,t_6,t_7,t_8 }$}做交叉熵损失得到$\\mathcal L^3_{\\text{MTP}}$； $$ \\bold{h}_i^{\\prime 3}=M_3[\\text{RMSNorm}(\\bold{h}_i^2);\\text{RMSNorm}(\\text{Emb}(t_{i+3}))]\\quad 1\\le i\\le 4, $$ $$ \\bold{h}^3_i=\\text{TRM}_3(\\bold{h}_i^{\\prime 3})\\quad 1\\le i\\le 4, $$ $$ P_{i+4}^3=\\text{OutHead}(\\bold{h}_i^3)\\quad 1\\le i\\le 4, $$ 第四层MTP模块：输入token {${t_5,t_6,t_7}$}，Transformer块的输入向量为{${\\bold{h}_1^{\\prime 4},\\bold{h}_2^{\\prime 4},\\bold{h}_3^{\\prime 4}}$}，输出为{${\\bold{h}_1^4,\\bold{h}_2^4,\\bold{h}_3^4 }$}，经过Output头得到{${P_6^4,P_7^4,P_8^4 }$}，与Target Tokens {${t_6,t_7,t_8 }$}做交叉熵损失得到$\\mathcal L^4_{\\text{MTP}}$； $$ \\bold{h}_i^{\\prime 4}=M_4[\\text{RMSNorm}(\\bold{h}_i^3);\\text{RMSNorm}(\\text{Emb}(t_{i+4}))]\\quad 1\\le i\\le 3, $$ $$ \\bold{h}^4_i=\\text{TRM}_4(\\bold{h}_i^{\\prime 4})\\quad 1\\le i\\le 3, $$ $$ P_{i+5}^4=\\text{OutHead}(\\bold{h}_i^4)\\quad 1\\le i\\le 3, $$ 综上，总的MTP训练目标函数为每一层预测深度的损失总和：\n$$ \\mathcal L_{\\text{MTP}}^1=\\text{CrossEntropy}(P^1_{3:8},t_{3:8})\\\\ \\mathcal L_{\\text{MTP}}^2=\\text{CrossEntropy}(P^2_{4:8},t_{4:8})\\\\ \\mathcal L_{\\text{MTP}}^3=\\text{CrossEntropy}(P^3_{5:8},t_{5:8})\\\\ \\mathcal L_{\\text{MTP}}^4=\\text{CrossEntropy}(P^4_{6:8},t_{6:8}) $$ 论文中的写法如下，用了$T+1$应该是对长度为T的句子开头补了eos token，本文的推导过程中的句子长度T就是包含了补token后的结果，特此区别。此外这边的每一层损失都是做了$\\frac{1}{T}$的归一化处理，这部分理解按照正常的交叉熵公式应该是用$\\frac{1}{T-k}$（按本文的$T$处理方式就是$\\frac{1}{T-1-k}$），这里需要看源码。\n$$ \\mathcal{L}_{\\text{MTP}}^k = \\text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = - \\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_i^k[t_i] $$ 最后对每一层的MTP损失进行均值处理，其中团队添加了一个权重因子$\\lambda$，整体的$\\mathcal L_{\\text{MTP}}$作为DeepSeek-V3的一个额外训练损失。\n$$ \\mathcal L_{MTP}=\\frac{\\lambda}{D}\\sum_{k=1}^D\\mathcal L_{\\text{MTP}}^k $$ 对于推理阶段中的MTP：由于MTP策略主要用于提升主模型的性能，因此在推理阶段，可以选择直接丢弃这些MTP模块。\nReferences [1] DeepSeek-AI. “DeepSeek-V3 Technical Report” arXiv preprint axXiv:2412.19437 (2024).\n","permalink":"https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/","summary":"\u003ch3 id=\"1-摘要\"\u003e1. 摘要\u003c/h3\u003e\n\u003cp\u003eDeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：\u003ca href=\"https://github.com/deepseek-ai/DeepSeek-V3\" class=\"entityLink\"\u003eDeepSeek-V3\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"2-deepseek-v3模型结构\"\u003e2. DeepSeek-V3模型结构\u003c/h3\u003e\n\u003ch4 id=\"21-basic-architecture\"\u003e2.1 Basic Architecture\u003c/h4\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/my-blog/images/2025-01-29-deepseek-v3/2025-01-29-image2.jpg\" alt=\"deepseek-v3-architecture\"  /\u003e\n\u003c/p\u003e\n\u003cdiv align='center' style=\"color: #999999\"\u003e图1: DeepSeek-V3基础结构图\u003c/div\u003e\n\u003cp\u003eDeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。\u003c/p\u003e\n\u003ch5 id=\"211-multi-head-latent-attention\"\u003e2.1.1 Multi-Head Latent Attention\u003c/h5\u003e\n\u003cp\u003e定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\\bold{h}_t\\in\\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\textcolor{blue}{\\bold{c}_t^{KV}}=W^{DKV}\\bold{h}_t,\\\\\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\left[\\mathbf{k}_{t,1}; \\mathbf{k}^C_{t,2}; \\dots; \\mathbf{k}^C_{t,n_h} \\right] = \\mathbf{k}^C_t = W^{UK} \\mathbf{c}^{KV}_t,\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\textcolor{blue}{\\mathbf{k}^R_t} = \\mathrm{RoPE}(W^{KR} \\mathbf{h}_t),\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\mathbf{k}_{t,i} = \\left[\\mathbf{k}^C_{t,i}; \\mathbf{k}^R_t \\right],\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\left[\\mathbf{v}^C_{t,1}; \\mathbf{v}^C_{t,2}; \\dots; \\mathbf{v}^C_{t,n_h} \\right] = \\mathbf{v}^C_t = W^{UV} \\mathbf{c}^{KV}_t.\n$$\n\u003c/div\u003e\n\u003cp\u003e其中$\\bold{c}_t^{KV}\\in\\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\\in\\mathbb R^{d_c\\times d}$为下投影矩阵，$W^{UK},W^{UV}\\in\\mathbb R^{d_hn_h\\times d_c}$为key和value的上投影矩阵。$W^{KR}\\in\\mathbb R^{d_h^R\\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\\textcolor{blue}{\\bold{c}_t^{KV}}$和$\\textcolor{blue}{\\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。\u003c/p\u003e","title":"DeepSeek-V3技术报告解读"},{"content":"1. 摘要 本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。\n2. 主要贡献 新的后训练范式：在Base Model上直接使用Large-Scale RL\n不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了自我验证，反思，生成长的CoT的能力。 团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。 蒸馏：小模型也可以很强大\n开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。 3. 方法 3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model DeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。\n3.1.1 Reinforcement Learning Algorithm 团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\\pi_{\\theta_{old}}$采样一组输出${o_1,o_2,\\cdots,o_G}$，然后使用如下优化目标优化策略模型$\\pi_\\theta$：\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right) - \\beta D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) \\right), \\end{align*} $$ $$ D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_i | q)}{\\pi_{\\theta}(o_i | q)} - \\log \\frac{\\pi_{\\text{ref}}(o_i | q)}{\\pi_{\\theta}(o_i | q)} - 1, $$ $$ A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}. $$ 其中$\\epsilon$和$\\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\\cdots,r_G}$计算得到。\n3.1.2 Reward Modeling 团队没有用神经网络模型来获取奖励（主要防止在large-scale RL中的reward hacking问题，且增添训练pipeline复杂度），采用的是基于规则的奖励函数，主要包含以下两种规则：\nAccuracy rewards：评估回答是否正确。例如，数学问题中，模型被要求提供某种格式下的最终答案；代码问题中，生成的代码能够被编译通过并基于预先准备的cases提供正确输出。\nFormat rewards：强制要求模型在其思考过程中打上‘\u0026lt;think\u0026gt;’和‘\u0026lt;/think\u0026gt;’标签。\n3.1.3 Training Template DeepSeek-R1-Zero的训练模版如图1所示。该模版首先要求模型生成推理过程，然后是最终答案。 图1: DeepSeek-R1-Zero训练prompt模版 3.1.4 Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero Performance 图2: DeepSeek-R1-Zero与OpenAI o1在推理基准上的比较 图3: DeepSeek-R1-Zero在AIME上的准确率随着训练步数的变化 此外，团队发现通过majority voting，DeepSeek-R1-Zero的性能还能够进一步加强，在AIME上能从71.0%提升至86.7%。总结，DeepSeek-R1-Zero证明了不使用SFT而直接使用强化学习能够做到很优秀的推理能力。\nSelf-evolution Process of DeepSeek-R1-Zero 图4: DeepSeek-R1-Zero平均回答长度随着RL训练步数变化，能够通过更多的思考时间来解决推理任务 团队发现随着RL训练步数的增加，模型生成长度不断提高，即表明模型回答问题时思考的时间越来越长，在这过程中模型出现了一些比较sophisticated的行为。比如reflection，模型会从新回看自己之前生成的内容；自发的探索其他可能的方法，这些能力并不是通过监督学习得到，而是通过RL训练过程中不断涌现出来的。\nAha Moment of DeepSeek-R1-Zero 图5: DeepSeek-R1-Zero的Aha Moment 这是在RL训练中间过程出现的一个case，即模型学会了通过重新评估自己先前给出的方案来为思考的过程支配更多的时间，这个case表明通过RL能够使模型导向更多超出预期的生成结果。\n3.1.5 Drawback of DeepSeek-R1-Zero DeepSeek-R1-Zero在阅读能力以及语言混合能力上有不足，对此，团队提供了DeepSeek-R1，使用human-friendly cold-start data并结合RL的方法训练出的模型。\n3.2 DeepSeek-R1: Reinforcement Learning with Cold Start 受DeepSeek-R1-Zero强大推理能力的启发，团队提出两个新的问题：\n通过加入一小部分高质量的数据作为冷启动之后，模型的推理能力能否进一步提升，或者模型收敛速度能否提快？ 除了生成强大的CoT能力外，能否训练出一个user-friendly的，具有strong general capabilities的模型？ 对此，团队设计了训练DeepSeek-R1的pipeline，包含下面四个阶段：\n3.2.1 Cold Start 为了避免RL训练的初始不稳定的冷启动阶段，团队收集了一小批long CoT data（高质量SFT数据）用于微调base model作为initial RL actor，为了收集这样的数据，团队探索了几种方法：\nfew-shot prompting with a long CoT as an example directly prompting models to generate detailed answers with reflection and verification 收集DeepSeek-R1-Zero的输出，做成可阅读模式，并通过人工精调这些输出 在该阶段，团队收集了几千条code-start data，并微调DeepSeek-V3-Base作为initial RL actor。相比于DeepSeek-R1-Zero，DeepSeek-R1添加了code-start data有以下几个好处：\n增加输出可阅读性：DeepSeek-R1-Zero一个关键不足是输出的阅读性较差，回复中会混杂多个语言，且对关键部分缺少markdown高亮。 增加模型的潜力 3.2.2 Reasoning-oriented Reinforcement Learning 基于冷启动数据微调后的DeepSeek-V3-Base，团队使用DeepSeek-R1-Zero中相同的RL训练来训练DeepSeek-R1，这过程主要增强模型coding，mathematics，science，logic reasoning能力。\n训练过程中团队发现模型输出的CoT经常混杂多个语言，尤其是当RL的prompt包含多种语言时。为了缓解该问题，团推引入了一种language consistency reward，用于衡量CoT中目标语言统一的比例。尽管消融实验表明添加这个reward会略微降低模型性能，但该reward能使模型输出更加的user-friendly。\n3.2.3 Rejection Sampling and Supervised Fine-Tuning 当上一个阶段收敛后，团队使用收敛后的ckpt收集SFT数据（这次不像冷启动数据只针对reasoning，该阶段的SFT数据也包含其他领域，用于提升模型writing，role-playing，以及其他general-purpose任务的能力）\nReasoning data：600k Non-Reasoning data：200k 3.2.4 Reinforcement Learning for all Scenarios 这一阶段主要为了强化模型除了reasoning外其他通用能力。该阶段中，奖励函数没有使用基于规则的，而是正常用reward model，采用DeepSeek-V3中RL的pipeline，并选择了相似分布的偏好数据对和prompt数据。\n除此之外，该阶段还增强模型的helpfulness和harmlessness。\n3.3 Distillation: Empower Small Models with Reasoning Capability 团队选了base model有：Qwen2.5-Math-1.5B，Qwen2.5-Math-7B，Qwen2.5-14B，Qwen2.5-32B，Llama-3.1-8B，Llama3.3-70B-Instruct。\n对于蒸馏模型，团队仅使用SFT，没有RL阶段。其中SFT数据为3.2.3中使用DeepSeek-R1获取的800K条数据。\n4. 实验 4.1 DeepSeek-R1 Evaluation 图6: DeepSeek-R1-Zero与其他模型效果对比 4.2 Distilled Model Evaluation 图7: DeepSeek-R1蒸馏的小模型在reasoning基准上的对比结果 5. 讨论 5.1 Distillation v.s. Reinforcement Learning 图8: RL模型和蒸馏模型在reasoning基准上的对比 上图中，DeepSeek-R1-Zero-Qwen-32B是基于Qwen-32B-Base模型，使用math，code，STEM数据用large-scale RL训练超过10K步得到的，而DeepSeek-R1-Distill-Qwen-32B为基于Qwen-32B-Base模型使用DeepSeek-R1蒸馏得到。结果表明：\n将大模型能力蒸馏到小模型上能表现出很好的效果，而小模型直接用large-scale RL不仅需要更多的算力，甚至也达不到蒸馏模型的效果。 尽管蒸馏方案经济且有效，扩充模型能力的边界仍需要基于更强的base models并使用larger-scale RL。 5.2 Unsuccessful Attempts 团队早期也尝试了Rrocess Reward Model（PRM）和Monte Carlo Tree Search（MCTS）等方案，但都失败了。\nProcess Reward Model：PRM有三大主要限制 在一条推理链中精细定义一个step比较困难 判断当前中间step是否正确是一个充满挑战的任务。模型标注的数据并不能得到满意结果，而人工标注很难scaling up。 不可避免存在reward hacking的问题，重新训练奖励模型消耗大，增加整个训练流程的复杂度。 Monte Carlo Tree Search：主要用于棋类RL算法 语言模型search space比棋类大得多，这样必须设定一个最大搜索限制，但这样会导致模型陷入局部最优。 价值模型直接影响生成质量，但训练一个好的价值模型很困难。 References [1] DeepSeek-AI. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning ” arXiv preprint axXiv:2501.12948 (2025).\n","permalink":"https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/","summary":"\u003ch3 id=\"1-摘要\"\u003e1. 摘要\u003c/h3\u003e\n\u003cp\u003e本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。\u003c/p\u003e\n\u003ch3 id=\"2-主要贡献\"\u003e2. 主要贡献\u003c/h3\u003e\n\u003cp\u003e新的后训练范式：在Base Model上直接使用Large-Scale RL\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了\u003cstrong\u003e自我验证，反思，生成长的CoT\u003c/strong\u003e的能力。\u003c/li\u003e\n\u003cli\u003e团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e蒸馏：小模型也可以很强大\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"3-方法\"\u003e3. 方法\u003c/h3\u003e\n\u003ch4 id=\"31-deepseek-r1-zero-reinforcement-learning-on-the-base-model\"\u003e3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\u003c/h4\u003e\n\u003cp\u003eDeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。\u003c/p\u003e\n\u003ch5 id=\"311-reinforcement-learning-algorithm\"\u003e3.1.1 Reinforcement Learning Algorithm\u003c/h5\u003e\n\u003cp\u003e团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\\pi_{\\theta_{old}}$采样一组输出${o_1,o_2,\\cdots,o_G}$，然后使用如下优化目标优化策略模型$\\pi_\\theta$：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\begin{align*}\n\\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\\n\u0026=\\frac{1}{G} \\sum_{i=1}^G \\left( \n\\min \\left( \n\\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \n\\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \n\\right) \n- \\beta D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}})\n\\right),\n\\end{align*}\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nD_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \n\\frac{\\pi_{\\text{ref}}(o_i | q)}{\\pi_{\\theta}(o_i | q)} \n- \\log \\frac{\\pi_{\\text{ref}}(o_i | q)}{\\pi_{\\theta}(o_i | q)} - 1,\n$$\n\u003c/div\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nA_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}.\n$$\n\u003c/div\u003e\n\u003cp\u003e其中$\\epsilon$和$\\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\\cdots,r_G}$计算得到。\u003c/p\u003e","title":"DeepSeek-R1技术报告解读"},{"content":"Retrieval-Augmented Generation for Large Language Models: A Survey 1. Overview of RAG 典型的RAG模型如图1所示 图1: 经典RAG模型 1.1 Naive RAG Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。\n索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。 检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。 生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。 1.2 Advanced RAG Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。\npre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。 优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。 优化初始query：常用的策略有query transformation，query expansion等。 post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。 chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。 chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。 1.3 Modular RAG 模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。\n引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等 引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。 图2: 三类不同RAG模型流程示意图 2. Retrieval Part 2.1 检索资源 从检索内容的数据上来看包含以下几种：\n无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等 半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。 结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。 LLMs生成内容 从检索的粒度来看，包含以下几种：\n对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章 对于知识图谱，检索粒度包含：实体，三元组，子图 2.2 索引的优化 在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。\nChunking Strategy：最常用的方法是将文档切分成固定token数的chunk（100，256，512）。越大的chunk能够捕获更多的内容，但也会带来更多噪音，处理更长时间，成本更高；越小的chunk相反。划分chunk存在破坏完整句子的问题，解决该问题的工作有Small2Big等。 Metadata Attachments：chunk可以由元数据（如：page number，file name，author，category timestamp等）扩充，从而检索过程可以使用元数据进行过滤，缩小检索范围。 Structural Index： 分层索引结构 知识图谱索引 2.3 Query的优化 Query扩充： Multi-Query：通过LLM将query扩充成多个，然后并行处理这些queries Sub-Query：对于复杂问题，可以将问题拆解成系列子问题 Chain-of-Verification（CoVe） Query Transformation： Query Rewrite：有些原始queries对于LLM检索来说并不是最优。因此prompt LLM来重写queries，工作如Rewrite-retrieve-read等 Query Routing 2.4 词向量模型（retriever） 从向量编码器角度分包含sparse encoder和dense encoder\nsparse encoder： TF-IDF BM25 dense retriever： BERT-based PLM 3. Generation Part 在完成检索部分后，把所有检索到的信息直接输入LLM来获取答案往往并不是最合理的方案。在生成阶段，一般会从两个方面引入一些调整：调整检索的内容、调整LLM。\n3.1 Context Curation 冗余信息会影响LLM最终的生成结果，通常，LLM会把注意力倾向长文本的开端和结尾，而容易忘记中间的部分。因此在RAG系统中，我们通常需要进一步处理检索到的信息。\nReranking：重排chunks的顺序 rule-based methods：Diversity，Relevance，MRR model-based methods：BERT series（SpanBERT），Cohere rerank，bge-reranker-large Context Selection/Compression：对检索内容的筛选和压缩 Reducing the number of documents 3.2 LLM Fine-tuning 对生成式LLM进行微调，主要适用特定场景下的生成，一般的PLM可能对这些场景了解程度不够，因此需要微调来辅助LLM生成。\n4. Augmentation Process in RAG 常规的RAG流程通过只包含一次检索步骤，然后接着一步生成步骤，对于复杂任务或多步推理场景这种方式局限性较大，因此有优化的检索过程来解决这些问题。\n4.1 迭代检索（Iterative Retrieval） 迭代检索过程中，知识库会基于初始query以及当前生成的文本被重复搜索，为LLM生成提供更全面的信息。相关工作：ITER-RETGEN等。\n4.2 递归检索（Recursive Retrieval） 递归检索通常用于信息检索来提升检索结果的深度和相关性。该过程会基于过往检索的结果迭代优化检索queries。相关工作：IRCoT，ToC等。\n4.3 适应性检索（Adaptive Retrieval） 适应性检索通过让LLMs能够主动决策最优检索的时刻以及检索的内容来优化RAG系统，提升检索信息的相关度以及效率。相关工作：Flare，Self-RAG，AutoGPT，Toolformer，Graph-Toolformer，WebGPT等。 图3: RAG中三类不同增强过程示意图 基于query的RAG方法（query-based） 1. REALM: Retrieval-Augmented Language Model Pre-Training Guu et al. (2020) 提出REALM，一种经典的query-based的RAG方法，文章使用BERT模型作为检索器：\n$$ p(z\\vert x)=\\frac{\\exp f(x,z)}{\\sum_{z^\\prime}\\exp f(x,z^\\prime)},\\\\ f(x,z)=\\text{Embed}_{\\text{input}}(x)^\\top\\text{Embed}_{\\text{doc}}(z) $$ 将检索的文本$z$与query $x$拼接用于answer $y$的生成。\n2. REPLUG: Retrieval-Augmented Black-Box Language Models Shi et al. (2024) 提出一种针对黑盒模型的query-based RAG方法\n无训练Method 基于输入query $x$，选定现有检索器，文档库$\\mathcal D={d_1,\\cdots,d_m}$，检索器为编码器结构，被用来同时对query和文档进行编码。$\\text{E}(d)$为编码器最后一层隐藏表征在所有token上的表征均值。计算query表征与所有文档表征的余弦相似度：\n$$ s(d,x)=cos(\\text E(d), \\text E(x)) $$ 选择其中相似度分数最高的$k$个文档构成集合$\\mathcal D^\\prime\\sub\\mathcal D$。这里为了高效检索，提前计算每个文档的向量表征并构建FAISS索引。\n根据前面计算的相似度分数计算每个相关文档的权重：\n$$ \\lambda(d,x)=\\frac{e^{s(d,x)}}{\\sum_{d\\in\\mathcal D^\\prime}e^{s(d,x)}} $$ 为了同时利用所有相关文档，切不超出模型最大输入长度，作者使用加权解码，用上述$\\lambda(d,x)$作为权重：\n$$ p(y\\vert x,\\mathcal D^\\prime)=\\sum_{d\\in\\mathcal D^\\prime}p(y\\vert d\\ \\circ\\ x)\\cdot\\lambda(d,x) $$ 其中$d\\ \\circ\\ x$表示文档$d$和query $x$的拼接。\n带训练Method 作者同时提出一种训练方法主要用于对齐检索器与生成器，训练过程中只更新检索器参数（针对黑盒模型）。首先用初始检索器检索$k$个最相关文档，与之前类似的，计算每个文档的权重分数：\n$$ P_R(d\\vert x)=\\frac{e^{s(d,x)/\\gamma}}{\\sum_{d\\in\\mathcal D^\\prime}e^{s(d,x)/\\gamma}} $$ 其中$\\gamma$为超参数控制softmax的温度，计算得到的$P_R(d\\vert x)$分布代表了检索器的检索分布。紧接着，给定ground truth $y$，对于每个相关文档，计算生成器在ground truth部分的LM perplexity $P_{LM}(y\\vert d,x)$，并得到生成器的分布：\n$$ Q(d\\vert x,y)=\\frac{e^{P_{LM}(y\\vert d,x)/\\beta}}{\\sum_{d\\in\\mathcal D^\\prime}e^{P_{LM}(y\\vert d,x)/\\beta}} $$ 其中$\\beta$也是调节softmax温度的超参数。最终根据上面检索器的分布$P_R(d\\vert x)$和生成器的分布$Q(d\\vert x,y)$计算两者的KL-divergence并作为损失函数优化检索器参数：\n$$ \\mathcal{L} = \\frac{1}{|\\mathcal{B}|} \\sum_{x \\in \\mathcal{B}} KL \\left( P_R(d \\mid x)\\ \\Vert\\ Q_{LM}(d \\mid x, y) \\right) $$ 其中$\\mathcal B$是query集合，每个query $x$均有一个ground truth $y$。注意到由于检索器参数更新，使得预先存好的所有文档向量表征会有所变化，为了高效训练，作者采用的方案是每训练$T$个steps后重新计算所有文档的向量表征。\n3. In-Context RALM: In-Context Retrieval-Augmented Language Models Ram et al. (2023) 提出一种基于in-context的RAG方法，该方法主要使用了Retrieval Stride和Retrieval Query Length两个trick。\nIn-Context RALM 不同于普通的query-based RAG方法只基于query检索一次文档库，In-Context RALM会在生成过程中不断基于当前的生成结果去多次检索文档库，定义目前生成的文本（包括query）为$x_{\\lt i}$，使用$x_{\\lt i}$检索得到的文档内容为$\\mathcal R_\\mathcal C(x_{\\lt i})$，那么In-Context RALM的生成过程可以通过如下公式定义：\n$$ p(x_i,\\dots,x_n)=\\Pi_{i=1}^n p_\\theta(x_i\\vert \\mathcal R_\\mathcal C(x_{\\lt i});x_{\\lt i}) $$ Retrieval Stride 由于频繁检索文档库会带来比较高的资源消耗，且降低生成速度，因此作者提出Retrieval Stride的概念，即每生成$s(s\u0026gt;1)$个token后进行一个检索，这样RALM的生成过程为：\n$$ p(x_1, \\ldots, x_n) = \\prod_{j=0}^{n_s-1} \\prod_{i=1}^s p_\\theta \\left( x_{s \\cdot j + i} \\mid \\left[ \\mathcal{R}_\\mathcal C(x_{\\leq s \\cdot j}); x_{\\lt s \\cdot j + i} \\right] \\right) $$ 其中$n_s=n/s$为检索的次数（Retrieval Strides）。实验结果表明使用较小的$s$（尽可能多的增加检索次数）会比使用较大的$s$效果好，但是会增加时间成本。\nRetrieval Query Length 作者指出尽管检索query原则上取决于所有的prefix tokens $x_{\\le s\\cdot j}$，但是与生成token最相关的信息往往都聚集在prefix tokens的末尾，如果检索query太长那么这些信息会被稀释。对此作者提出Retrieval Query Length的概念，即控制query长度不超过$\\ell$，当query长度超过$\\ell$时截取整个query的最后$\\ell$个token，即$q_j^{s,\\ell}:=x_{s\\cdot j-\\ell+1},\\cdots,x_{s\\cdot j}$，应用上述trick后的生成过程定义如下：\n$$ p(x_1, \\dots, x_n) = \\prod_{j=0}^{n_s-1} \\prod_{i=1}^s p_\\theta \\left( x_{s \\cdot j + i} \\middle| \\left[ \\mathcal{R}_\\mathcal{C} \\left( q_j^{s,\\ell} \\right); x_{\u003c s \\cdot j + i} \\right] \\right) $$ 4. SELF-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection Asai et al. (2024) 提出了一种基于反馈的RAG框架，主要通过引入Critic模型和Generator模型，Critic模型会在Generator模型生成前判断是否需要进行检索，如果不需要则直接让Generator生成下一个sequence（文章以一个完整的sequence为单位作为检索间隔），否则使用检索器检索相关文档，之后会让Critic判别每个文档的相关性与是否支持回答该问题等信息。作者在模型词表引入一些reflection tokens作为Critic模型的判别输出结果，通过基于prompt GPT4的方式获取有效监督数据并训练Critic模型以及Generator模型（蒸馏GPT4模型知识）。SELF-RAG整体流程框架如图4所示。\n图4: SELF-RAG整体框架 四类reflection tokens 图5: SELF-RAG中使用的四类reflection tokens SELF-RAG Inference算法 图6: SELF-RAG Inference算法流程 SELF-RAG Training算法（Critic $\\mathcal C$ 和Generator $\\mathcal M$） 图7: SELF-RAG Training算法流程 其中Eq1: $$ \\max_\\mathcal C\\mathbb E_{((x,y),r)\\sim\\mathcal D_{critic}}\\log p_\\mathcal C(r\\vert x,y),\\ r\\ \\text{for reflection tokens} $$\nEq2:\n$$ \\max_\\mathcal M\\mathbb E_{(x,y,r)\\sim\\mathcal D_{gen}}\\log p_\\mathcal M(y,r\\vert x) $$\n基于表征的RAG方法（Representation-based） 1. FID: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Izacard et al. (2021) 提出一种基于representation的RAG方法，作者使用encoder-decoder模型（BART），对于检索器，使用BM25和DPR两种方法检索相关文档，对每个文档都分别使用encoder编码成隐空间表征，并将所有的表征拼接在一起输入decoder解码出answer，作者命名这类结构为Fusion-in-Decoder，结构示意图如图8所示。\n作者在处理数据时，在问题（question），文档标题（title），文档内容（context）之前都添加了特殊tokens：\u0026quot;$\\text{question:}$\u0026quot;，$\\text{title:}$，$\\text{context:}$。\n图8: Fusion-in-Decoder结构图 References [1] Gao et al. “Retrieval-Augmented Generation for Large Language Models: A Survey” arXiv preprint arXiv:2312.10997 (2023)\n[2] Guu et al. “Retrieval Augmented Language Model Pre-Training” ICML 2020.\n[3] Shi et al. “REPLUG: Retrieval-Augmented Black-Box Language Models” NAACL 2024.\n[4] Ram et al. “In-Context Retrieval-Augmented Language Models ” TACL 2023.\n[5] Asai et al. “Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection” ICLR 2024.\n[6] Izacard et al. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering” EACL 2021.\n","permalink":"https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/","summary":"\u003ch3 id=\"retrieval-augmented-generation-for-large-language-models-a-survey\"\u003eRetrieval-Augmented Generation for Large Language Models: A Survey\u003c/h3\u003e\n\u003ch4 id=\"1-overview-of-rag\"\u003e1. Overview of RAG\u003c/h4\u003e\n\u003cp\u003e典型的RAG模型如图1所示\n\u003cimg loading=\"lazy\" src=\"/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image1.png\" alt=\"typical rag model\"  /\u003e\n\u003c/p\u003e\n\u003cdiv align='center' style=\"color: #999999\"\u003e图1: 经典RAG模型\u003c/div\u003e\n\u003ch5 id=\"11-naive-rag\"\u003e1.1 Naive RAG\u003c/h5\u003e\n\u003cp\u003eNaive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。\u003c/li\u003e\n\u003cli\u003e检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。\u003c/li\u003e\n\u003cli\u003e生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"12-advanced-rag\"\u003e1.2 Advanced RAG\u003c/h5\u003e\n\u003cp\u003eAdvanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。\n\u003cul\u003e\n\u003cli\u003e优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。\u003c/li\u003e\n\u003cli\u003e优化初始query：常用的策略有query transformation，query expansion等。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003epost-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。\n\u003cul\u003e\n\u003cli\u003echunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。\u003c/li\u003e\n\u003cli\u003echunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"13-modular-rag\"\u003e1.3 Modular RAG\u003c/h5\u003e\n\u003cp\u003e模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等\u003c/li\u003e\n\u003cli\u003e引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image2.png\" alt=\"three rag types\"  /\u003e\n\u003c/p\u003e\n\u003cdiv align='center' style=\"color: #999999\"\u003e图2: 三类不同RAG模型流程示意图\u003c/div\u003e\n\u003ch4 id=\"2-retrieval-part\"\u003e2. Retrieval Part\u003c/h4\u003e\n\u003ch5 id=\"21-检索资源\"\u003e2.1 检索资源\u003c/h5\u003e\n\u003cp\u003e从检索内容的数据上来看包含以下几种：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等\u003c/li\u003e\n\u003cli\u003e半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。\u003c/li\u003e\n\u003cli\u003e结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。\u003c/li\u003e\n\u003cli\u003eLLMs生成内容\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e从检索的粒度来看，包含以下几种：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章\u003c/li\u003e\n\u003cli\u003e对于知识图谱，检索粒度包含：实体，三元组，子图\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"22-索引的优化\"\u003e2.2 索引的优化\u003c/h5\u003e\n\u003cp\u003e在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。\u003c/p\u003e","title":"RAG路线"},{"content":"1. 基本概念，公式 策略$\\pi$，状态$s\\in\\mathcal S$，动作$a\\in\\mathcal A$，奖励$r\\in\\mathcal R$\n转移函数$P$给出当采取行动$a$从状态$s$转移到$s^\\prime$，同时获得奖励$r$的概率\n$$P(s^\\prime,r\\vert s,a)=\\mathbb P[S_{t+1}=s^\\prime,R_{t+1}=r\\vert S_t=s,A_t=a]$$ 状态转移函数$P^a_{ss^\\prime}$\n$$P^a_{ss^\\prime}=P(s^\\prime\\vert s,a)=\\mathbb P[S_{t+1}=s^\\prime|S_t=s,A_t=a]=\\sum_{r\\in\\mathcal R}P(s^\\prime,r\\vert s,a)$$ 奖励函数$R$预测给定状态和动作后的下一个奖励值\n$$R(s,a)=\\mathbb E[R_{t+1}\\vert S_t=s,A_t=a]=\\sum_{r\\in\\mathcal R}r\\sum_{s^\\prime\\in\\mathcal S}P(s^\\prime,r\\vert s,a)$$ 策略$\\pi$给出在状态$s$下会采取何种行动，分为两种\n确定性：$\\pi(s)=a$ 随机性：$\\pi(a\\vert s)=\\mathbb P_\\pi[A=a\\vert S=s]$ 回报$G_t$，即未来的奖励之和，其中$\\gamma\\in[0,1]$为惩罚因子\n$$G_t=R_{t+1}+\\gamma R_{t+2}+\\dots=\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$ 状态价值函数$V_\\pi(s)$给出在状态$s$下的期望回报\n$$V_\\pi(s)=\\mathbb E_\\pi[G_t\\vert S_t=s]$$ 动作价值函数$Q_\\pi(s,a)$给出在状态$s$下采取动作$a$的期望回报\n$$Q_\\pi(s,a)=\\mathbb E_\\pi[G_t\\vert S_t=s, A_t=a]$$ 状态价值和动作价值的关系\n$$V_\\pi(s)=\\sum_{a\\in\\mathcal A}Q_\\pi(s,a)\\pi(a|s)=\\mathbb E_{a\\sim\\pi}Q_\\pi(s,a)$$ 优势函数$A_\\pi(s,a)$定义为动作价值与状态价值的差 $$A_\\pi(s,a)=Q_\\pi(s,a)-V_\\pi(s)$$ 最优价值函数定义为在最优策略下的价值函数，即能够产生最大回报 $$V_*(s)=\\max_\\pi V_\\pi(s)\\\\ Q_*(s,a)=\\max_\\pi Q_\\pi(s,a)$$ 最优策略定义为实现最优价值的策略，即对任意状态$s$都有$V_\\pi(s)\\ge V_{\\pi^\\prime}(s)$，最优策略可能有多个，都将其表示为$\\pi_*(s)$\n$$\\pi_*=\\arg\\max_\\pi V_\\pi(s)\\\\ \\pi_*=\\arg\\max_\\pi Q_\\pi(s,a)$$ 因此，以下关系是成立的\n$$V_{\\pi_*}(s)=V_*(s)\\\\ Q_{\\pi_*}(s,a)=Q_*(s,a)$$ 2. 马尔可夫过程（MDPs） 几乎所有RL问题都可以划在马尔可夫过程内，马尔可夫过程内的所有状态都有同一个特性，即未来的状态只取决于当下的状态，与历史状态无关 $$\\mathbb P[S_{t+1}\\vert S_t]=\\mathbb P[S_{t+1}\\vert S_1,\\dots S_t]$$ 一个马尔可夫决策过程包含五个元素$\\mathcal M=\\langle \\mathcal S,\\mathcal A,\\mathcal P,\\mathcal R,\\gamma\\rangle$，对应的符号与基本符号含义相同\n$\\mathcal S$：状态集合 $\\mathcal A$：动作集合 $\\mathcal P$：转移概率函数 $\\mathcal R$：奖励函数 $\\gamma$：惩罚因子 3. 贝尔曼方程（Bellman Equations） 贝尔曼方程主要将价值函数分解成及时奖励和折扣后的未来价值\n$$ \\begin{align*} V(s)\u0026=\\mathbb E[G_t\\vert S_t=s]\\\\ \u0026=\\mathbb E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dots\\vert S_t=s]\\\\ \u0026=\\mathbb E[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+\\dots)\\vert S_t=s]\\\\ \u0026=\\mathbb E[R_{t+1}+\\gamma G_{t+1}\\vert S_t=s]\\\\ \u0026=\\mathbb E[R_{t+1}\\vert S_t=s] + \\gamma\\mathbb E[G_{t+1}\\vert S_t=s]\\\\ \u0026=R(s) + \\gamma\\mathbb E[V_{t+1}\\vert S_t=s]\\\\ \u0026=R(s) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}\\mathbb P(S_{t+1}=s^\\prime\\vert S_t=s)V(s^\\prime) \\end{align*} $$ 写成矩阵形式，假设$\\mathcal S={s_1,s_2,\\dots,s_n}$\n$$ \\left[ \\begin{array}{c} V(s_1) \\\\ V(s_2) \\\\ \\vdots \\\\ V(s_n) \\end{array} \\right] = \\left[ \\begin{array}{c} R(s_1) \\\\ R(s_2) \\\\ \\vdots \\\\ R(s_n) \\end{array} \\right] + \\gamma \\left[ \\begin{array}{cccc} \\mathbb{P}(s_1 | s_1) \u0026 \\mathbb{P}(s_2 | s_1) \u0026 \\cdots \u0026 \\mathbb{P}(s_n | s_1) \\\\ \\mathbb{P}(s_1 | s_2) \u0026 \\mathbb{P}(s_2 | s_2) \u0026 \\cdots \u0026 \\mathbb{P}(s_n | s_2) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\mathbb{P}(s_1 | s_n) \u0026 \\mathbb{P}(s_2 | s_n) \u0026 \\cdots \u0026 \\mathbb{P}(s_n | s_n) \\end{array} \\right] \\left[ \\begin{array}{c} V(s_1) \\\\ V(s_2) \\\\ \\vdots \\\\ V(s_n) \\end{array} \\right] $$ 关于$\\mathbb E[G_{t+1}\\vert S_t=s]=\\mathbb E[V_{t+1}\\vert S_t=s]$的推导过程如下，主要关注的点：从第一行到第二行为对随机变量$G_{t+1}$的期望进行求和展开，从第三行到第四行为对随机变量$S_{t+1}$进行求和展开\n$$\\begin{align*} \\mathbb E[V_{t+1}\\vert S_t=s]\u0026=\\mathbb E[\\mathbb E[G_{t+1}\\vert S_{t+1}]\\vert S_t=s]\\\\ \u0026=\\mathbb E[\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\mathbb P(G_{t+1}=g^\\prime\\vert S_{t+1})\\vert S_t=s]\\\\ \u0026=\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\mathbb E[\\mathbb P(G_{t+1}=g^\\prime\\vert S_{t+1})\\vert S_t=s] \\\\ \u0026=\\sum_{s^\\prime\\in\\mathcal S}\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\mathbb P(G_{t+1}=g^\\prime\\vert S_{t+1}=s^\\prime,S_t=s)\\mathbb P(S_{t+1}=s^\\prime\\vert S_t=s)\\\\ \u0026=\\sum_{s^\\prime\\in\\mathcal S}\\sum_{g^\\prime\\in\\mathcal G}\\frac{g^\\prime\\mathbb P(G_{t+1}=g^\\prime\\vert S_{t+1}=s^\\prime,S_t=s)\\mathbb P(S_{t+1}=s^\\prime, S_t=s)}{\\mathbb P(S_t=s)}\\\\ \u0026=\\sum_{s\\prime\\in\\mathcal S}\\sum_{g^\\prime\\in\\mathcal G}\\frac{g^\\prime\\mathbb P(G_{t+1}=g^\\prime,S_{t+1}=s^\\prime,S_t=s)}{\\mathbb P(S_t=s)}\\\\ \u0026=\\sum_{s\\prime\\in\\mathcal S}\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\mathbb P(G_{t+1}=g^\\prime,S_{t+1}=s^\\prime\\vert S_t=s)\\\\ \u0026=\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\sum_{s^\\prime\\in\\mathcal S}\\mathbb P(G_{t+1}=g^\\prime, S_{t+1}=s^\\prime\\vert S_t=s)\\\\ \u0026=\\sum_{g^\\prime\\in\\mathcal G}g^\\prime\\mathbb P(G_{t+1}=g^\\prime\\vert S_t=s)\\\\ \u0026=\\mathbb E[G_{t+1}\\vert S_t=s] \\end{align*}$$ 基于上述的推导同理可以对$Q(s,a)$进行分解\n$$\\begin{align*} Q(s,a)\u0026=\\mathbb E[G_t\\vert S_t=s,A_t=a]\\\\ \u0026=\\mathbb E[R_{t+1} + \\gamma V_{t+1}\\vert S_t=s, A_t=a]\\\\ \u0026=R(s, a) + \\gamma \\mathbb E[V_{t+1}\\vert S_t=s, A_t=a]\\\\ \u0026=R(s, a) + \\gamma \\mathbb E[\\mathbb E_{a\\sim\\pi}Q(S_{t+1},a)\\vert S_t=s, A_t=a] \\end{align*}$$ 3.1 贝尔曼期望方程 贝尔曼期望方程公式如下，这两个方程给出了当前状态的价值与未来状态价值之间的关联以及当前时刻的动作价值函数Q与未来时刻的动作价值函数Q之间的关联。\n$V_\\pi(s)=\\sum_{a\\in\\mathcal A}\\pi(a|s)(R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_\\pi(s^\\prime))$ $Q_\\pi(s,a)=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}\\sum_{a^\\prime\\in\\mathcal A}\\pi(a^\\prime\\vert s^\\prime)Q_\\pi(s^\\prime, a^\\prime)$ 由贝尔曼方程的结果，引入策略$\\pi$进一步推导\n$$\\begin{align*} V_\\pi(s)\u0026=\\sum_{a\\in\\mathcal A}\\pi(a\\vert s)Q_\\pi(s,a)\\\\ \u0026=\\sum_{a\\in\\mathcal A}\\pi(a\\vert s)(R(s,a) + \\gamma\\mathbb E[V_{\\pi}(s^\\prime)\\vert S_t=s, A_t=a])\\\\ \u0026=\\sum_{a\\in\\mathcal A}\\pi(a|s)(R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_\\pi(s^\\prime)) \\\\ Q_\\pi(s,a)\u0026=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_\\pi(s^\\prime)\\\\ \u0026=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}\\sum_{a^\\prime\\in\\mathcal A}\\pi(a^\\prime\\vert s^\\prime)Q_\\pi(s^\\prime, a^\\prime) \\end{align*}$$ 3.2 贝尔曼最优方程 $$\\begin{align*} V_*(s)\u0026=\\max_{a\\in\\mathcal A}Q_*(s,a)\\\\ Q_*(s,a)\u0026=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P_{ss^\\prime}^a V_*(s^\\prime)\\\\ V_*(s)\u0026=\\max_{a\\in\\mathcal A}(R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_*(s^\\prime))\\\\ Q_*(s,a)\u0026=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}\\max_{a^\\prime\\in\\mathcal A}Q_*(s^\\prime,a^\\prime) \\end{align*}$$ 第一个式子理解：当智能体来到状态$s$时，接下来假设有两个动作$a_1,a_2$可供选择，采取动作$a_1$后，能够得到的最优价值是$Q_*(s,a_1)$（在最优策略的前提下），采取动作$a_2$后，能够得到的最优价值是$Q_*(s, a_2)$，如果智能体想获得尽可能大的价值，那么它会采取更高价值相关的动作，因此使用最优策略时，在状态$s$下，$V_*(s)=\\max_{a\\in\\mathcal A}Q_*(s,a)$。\n第二个式子理解：基于贝尔曼期望方程$Q_\\pi(s,a)=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_\\pi(s^\\prime)$，智能体在状态$s$采取动作$a$后，到达的每一个新的状态都有最优价值$V_*(s^\\prime)$（在最优策略下），因此得到(12)式。\n第三、四式子理解：将第二式带入第一式得到第三式，将第一式带入第二式得到第四式。\n4. 动态规划（Dynamic Programming） 当模型已知，根据贝尔曼方程，可以使用动态规划迭代求解价值函数并优化策略。\n4.1 策略评估 价值函数的贝尔曼期望方程表示，当策略$\\pi$固定时，$V_\\pi$是一个“最终的收敛值”，满足某种平衡关系，这意味着，$V_\\pi(s)$是所有状态的值函数在多次迭代后的稳定结果。策略评估的迭代公式完全由价值函数的贝尔曼期望方程改写成迭代形式得到\n$$\\begin{align*}V_{t+1}(s)\u0026=\\sum_{a\\in\\mathcal A}\\pi(a|s)(R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P(s^\\prime\\vert s, a)V_t(s^\\prime))\\\\ \u0026=\\mathbb E_\\pi[r+\\gamma V_t(s^\\prime)\\vert S_t=s]=\\sum_a\\pi(a\\vert s)\\sum_{s^\\prime, r}P(s^\\prime,r\\vert s,a)(r+\\gamma V_t(s^\\prime)) \\end{align*}$$ 4.2 策略优化 当我们计算完状态价值函数后，对于当前策略，可以通过动作价值函数Q的贝尔曼期望方程计算当前动作价值函数Q\n$$\\begin{align*}Q_{\\pi_i}(s,a)\u0026=R(s,a) + \\gamma\\sum_{s^\\prime\\in\\mathcal S}P^a_{ss^\\prime}V_{\\pi_i}(s^\\prime)\\\\ \u0026=\\mathbb E[R_{t+1}+\\gamma V_{\\pi_i}(S_{t+1})\\vert S_t=s,A_t=a]\\\\ \u0026=\\sum_{s^\\prime, r}P(s^\\prime,r\\vert s,a)(r+\\gamma V_{\\pi_i}(s^\\prime)) \\end{align*}$$ 对于每个状态，我们取使其得到最大Q价值的动作，从而更新策略\n$$\\pi_{i+1}(s)=\\argmax_a Q_{\\pi_i}(s,a)$$ 4.3 策略迭代过程 基于策略评估和策略优化，实现策略迭代过程。假设有初始策略$\\pi_0$、初始状态价值函数$V_0(s)$、奖励函数$R(s,a)$、状态转移函数$P^a_{ss^\\prime}$，首先通过策略评估迭代状态价值函数得到$V_{\\pi_0}$，然后计算动作价值函数Q$Q_{\\pi_0}(s,a)$，并基于最新动作价值函数Q更新策略得到$\\pi_1$，基于这个流程不断迭代策略和状态价值函数最终收敛得到$V_*, \\pi_*$。\n$$\\pi_0\\xrightarrow[V_0]{\\text{evaluation}}V_{\\pi_0}\\xrightarrow[\\text{improve}]{Q_{\\pi_0}(s,a)}\\pi_1\\xrightarrow[V_{\\pi_0}]{\\text{evaluation}}V_{\\pi_1}\\xrightarrow[\\text{improve}]{Q_{\\pi_1}(s,a)}\\pi_2\\xrightarrow[V_{\\pi_1}]{\\text{evaluation}}\\dots\\xrightarrow[\\text{improve}]{}\\pi_*\\xrightarrow[]{\\text{evaluation}}V_*$$ 5. 蒙特卡洛方法（Monte-Carlo Methods） 蒙特卡洛方法（MC）使用对真实环境的结果做均值计算得到价值状态函数和动作价值函数Q，需要模型完整学完一个episode $S_1,A_1,R_2,\\dots,S_T$来计算$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$，根据$V(s)=\\mathbb E[G_t\\vert S_t=s]$以及$Q(s,a)=\\mathbb E[G_t\\vert S_t=s, A_t=a]$计算$V(s)$和$Q(s,a)$，是一个model-free的方法。\n$$V(s)=\\frac{\\sum_{t=1}^T\\mathbb 1[S_t=s]G_t}{\\sum_{t=1}^T\\mathbb 1[S_t=s]}\\\\ Q(s,a)=\\frac{\\sum_{t=1}^T\\mathbb 1[S_t=s, A_t=a]G_t}{\\sum_{t=1}^T\\mathbb 1[S_t=s,A_t=a]}$$ 为了通过MC学习最优策略，我们采取和动态规划中使用的GPI (Generalized Policy Iteration)相类似的方法：\n根据计算的$Q_{\\pi_i}(s,a)$更新策略：$\\pi_{i+1}(s)=\\argmax_{a\\in\\mathcal A}Q(s,a)$。 使用新策略$\\pi_{i+1}$生成一个新的episode：$S^{i+1}_1,A^{i+1}_1,R^{i+1}_2,\\dots,S^{i+1}_T$。 根据新的episode估计新的动作价值函数Q：$Q_{\\pi_{i+1}}(s,a)=\\frac{\\sum_{t=1}^T(\\mathbb 1[S_t=s,A_t=a]\\sum_{k=0}^{T-t-1}\\gamma^k R_{t+k+1})}{\\sum_{t=1}^T\\mathbb 1[S_t=s, A_t=a]}$ 6. 时序差分学习（Temporal-Difference Learning） 时序差分（TD）也是一种model-free方法，从经验episodes中学习，但与MC不同的是，TD可以从不完整的episodes中学习。价值函数更新：\n$$V(S_t)\\leftarrow V(S_t) + \\alpha [G_t - V(S_t)]\\\\ V(S_t)\\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$$ 其中 $R_{t+1} + \\gamma V(S_{t+1})-V(S_t)$ 称为时序差分误差（TD-error）$\\alpha$ 表示更新的步长。蒙特卡洛方法用上面第一个式子作为更新目标，需要计算$G_t$从而需要完整的episodes，时序差分方法用第二个式子作为更新目标，不需要完整的episodes。\n6.1 $\\epsilon-\\text{greedy}$算法 在更新策略时一般使用广义策略迭代（GPI）的思想，但如果在策略提升中一直使用贪婪算法得到一个确定性策略，可能会导致某些状态动作对$(s,a)$永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好，对此常用的解决方案是采用$\\epsilon-\\text{greedy}$ 算法：即有$1-\\epsilon$的概率采用动作价值最大的动作，另外有$\\epsilon$的概率从动作空间中的其他动作随即采取一个。\n$$\\pi(a\\vert s)=\\begin{cases} \\epsilon/|\\mathcal A| + 1 - \\epsilon \u0026 a=\\argmax_{a^\\prime}Q(s,a^\\prime)\\\\ \\epsilon/|\\mathcal A| \u0026 \\text{other action} \\end{cases}$$ 6.2 Sarsa算法（On-Policy TD control） 使用时序差分算法来估计动作价值函数Q：\n$$Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha(R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t))$$ 初始化$t=0$ 从$S_0$开始，选择动作$A_0=\\argmax_{a\\in\\mathcal A}Q(S_0,a)$，一般使用$\\epsilon-\\text{greedy}$ 在$t$时刻，采取动作$A_t$，得到奖励$R_{t+1}$，进入下一个状态$S_{t+1}$ $A_{t+1}=\\argmax_{a\\in\\mathcal A}Q(S_{t+1},a)$ 更新动作价值函数Q：$Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha(R_{t+1} + \\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t))$ $t=t+1$，重复step3-step5 6.3 Q-Learning算法（Off-Policy TD control） Q-Learning中动作价值函数Q的更新方法：\n$$Q(S_t,A_t)\\leftarrow Q(S_t, A_t)+\\alpha(R_{t+1}+\\gamma\\max_{a\\in\\mathcal A}Q(S_{t+1},a)-Q(S_t,A_t))$$ 初始化$t=0$ 从$S_0$状态开始 在$t$时刻，根据动作价值函数Q选取动作$A_t=\\argmax_{a\\in\\mathcal A}Q(S_t, a)$，一般使用$\\epsilon-\\text{greedy}$ 执行动作$A_t$后，得到奖励$R_{t+1}$，进入下一个状态$S_{t+1}$ 更新动作价值函数Q：$Q(S_t,A_t)\\leftarrow Q(S_t, A_t)+\\alpha(R_{t+1}+\\gamma\\max_{a\\in\\mathcal A}Q(S_{t+1},a)-Q(S_t,A_t))$ $t=t+1$，重复step3-step5 Q-Learning算法与Sarsa算法主要区别在于，Q-Learning算法在进入状态$S_{t+1}$后便更新动作价值函数，不需要获取动作$A_{t+1}$，而Sarsa则当获取到动作$A_{t+1}$才更新动作价值函数，且二者的更新算法有差异。 6.4 Deep Q-Network 当状态动作空间很大或者状态空间是连续的时候，往往采用函数来估计动作价值函数Q，例如，使用含参数$\\theta$的函数来计算Q值，即$Q(s,a;\\theta)$。DQN采取时序差分中的Q函数更新值来设计损失函数，具体来说\n$$\\theta_*=\\arg\\min_\\theta\\frac{1}{2N}\\sum_{i=1}^N[Q(s_i,a_i;\\theta)-(r_i+\\gamma\\max_{a^\\prime}Q(s^\\prime_i,a^\\prime;\\theta))]^2$$ 观察上面的优化式会发现拟合的target值$r_i+\\gamma\\max_{a^\\prime}Q(s^\\prime_i,a^\\prime;\\theta)$也随着参数$\\theta$的更新而变化，经验上来看这会导致训练的不稳定，针对该问题，DQN采用两套网络函数，一个是用于训练更新的网络$Q(s,a;\\theta)$，一个是目标网络$Q(s,a;\\theta^-)$。对于训练网络，使用上面的损失函数优化并正常使用梯度下降更新参数，对于目标网络，用于计算target值$r_i+\\gamma\\max_{a^\\prime}Q(s^\\prime_i,a^\\prime;\\theta^-)$，为了让更新目标更稳定，目标网络不回每一步都更新，而是每隔$C$步与训练网络同步一次，即$\\theta^-\\leftarrow\\theta$。\n在Q-Learning算法中，每个数据只会用来更新一次Q函数，在DQN中，采用了经验回放（experience replay）方法，具体做法是维护一个经验池，每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到经验池中，当经验池达到一定大小开始训练，训练从经验池中随机采样进行训练，一批经验数据训练完成后再进行经验数据的获取。DQN算法具体流程如下：\n用随机网络参数$\\theta$初始化网络$Q(s,a;\\theta)$ 复制相同的参数$\\theta^-\\leftarrow\\theta$初始化目标网络$Q(s,a;\\theta^-)$ 初始化经验池$R$ 获取环境初始状态$s_0$ 时间步循环$t=0\\rightarrow T$，根据当前网络$Q(s,a;\\theta)$使用$\\epsilon-\\text{greedy}$策略选择动作$a_t$，执行动作$a_t$获取回报$r_{t+1}$，环境状态变为$s_{t+1}$，将$(s_t,a_t,r_{t+1},s_{t+1})$存储进经验池$R$中，当$R$中数据足够，从$R$中逐批采样$N$个数据 $(s_i,a_i,r_i, s_i^\\prime)_{i=1,\\dots,N}$，对每个数据，用目标网络计算$y_i=r_i+\\gamma\\max_{a^\\prime}Q(s_i^\\prime,a^\\prime;\\theta^-)$，最小化目标损失$\\mathcal L=\\frac{1}{2N}\\sum_{i}(y_i-Q(s_i,a_i;\\theta))^2$，更新目标网络 对不同序列重复step4-step5 6.5 结合TD与MC 前面的Sarsa以及Q-Learning中的价值估计更新都是只使用了一步动作后的回报，即$G_t=R_{t+1}+\\gamma V(S_{t+1})$，基于此可以延伸至多步的回报来更新价值。假设$n$步回报为$G^{(n)}_t, n=1,\\dots,\\infty$，有\n$n$ $G_t$ $\\text{notes}$ $n=1$ $G^{(1)}_t = R_{t+1}+\\gamma V(S_{t+1})$ TD Learning $n=2$ $G^{(2)}_t = R_{t+1}+\\gamma R_{t+2} + \\gamma^2 V(S_{t+2})$ $\\dots$ $n=n$ $G^{(n)}_t=R_{t+1}+\\gamma R_{t+2} + \\dots + \\gamma^{n-1}R_{t+n} + \\gamma^n V(S_{t+n})$ $\\dots$ $n=\\infty$ $G^{(\\infty)}_t=R_{t+1}+\\gamma R_{t+2} + \\dots + \\gamma^{T-t-1}R_T + \\gamma^{T-t}V(S_T)$ MC estimation 从而，$n$步的TD-Learning将采用下面的式子更新状态价值函数\n$$V(S_t)\\leftarrow V(S_t) + \\alpha(G^{(n)}_t-V(S_t))$$ 这显然存在一个问题，即$n$使用哪一个值，一个简单的做法是对所有可能的$n$进行加权求和得到所有回报的加权求和值，称作$\\lambda$-回报：$G^\\lambda_t=(1-\\lambda)\\sum_{n=1}^\\infty\\lambda^{n-1}G_t^{(n)}$，采用这种回报用于更新的记作$\\text{TD}(\\lambda)$，原始版本等价于$\\text{TD}(0)$。$G_t^\\lambda$中乘上的$(1-\\lambda)$系数主要用于归一化，因为$1+\\lambda+\\lambda^2+\\dots=1/(1-\\lambda)$。\n7. 策略梯度算法（Policy Gradient） 上述介绍的方法都是通过学习状态/动作价值函数然后选择最优动作不断迭代。策略梯度方法直接学习策略函数本身$\\pi_\\theta(a\\vert s)$，其中$\\theta$为可学习参数。梯度策略的目的是最大化在这个策略在环境中的期望回报，目标函数定义为\n$$J(\\theta)=\\mathbb E_{s_0}[V_{\\pi_\\theta}(s_0)]$$ 其中$s_0$表示初始状态，通过将目标函数对策略中参数$\\theta$求导，并使用梯度上升法最大化目标函数，从而得到最优策略。\n7.1 状态访问分布 在求解目标函数对$\\theta$的梯度前，先介绍一个状态访问分布。在MDP中定义初始状态分布为$v_0(s)$，定义$P_t^\\pi(s)$表示采取策略$\\pi$使得智能体在$t$时刻状态为$s$的概率，所以有$P^\\pi_0(s)=v_0(s)$，然后定义一个策略的状态访问分布（state visitation distribution）：\n$$v_\\pi(s)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^tP^\\pi_t(s)$$ 其中$1-\\gamma$为使得概率和为1的归一化因子，$v_{\\pi}(s)$可以理解成一个MDP稳定后的状态概率分布。\n7.2 策略梯度推导 $J(\\theta)$对$\\theta$的梯度有以下公式\n$$\\begin{align*} \\nabla_\\theta J(\\theta)\u0026\\propto\\sum_{s\\in\\mathcal S}v_{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal A}Q_{\\pi_\\theta}(s,a)\\nabla_\\theta\\pi_\\theta(a\\vert s)\\\\ \u0026=\\sum_{s\\in\\mathcal S}v_{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a)\\frac{\\nabla_\\theta\\pi_{\\theta}(a\\vert s)}{\\pi_\\theta(a\\vert s)}\\\\ \u0026=\\mathbb E_{\\pi_\\theta}[Q_{\\pi_\\theta}(s,a)\\nabla_\\theta\\ln\\pi_\\theta(a\\vert s)] \\end{align*}$$ 先从状态价值函数推导开始：\n$$\\begin{align*} \\nabla_\\theta V_{\\pi_\\theta}(s)\u0026=\\nabla_\\theta(\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a))\\\\ \u0026=\\sum_{a\\in\\mathcal A}(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a) + \\pi_\\theta(a\\vert s)\\nabla_\\theta Q_{\\pi_\\theta}(s,a))\\\\ \u0026=\\sum_{a\\in\\mathcal A}(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a) + \\pi_\\theta(a\\vert s)\\nabla_\\theta\\sum_{s^\\prime,r}P(s^\\prime,r\\vert s,a)(r+\\gamma V_{\\pi_\\theta}(s^\\prime)))\\\\ \u0026=\\sum_{a\\in\\mathcal A}(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a) + \\gamma\\pi_\\theta(a\\vert s)\\sum_{s^\\prime,r}P(s^\\prime,r\\vert s,a)\\nabla_\\theta V_{\\pi_\\theta}(s^\\prime))\\\\ \u0026=\\sum_{a\\in\\mathcal A}(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a) + \\gamma\\pi_\\theta(a\\vert s)\\sum_{s^\\prime}P(s^\\prime\\vert s,a)\\nabla_\\theta V_{\\pi_\\theta}(s^\\prime)) \\end{align*}$$ 为了简化表示，定义$\\phi(s)=\\sum_{a\\in\\mathcal A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_{\\pi_\\theta}(s,a)$，定义$d_{\\pi_\\theta}(s\\rightarrow x,k)$表示策略$\\pi_\\theta$从状态$s$出发$k$步后到达状态$x$的概率。\n$$\\begin{align*} \\nabla_\\theta V_{\\pi_\\theta}(s)\u0026=\\phi(s)+\\gamma\\sum_{a\\in\\mathcal A}\\pi_\\theta(a\\vert s)\\sum_{s^\\prime}P(s^\\prime\\vert s,a)\\nabla_\\theta V_{\\pi_\\theta}(s^\\prime)\\\\ \u0026=\\phi(s) + \\gamma\\sum_{s^\\prime}\\nabla_\\theta V_{\\pi_\\theta}(s^\\prime)\\sum_{a}\\pi_\\theta(a\\vert s)P(s^\\prime\\vert s,a)\\\\ \u0026=\\phi(s) + \\gamma\\sum_{s^\\prime}d_{\\pi_\\theta}(s\\rightarrow s^\\prime,1)\\nabla_\\theta V_{\\pi_\\theta}(s^\\prime)\\\\ \u0026=\\phi(s) + \\gamma\\sum_{s^\\prime}d_{\\pi_\\theta}(s\\rightarrow s^\\prime,1)[\\phi(s^\\prime) + \\gamma\\sum_{s^{\\prime\\prime}}d_{\\pi_\\theta}(s^\\prime\\rightarrow s^{\\prime\\prime},1)\\nabla_\\theta V_{\\pi_\\theta}(s^{\\prime\\prime})]\\\\ \u0026=\\phi(s) + \\gamma\\sum_{s^\\prime}d_{\\pi_\\theta}(s\\rightarrow s^\\prime,1)\\phi(s^\\prime) + \\gamma^2\\sum_{s^{\\prime\\prime}}d_{\\pi_\\theta}(s\\rightarrow s^{\\prime\\prime},2)\\nabla_\\theta V_{\\pi_\\theta}(s^{\\prime\\prime})\\\\ \u0026=\\phi(s) + \\gamma\\sum_{s^\\prime}d_{\\pi_\\theta}(s\\rightarrow s^{\\prime},1)\\phi(s^\\prime) + \\gamma^2\\sum_{s^{\\prime\\prime}}d_{\\pi_\\theta}(s^\\prime\\rightarrow s^{\\prime\\prime}, 2)\\phi(s^{\\prime\\prime}) + \\gamma^3\\sum_{s^{\\prime\\prime\\prime}}d_{\\pi_\\theta}(s\\rightarrow s^{\\prime\\prime\\prime}, 3)\\nabla_\\theta V_{\\pi_\\theta}(s^{\\prime\\prime\\prime})\\\\ \u0026=\\dots\\\\ \u0026=\\sum_{k=0}^\\infty\\sum_{x\\in\\mathcal S}\\gamma^k d_{\\pi_\\theta}(s\\rightarrow x,k)\\phi(x) \\end{align*}$$ 定义$\\eta (s)=\\mathbb E_{s_0}[\\sum_{k=0}^\\infty\\gamma^kd_{\\pi_\\theta}(s_0\\rightarrow s,k)]=\\frac{1}{1-\\gamma}v_{\\pi_\\theta}(s)$，有\n$$\\begin{align*} \\nabla_\\theta J(\\theta)\u0026=\\nabla_\\theta\\mathbb E_{s_0}[V_{\\pi_\\theta}(s_0)]\\\\ \u0026=\\mathbb E_{s_0}[\\sum_s\\sum_{k=0}^\\infty\\gamma^kd_{\\pi_\\theta}(s_0\\rightarrow s,k)\\phi(s)]\\\\ \u0026=\\sum_s\\mathbb E_{s_0}[\\sum_{k=0}^\\infty\\gamma^k d_{\\pi_\\theta}(s_0\\rightarrow s,k)]\\phi(s)\\\\ \u0026=\\sum_s\\eta(s)\\phi(s)\\\\ \u0026=(\\sum_s\\eta(s))\\sum_s\\frac{\\eta(s)}{\\sum_s\\eta(s)}\\phi(s)\\\\ \u0026\\propto\\sum_s\\frac{\\eta(s)}{\\sum_s\\eta(s)}\\phi(s)\\\\ \u0026=\\sum_s v_{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal A}Q_{\\pi_\\theta}(s,a)\\nabla_\\theta\\pi_\\theta(a\\vert s)\\\\ \u0026=\\mathbb E_{\\pi_\\theta}[Q_{\\pi_\\theta}(s,a)\\nabla_\\theta\\ln\\pi_\\theta(a\\vert s)] \\end{align*}$$ 至此，证明完毕。\n7.3 REINFORCE算法 REINFORCE，也被称为蒙特卡洛策略梯度：\n1.$\\ $随机初始化策略参数$\\theta$\n2.$\\ $用当前策略$\\pi_\\theta$生成一个episode $S_1,A_1, R_2,S_2,A_2,\\dots,S_T$\n3.$\\ $$\\text{For } t=1, 2, \\dots, T:$\n$\\quad$1.$\\ $估计当前时刻$t$到时刻$T$的回报$G_t$\n$\\quad$2.$\\ \\theta\\leftarrow \\theta + \\alpha\\gamma^t G_t\\nabla_\\theta\\ln\\pi_\\theta(A_t\\vert S_t)$\n7.4 Actor-Critic算法 前面DQN为基于值函数的方法，REINFORCE为基于策略的方法，如果价值函数和策略同时被学习，那就是Actor-Critic算法。\nCritic：会给价值函数添加可学习参数$\\omega$，根据不同算法可以是学习动作价值函数$Q_\\omega(a,s)$或者是状态价值函数$V_\\omega(s)$。Critic更新价值函数参数$\\omega$ Actor：受Critic方向指导，更新策略参数$\\theta$ 1.$\\ $ 随机初始化状态$s$，价值函数参数$\\omega$，策略函数参数$\\theta$；并根据当前策略采样一个动作$a\\sim\\pi_\\theta(a\\vert s)$\n2.$\\ \\text{For }t=1,\\dots,T:$\n$\\quad$ 1.$\\ $采样当前时刻奖励$r_t\\sim R(s,a)$以及下一时刻状态$s^\\prime\\sim P(s^\\prime\\vert s,a)$\n$\\quad$ 2.$\\ $采样下一时刻动作$a^\\prime\\sim\\pi_\\theta(a^\\prime\\vert s^\\prime)$\n$\\quad$ 3.$\\ $更新策略参数：$\\theta\\leftarrow\\theta+\\alpha_\\theta Q_\\omega(s,a)\\nabla_\\theta\\ln\\pi_\\theta(a\\vert s)$\n$\\quad$ 4.$\\ $计算当时刻$t$的动作价值修正值：\n$\\quad\\quad$ $G_{t:t+1}=r_t + \\gamma Q_\\omega(s^\\prime,a^\\prime) - Q_\\omega(s,a)$\n$\\quad\\quad$ 使用修正值来更新价值函数参数：\n$\\quad\\quad$ $\\omega\\leftarrow\\omega + \\alpha_\\omega G_{t:t+1}\\nabla_\\omega Q_\\omega(s,a)$\n$\\quad$ 5.$\\ $更新当前时刻动作和状态：$a\\leftarrow a^\\prime,s\\leftarrow s^\\prime$\n其中$\\alpha_\\theta$和$\\alpha_\\omega$分别是策略函数参数、价值函数参数的学习率。Step2.4中关于价值函数参数更新这里，基于时序差分中的更新算法设计的损失函数：\n$$\\mathcal L(\\omega)=\\frac{1}{2}(r_t + \\gamma Q_\\omega(s^\\prime,a^\\prime) - Q_\\omega(s,a))^2$$ 与DQN中一类，采取类似目标网络的方法，将上式中$r_t + \\gamma Q_\\omega(s^\\prime,a^\\prime)$作为时序差分目标，不会产生梯度来更新价值函数，因此价值函数的梯度为:\n$$\\begin{align*} \\nabla_\\omega\\mathcal L(\\omega)\u0026=-(r_t + \\gamma Q_\\omega(s^\\prime,a^\\prime) - Q_\\omega(s,a))\\nabla_\\omega Q_\\omega(s,a)\\\\ \u0026=-G_{t:t+1}\\nabla_\\omega Q_\\omega(s,a) \\end{align*}$$ 注意在更新策略函数参数时用的是正向传播，更新价值函数参数时用的是反向传播。\n7.5 TRPO算法（Trust Region Policy Optimization） 在策略梯度算法中存在一个缺点，即使用梯度更新的方法优化策略参数，存在由于学习步长太长导致策略突然变差，进而影响训练效果的问题。对于该问题，TRPO给出的解决方案是：在参数更新时考虑找到一块信任区域（trust region），在该区域上进行策略参数更新能够保证策略性能单调优化。\n假设当前策略$\\pi_\\theta$，考虑如何借助当前$\\theta$找到一个更优的参数$\\theta^\\prime$，使得$J(\\theta^\\prime)\\ge J(\\theta)$，由于初始状态$s_0$的分布与策略无关，因此$J(\\theta)$可以写成对新策略$\\pi_{\\theta^\\prime}$的期望（理解上就是新策略的期望能覆盖所有可能的状态轨迹）：\n$$ \\begin{align*} J(\\theta)\u0026=\\mathbb E_{s_0}[V_{\\pi_{\\theta}}(s_0)]\\\\ \u0026=\\mathbb E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^\\infty \\gamma^t V_{\\pi_\\theta}(s_t)-\\sum_{t=1}^\\infty\\gamma^t V_{\\pi_\\theta}(s_t)\\right]\\\\ \u0026=-\\mathbb E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=0}^\\infty\\gamma^t(\\gamma V_{\\pi_\\theta}(s_{t+1})-V_{\\pi_\\theta}(s_t))\\right] \\end{align*} $$ 基于上述等式：\n$$ \\begin{align*} J(\\theta^\\prime)-J(\\theta)\u0026=\\mathbb E_{s_0}[V_{\\pi_{\\theta^\\prime}}(s_0)]-\\mathbb E_{s_0}[V_{\\pi_\\theta}(s_0)]\\\\ \u0026=\\mathbb E_{\\pi_{\\theta^\\prime}}\\left[\\sum_{t=1}^\\infty\\gamma^t r(s_t,a_t)]+\\mathbb E_{\\pi_{\\theta^\\prime}}[\\sum_{t=1}^\\infty\\gamma^t(\\gamma V_{\\pi_\\theta}(s_{t+1})-V_{\\pi_\\theta}(s_t))\\right]\\\\ \u0026=\\mathbb E_{\\pi_\\theta^\\prime}\\left[\\sum_{t=0}^\\infty\\gamma^t[r(s_t,a_t)+\\gamma V_{\\pi_\\theta}(s_{t+1})-V_{\\pi_\\theta}(s_t)]\\right] \\end{align*} $$ 定义优势函数$A_{\\pi_\\theta}(s_t,a_t)=r(s_t,a_t)+\\gamma V_{\\pi_\\theta}(s_{t+1})-V_{\\pi_\\theta}(s_t)$:\n$$ \\begin{align*} J(\\theta^\\prime)-J(\\theta)\u0026=\\mathbb E_{\\pi_\\theta^\\prime}\\left[\\sum_{t=0}^\\infty\\gamma^t A_{\\pi_\\theta}(s_t,a_t)\\right]\\\\ \u0026=\\sum_{t=0}^\\infty\\gamma^t\\mathbb E_{s_t\\sim P_t^{\\pi_{\\theta^\\prime}}}\\mathbb E_{a_t\\sim\\pi_{\\theta^\\prime}(\\cdot\\vert s_t)}[A_{\\pi_\\theta}(s_t,a_t)]\\\\ \u0026=\\frac{1}{1-\\gamma}\\mathbb E_{s\\sim\\mathcal v_{\\pi_\\theta^\\prime}}\\mathbb E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot\\vert s)}[A_{\\pi_\\theta}(s,a)] \\end{align*} $$ 最后用到了状态访问分布：$\\mathcal v_{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^t P_t^\\pi(s)$，因此新策略只需满足$\\mathbb E_{s\\sim\\mathcal v_{\\pi_\\theta^\\prime}}\\mathbb E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot\\vert s)}[A_{\\pi_\\theta}(s,a)]\\ge 0$，就能保证$J(\\theta^\\prime)\\ge J(\\theta)$。\n直接求解比较困难，因为$\\pi_{\\theta^\\prime}$是需要求解的策略，但又需要用它收集样本。对此TRPO做了一步近似操作，对状态访问分布进行处理，具体而言，忽略新旧策略之间状态访问分布的变化，直接采用旧的策略$\\pi_\\theta$的状态分布，定义：\n$$ L_\\theta(\\theta^\\prime)=J(\\theta) + \\frac{1}{1-\\gamma}\\mathbb E_{s\\sim\\mathcal v_{\\pi_\\theta}}\\mathbb E_{a\\sim\\pi_{\\theta^\\prime}(\\cdot\\vert s)}[A_{\\pi_\\theta}(s,a)] $$ 接着，用重要性采样对动作分布进行处理：\n$$ L_\\theta(\\theta^\\prime)=J(\\theta)+\\mathbb E_{s\\sim\\mathcal v_{\\pi_\\theta}}\\mathbb E_{a\\sim\\pi_\\theta(\\cdot\\vert s)}\\left[\\frac{\\pi_{\\theta^\\prime}(a\\vert s)}{\\pi_\\theta(a\\vert s)}A_{\\pi_\\theta}(s,a)\\right] $$ 这样可以基于旧策略$\\pi_\\theta$采样的数据来估计优化新策略，为了保证新旧策略足够相似，TRPO使用Kullback-Leibler（KL）散度来衡量策略之间的距离，给出整体优化式（$\\theta_k$代表前面的$\\theta$，表示$k$次迭代后的策略）：\n$$ \\max_{\\theta^\\prime} L_\\theta(\\theta^\\prime)\\quad s.t.\\ \\mathbb E_{s\\sim\\mathcal v_{\\pi_{\\theta_k}}}[D_{KL}(\\pi_{\\pi_{\\theta_k}}(\\cdot\\vert s),\\pi_{\\theta^\\prime}(\\cdot\\vert s))]\\le \\delta $$ 即得到TRPO的优化目标：\n$$ % \\begin{align*} \\max_\\theta\\quad\\mathbb E_{s\\sim\\mathcal v_{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim\\pi_{\\theta_k}(\\cdot\\vert s)}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\pi_{\\theta_k}(a\\vert s)}A_{\\pi_{\\theta_k}}(s,a)\\right]\\\\ s.t.\\quad \\mathbb E_{s\\sim\\mathcal v_{\\pi_{\\theta_k}}}\\left[D_{KL}(\\pi_{\\theta_k}(\\cdot\\vert s),\\pi_\\theta(\\cdot\\vert s))\\right]\\le\\delta % \\end{align*} $$ 7.6 PPO算法（Proximal Policy Optimization） PPO-惩罚（PPO-Penalty） 针对TRPO的优化目标函数，PPO-Penalty将KL散度的约束放进目标函数中，变成一个无约束优化问题，并在迭代过程中更新KL散度的系数：\n$$ \\arg\\max_\\theta\\ \\mathbb E_{s\\sim\\mathcal v_{\\pi_\\theta}}\\mathbb E_{a\\sim\\pi_{\\theta_k}(\\cdot\\vert s)}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\pi_{\\theta_k}(a\\vert s)}A_{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot\\vert s),\\pi_\\theta(\\cdot\\vert s)]\\right] $$ 令$d_k=D_{KL}^{\\mathcal v_{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)$，$\\beta$的更新规则（其中$\\delta$为超参数）：\n$\\text{if }d_k\u0026lt;\\delta/1.5,\\ \\text{then}\\ \\beta_{k+1}=\\beta_k/2$ $\\text{if }d_k\u0026gt;\\delta/1.5,\\ \\text{then}\\ \\beta_{k+1}=\\beta_k\\times 2$ $\\text{else}\\ \\beta_{k+1}=\\beta_k$ PPO-截断（PPO-Clip） 另一种形式PPO-截断（PPO-Clip）在目标函数中加入限制，保证更新后参数与更新前参数差距在一定范围内：\n$$ \\arg\\max_\\theta \\mathbb{E}_{s \\sim \\mathcal v_{\\pi_{\\theta_k}}} \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot | s)} \\left[ \\min \\left( \\frac{\\pi_\\theta(a | s)}{\\pi_{\\theta_k}(a | s)} A_{\\pi_{\\theta_k}}(s, a), \\operatorname{clip} \\left( \\frac{\\pi_\\theta(a | s)}{\\pi_{\\theta_k}(a | s)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_{\\pi_{\\theta_k}}(s, a) \\right) \\right] $$ References [1] Weng. “A (Long) Peek into Reinforcement Learning” lilianweng.github.io (2018).\n[2] Mocode. “【强化学习理论】贝尔曼最优方程公式推导 ” CSDN (2023).\n[3] 蘑菇书EasyRL. “马尔可夫决策过程” datawhalechina.github.io.\n[4] 手动学强化学习 “时序差分算法” hrl.boyuai.com\n[5] 手动学强化学习 “DQN 算法” hrl.boyuai.com\n[6] 手动学强化学习 “策略梯度算法” hrl.boyuai.com\n[7] 手动学强化学习 “TRPO算法” hrl.boyuai.com\n","permalink":"https://tqzhong.github.io/my-blog/posts/2024-11-21-reinforcement-learning/","summary":"\u003ch3 id=\"1-基本概念公式\"\u003e1. 基本概念，公式\u003c/h3\u003e\n\u003cp\u003e策略$\\pi$，状态$s\\in\\mathcal S$，动作$a\\in\\mathcal A$，奖励$r\\in\\mathcal R$\u003c/p\u003e\n\u003cp\u003e转移函数$P$给出当采取行动$a$从状态$s$转移到$s^\\prime$，同时获得奖励$r$的概率\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$P(s^\\prime,r\\vert s,a)=\\mathbb P[S_{t+1}=s^\\prime,R_{t+1}=r\\vert S_t=s,A_t=a]$$\n\u003c/div\u003e\n\u003cp\u003e状态转移函数$P^a_{ss^\\prime}$\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$P^a_{ss^\\prime}=P(s^\\prime\\vert s,a)=\\mathbb P[S_{t+1}=s^\\prime|S_t=s,A_t=a]=\\sum_{r\\in\\mathcal R}P(s^\\prime,r\\vert s,a)$$\n\u003c/div\u003e\n\u003cp\u003e奖励函数$R$预测给定状态和动作后的下一个奖励值\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$R(s,a)=\\mathbb E[R_{t+1}\\vert S_t=s,A_t=a]=\\sum_{r\\in\\mathcal R}r\\sum_{s^\\prime\\in\\mathcal S}P(s^\\prime,r\\vert s,a)$$\n\u003c/div\u003e\n\u003c!-- $$R(s)=\\mathbb E[R_{t+1}\\vert S_t=s]$$ --\u003e\n\u003cp\u003e策略$\\pi$给出在状态$s$下会采取何种行动，分为两种\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e确定性：$\\pi(s)=a$\u003c/li\u003e\n\u003cli\u003e随机性：$\\pi(a\\vert s)=\\mathbb P_\\pi[A=a\\vert S=s]$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e回报$G_t$，即未来的奖励之和，其中$\\gamma\\in[0,1]$为惩罚因子\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$G_t=R_{t+1}+\\gamma R_{t+2}+\\dots=\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\n\u003c/div\u003e\n\u003cp\u003e状态价值函数$V_\\pi(s)$给出在状态$s$下的期望回报\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$V_\\pi(s)=\\mathbb E_\\pi[G_t\\vert S_t=s]$$\n\u003c/div\u003e\n\u003cp\u003e动作价值函数$Q_\\pi(s,a)$给出在状态$s$下采取动作$a$的期望回报\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$Q_\\pi(s,a)=\\mathbb E_\\pi[G_t\\vert S_t=s, A_t=a]$$\n\u003c/div\u003e\n\u003cp\u003e状态价值和动作价值的关系\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$V_\\pi(s)=\\sum_{a\\in\\mathcal A}Q_\\pi(s,a)\\pi(a|s)=\\mathbb E_{a\\sim\\pi}Q_\\pi(s,a)$$\n\u003c/div\u003e\n优势函数$A_\\pi(s,a)$定义为动作价值与状态价值的差\n\u003cdiv class=\"scroll-container\"\u003e\n$$A_\\pi(s,a)=Q_\\pi(s,a)-V_\\pi(s)$$\n\u003c/div\u003e\n最优价值函数定义为在最优策略下的价值函数，即能够产生最大回报\n\u003cdiv class=\"scroll-container\"\u003e\n$$V_*(s)=\\max_\\pi V_\\pi(s)\\\\\nQ_*(s,a)=\\max_\\pi Q_\\pi(s,a)$$\n\u003c/div\u003e\n\u003cp\u003e最优策略定义为实现最优价值的策略，即对任意状态$s$都有$V_\\pi(s)\\ge V_{\\pi^\\prime}(s)$，最优策略可能有多个，都将其表示为$\\pi_*(s)$\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\\pi_*=\\arg\\max_\\pi V_\\pi(s)\\\\\n\\pi_*=\\arg\\max_\\pi Q_\\pi(s,a)$$\n\u003c/div\u003e\n\u003cp\u003e因此，以下关系是成立的\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$V_{\\pi_*}(s)=V_*(s)\\\\\nQ_{\\pi_*}(s,a)=Q_*(s,a)$$\n\u003c/div\u003e\n\u003ch3 id=\"2-马尔可夫过程mdps\"\u003e2. 马尔可夫过程（MDPs）\u003c/h3\u003e\n\u003cp\u003e几乎所有RL问题都可以划在马尔可夫过程内，马尔可夫过程内的所有状态都有同一个特性，即未来的状态只取决于当下的状态，与历史状态无关\n$$\\mathbb P[S_{t+1}\\vert S_t]=\\mathbb P[S_{t+1}\\vert S_1,\\dots S_t]$$\n一个马尔可夫决策过程包含五个元素$\\mathcal M=\\langle \\mathcal S,\\mathcal A,\\mathcal P,\\mathcal R,\\gamma\\rangle$，对应的符号与基本符号含义相同\u003c/p\u003e","title":"强化学习笔记"},{"content":"本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。\nSupervised finetuning 首先在主节点克隆deepspeed-chat仓库。\n使用的主要环境：\n1 2 3 4 5 6 7 8 9 pip install torch==1.13.0 pip install datasets pip install sentencepiece pip install protobuf==3.20.3 pip install accelerate pip install deepspeed==0.10.0 pip install transformers==4.44.2 pip install tensorboard pip install numpy==1.26.4 deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：\n1 2 sudo apt update sudo apt install nvidia-cuda-toolkit 之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。\n跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：\n1 2 3 4 datasets └── synthetic-instruct-gptj-pairwise ├── dataset_infos.json └── train-00000-of-00001-1e5d57b93c448e7a.parquet 在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:\n1 2 3 4 5 6 7 8 9 class PromptRawDataset(object): def __init__(self, output_path, seed, local_rank, dataset_name): self.output_path = output_path self.seed = seed self.local_rank = local_rank \u0026#39;\u0026#39;\u0026#39;原始数据的读取，这里根据自己数据集作相应修改\u0026#39;\u0026#39;\u0026#39; self.raw_datasets = load_dataset(\u0026#39;parquet\u0026#39;, data_files=dataset_name) ... 到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。\n单节点跑通之后就开始多节点训练，多节点训练首先每个节点需要安装pdsh工具：\n1 sudo apt install pdsh 其次多节点需要在deepspeed启动命令添加--hostfile参数以及配置NCCL参数，hostfile文件形式如下：\n1 2 3 1.2.3.4 slots=8 1.2.3.5 slots=8 ... 第一列是节点ip，第二列是该节点的gpu数量。NCCL参数配置如下：\n1 OPTIONS_NCCL=\u0026#34;NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1\u0026#34; 其中NCCL_SOCKET_IFNAME是服务器上可用的网络接口，可以通过ip addr show命令查看。\n然后对于每个子节点，都要配置相同的环境（见上）以及相同的代码路径结构，模型文件每个节点都要保存（这里我直接把deepspeed目录打包scp到各个节点了），数据集文件主需要存在主节点上即可。这里卡的比较久的地方是子节点训练环境的位置问题，起初我把训练环境都装在每个节点的一个conda虚拟环境里，主节点进入虚拟环境启动训练脚本，但是当通信到子节点的时候报错提示找不到相关环境：\n1 /usr/bin/python3: Error while finding module specification for \u0026#39;deepspeed.launcher.launch\u0026#39; (ModuleNotFoundError: No module named \u0026#39;deepspeed\u0026#39;) 问题在于这里通信到子节点不会访问对应conda虚拟环境，后来我在子节点conda base下装训练环境也还是不行。最后解决方法是得在linux默认环境下（不带base）把训练依赖装好，这下马上就跑通了。应该是有在conda下也能运行的方法，后续了解了再补充。\n最终的训练脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 #!/bin/bash # Copyright (c) Microsoft Corporation. # SPDX-License-Identifier: Apache-2.0 # DeepSpeed Team OUTPUT=$1 ZERO_STAGE=$2 if [ \u0026#34;$OUTPUT\u0026#34; == \u0026#34;\u0026#34; ]; then OUTPUT=./output fi if [ \u0026#34;$ZERO_STAGE\u0026#34; == \u0026#34;\u0026#34; ]; then ZERO_STAGE=3 fi mkdir -p $OUTPUT num_nodes=3 num_gpus=8 OPTIONS_NCCL=\u0026#34;NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1\u0026#34; model_name_or_path=/home/iaiustc/sft-rlhf/models/opt-1.3 data_output_path=/home/iaiustc/sft-rlhf/output/cache_dir/ds_chat batch_size=2 max_seq_len=512 learning_rate=9.65e-6 weight_decay=0. num_train_epochs=1 gradient_accumulation_steps=4 lr_scheduler_type=cosine num_warmup_steps=0 seed=1234 eval_interval=100 save_interval=100 print_interval=10 ARGS=\u0026#34; \\ --data_path /home/iaiustc/sft-rlhf/datasets/synthetic-instruct-gptj-pairwise/train-00000-of-00001-1e5d57b93c448e7a.parquet \\ --data_output_path ${data_output_path} \\ --data_split 2,4,4 \\ --model_name_or_path ${model_name_or_path} \\ --per_device_train_batch_size ${batch_size} \\ --per_device_eval_batch_size ${batch_size} \\ --max_seq_len ${max_seq_len} \\ --learning_rate ${learning_rate} \\ --weight_decay ${weight_decay} \\ --num_train_epochs ${num_train_epochs} \\ --gradient_accumulation_steps ${gradient_accumulation_steps} \\ --lr_scheduler_type ${lr_scheduler_type} \\ --num_warmup_steps ${num_warmup_steps} \\ --seed ${seed} \\ --zero_stage $ZERO_STAGE \\ --deepspeed \\ --enable_tensorboard \\ --tensorboard_path $OUTPUT \\ --output_dir $OUTPUT \\ --eval_interval ${eval_interval} --save_interval ${save_interval} --print_interval ${print_interval} \u0026#34; if [[ ${num_nodes} -gt 1 ]]; then # create hostfile if num_nodes \u0026gt; 1 python create_hostfile.py hostfile_arg=\u0026#34;--hostfile ./output/hostfile\u0026#34; else hostfile_arg=\u0026#34;\u0026#34; fi deepspeed --num_nodes ${num_nodes} --num_gpus ${num_gpus} \\ ${hostfile_arg} --master_port 12346 \\ main.py \u0026#34;$@\u0026#34; ${ARGS} 2\u0026gt;\u0026amp;1 | tee \u0026#34;${OUTPUT}/training2.log\u0026#34; 至此关于使用deepspeed进行多机多卡做sft训练就完成了，后续关于reward model以及rlhf的训练应该差不多，等实现完后更新。\nReward Model Reward Model本质上就是base model添加一个projction_head头得到的，projction_head头是把base model最后一层输出的hidden_states投影到1维上。因此在多机多卡的训练执行所需基本调整和Supervised Finetuning一样，这里主要记录一下RewardModel类的几个主要功能函数实现细节。\n1. forward函数 DeepSpeed-Chat/dschat/utils/model/reward_model.py\nforward函数用于RM训练计算训练损失以及训练chosen数据和rejected数据的平均得分，也是一种训练参考指标。RM训练损失函数\n$$ \\mathcal L_R=-\\mathbb E_{(x,y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(r_\\phi(x, y_w)-r_\\phi(x,y_l))] $$ 具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class RewardModel(nn.Model): def __init__(self, ...): ... ... def forward(self, input_ids=None, ...): transformer_outputs = self.rwtransformer(...) # 输出最后一层特征 # hidden_states.shape (bs*2, max_seq_len, hidden_size)，数据是前一半为chosen部分，后一半为rejected部分 hidden_states = transformer_outputs[0] # rewards.shape: (bs*2, max_seq_len) rewards = self.v_head(hidden_states).squeeze(-1) bs = input_ids.shape[0] // 2 chosen_ids = input_ids[:bs] rejected_ids = input_ids[bs:] chosen_rewards = rewards[:bs] rejected_rewards = rewards[bs:] chosen_mean_scores = [] rejected_mean_scores = [] loss = 0. for i in range(bs): # (max_seq_len, ) chosen_id = chosen_ids[i] rejected_id = rejected_ids[i] # (bs, max_seq_len) chosen_reward = chosen_rewards[i] rejected_reward = rejected_rewards[i] # 得到chosen_id张量中元素为0的坐标，例如a = torch.tensor([1,2,3,0,0,0]),(a == 0).nonzero()为torch.tensor([[3],[4],[5]]);如果a = torch.tensor([[1,2,3,0,0,0]]), (a == 0).nonzero()为torch.tensor([[0,3],[0,4],[0,5]]) c_inds = (chosen_id == self.PAD_ID).nonzero() # c_ind为chosen_sentence的answer后的第一个pad_token的index，例如chosen_id=torch.tensor([1,2,3,0,0,0]) 那么c_ind=3 # num_padding_at_beginning这个参数主的出现主要由于opt系列模型在input前有一个固定数量（等于1）的padding token (\u0026lt;/s\u0026gt;，和bert有点像)，对于其他autoregression模型没有这种。源码用了opt模型因此这里num_padding_at_beginning设置为1 c_ind = c_inds[self.num_padding_at_beginning].item() if len(c_inds) \u0026gt; self.num_padding_at_beginning else seq_len check_divergence = (chosen_id != rejected_id).nonzero() # 如果当前chosen_sentence和rejected_sentence完全相同,这对数据只计算末位的损失(?) if len(check_divergence) == 0: end_ind = rejected_reward.size(-1) divergence_ind = end_ind - 1 r_ind = c_ind else: r_inds = (rejected_id == self.PAD_ID).nonzero() r_ind = r_inds[self.num_padding_at_beginning].item() if len(r_inds) \u0026gt; self.num_padding_at_beginning else seq_len # end_ind 为c_ind,r_ind两者大值，即计算损失的有效末尾index end_ind = max(c_ind, r_ind) # divergence_ind为chosen_sentence和reject_sentence两者answer的第一个token的index，即计算损失的有效起始index divergence_ind = check_divergence[0] assert divergence_ind \u0026gt; 0 c_truncated_reward = chosen_reward[divergence_ind:end_ind] r_truncated_reward = rejected_reward[divergence_ind:end_ind] # 这两个mean_scores只保留有效answer部分末尾的reward chosen_mean_scores.append(chosen_reward[c_ind - 1]) rejected_mean_scores.append(rejected_reward[r_ind - 1]) loss += -torch.nn.functional.logsigmoid(c_truncated_reward - r_truncated_reward).mean() loss = loss / bs # (bs, ) chosen_mean_scores = torch.stack(chosen_mean_scores) rejected_mean_scores = torch.stack(rejected_mean_scores) return { \u0026#34;loss\u0026#34;: loss, \u0026#34;chosen_mean_scores\u0026#34;: chosen_mean_scores, \u0026#34;rejected_mean_scores\u0026#34;: rejected_mean_scores, } 2. forward_value函数 DeepSpeed-Chat/dschat/utils/model/reward_model.py\nforward_value函数主要用于rlhf阶段reward model和critic model前向计算reward和value，因此这里的input_ids输入不再是chosen和rejected一半一半，而是基于prompt生成的sequence。具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class RewardModel(nn.Model): def __init__(self, ...): ... ... def forward_value(self, input_ids, ...): transformer_outputs = self.rwtransformer(input_ids, ...) hidden_states = transformer_outputs[0] # (bs, max_seq_len) values = self.v_head(hidden_states).squeeze(-1) if return_value_only: return values else: assert prompt_length \u0026gt; 1, \u0026#34;prompt_length must be greater than 1 to help select the end score\u0026#34; bs = values.size(0) seq_len = input_ids.shape[1] chosen_end_scores = [] for i in range(bs): input_id = input_ids[i] value = values[i] # c_ind和forward中含义一样，也是有效answer后的第一个pad_token的index，这里之所以先去除prompt_length部分的id进行计算最后再加上prompt_length，主要因为在rlhf阶段，prompt数据集也是按batch从dataloader中获取，所以prompt中也存在padding（padding + true_prompt），所以这里是为了去除prompt中padding的干扰 c_inds = (input_id[prompt_length:] == self.PAD_ID).nonzero() c_ind = c_inds[0].item() + prompt_length if len(c_inds) \u0026gt; 0 else seq_len chosen_end_scores.append(value[c_ind - 1]) return { \u0026#34;values\u0026#34;: values, # (bs, max_seq_len) \u0026#34;chosen_end_scores\u0026#34;: torch.stack(chosen_end_scores), # (bs,) } RLHF RLHF阶段在代码跑通上与Supervised Finetuning和Reward Model训练的设置一致，前面跑通了，这一阶段基本改一下输入参数就可以直接跑，因此这里主要记录deepspeed关于rlhf部分的实现细节。主要函数均在在DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py脚本的DeepSpeedPPOTrainer类中。\n1. _generate_sequence函数 _generate_sequence函数输入一个batch（bs）的prompts数据，生成一个seq_bs的sequence，这里面主要对生成answer的长度做了过滤，生成的sequence的answer部分长度小于等于1的数据会被扔掉，所以输出sequence的维度变为seq_bs。\n2. generate_experience函数 generate_experience函数用于生成经验数据，输入是一个batch（bs）的prompts数据，输出包括reference model的logprobs，actor model的logprobs，reward model的rewards，critic model的value，以及sequence的input_ids和attention_mask\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def generate_experience(self, prompts, mask): # 所有4个模型进入eval模式 self.eval() seq = self._generate_sequence(prompts, mask, step) if seq is None: assert self.last_generated_experience is not None, f\u0026#39;Invalid generated experience at step={step}\u0026#39; prompts = self.last_generated_experience[\u0026#39;prompts\u0026#39;] seq = self.last_generated_experience[\u0026#39;seq\u0026#39;] else: self.last_generated_experience = {\u0026#39;prompts\u0026#39;: prompts, \u0026#39;seq\u0026#39;: seq} # actor model和critic model进入train模式 self.train() pad_token_id = self.tokenizer.pad_token_id attention_mask = seq.not_equal(pad_token_id).long() with torch.no_grad(): output = self.actor_model(seq, attention_mask=attention_mask) output_ref = self.ref_model(seq, attention_mask=attention_mask) reward_score = self.reward_model.forward_value(seq, attention_mask, prompt_length=self.prompt_length)[\u0026#39;chosen_end_scores\u0026#39;].detach() values = self.critic_model.forward_value(seq, attention_mask, return_value_only=True).detach()[:, :-1] logits = output.logits logits_ref = output_ref.logits return { \u0026#39;prompt\u0026#39;: prompts, # (bs, max_prompt_len) \u0026#39;logprobs\u0026#39;: gather_log_probs(logits[:, :-1, :], seq[:, 1:]), # (seq_bs, max_seq_len - 1) \u0026#39;ref_logprobs\u0026#39;: gather_log_probs(logits_ref[:, :-1, :], seq[:, 1:]), # (seq_bs, max_seq_len - 1) \u0026#39;value\u0026#39;: values, # (seq_bs, max_seq_len - 1) \u0026#39;rewards\u0026#39;: reward_score, # (seq_bs, ) \u0026#39;input_ids\u0026#39;: seq, # (seq_bs, max_seq_len - 1) \u0026#39;attention_mask\u0026#39;: attention_mask # (seq_bs, max_seq_len) } 3. compute_rewards函数 首先介绍rlhf中的reward计算公式\n$$ r_{KL}=r(x,y)-\\beta log\\frac{\\pi^{RL}_{old}(y|x)}{\\pi^{SFT}(y|x)} $$ 具体代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_rewards(self, prompts, log_probs, ref_log_probs, reward_score, action_mask): kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs) rewards = kl_divergence_estimate # (bs, max_seq_len - 1) start = prompts.shape[1] - 1 # ends为batch中各个数据的最后一个有效token的index，是一个数组 # 这里分开prompt部分单独计算也是由于prompts中存在padding ends = start + action_mask[:, start:].sum(1) + 1 # RM得到的奖励值限定在一定范围 reward_clip = torch.clamp(reward_score, -self.clip_reward_value, self.clip_reward_value) batch_size = log_probs.shape[0] for j in range(batch_size): rewards[j, start:end[j]][-1] += reward_clip[j] return rewards # (bs, max_seq_len - 1) 4. actor_loss_fn函数 在一个ppo_batch中，actor损失计算公式\n$$ pg\\_loss=E_{\\tau\\sim\\pi_{old}^{RL}}E_{(s_t,a_t)\\sim\\tau}[max(-\\hat A_t\\cdot\\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},-\\hat A_t\\cdot clip(\\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},1-\\epsilon,1+\\epsilon))] $$ 其中$\\tau$指的仅是“answer”部分内容，不包括“prompt”部分。\n1 2 3 4 5 6 7 8 9 def actor_loss_fn(self, logprobs, old_logprobs, advantages, mask): # policy gradient loss # 重要性采样权重计算 ratio = exp(log(new) - log(old)) log_ratio = (logprobs - old_logprobs) * mask ratio = torch.exp(log_ratio) pg_loss1 = -advantages * ratio pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0 + self.cliprange) pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum() return pg_loss 5. critic_loss_fn函数 在一个ppo_batch中，critic的损失计算公式：1）裁剪新价值估计$V_{new}$，使其不至于太偏离采集经验时的旧价值估计，使得经验回放仍能有效：\n$$ V_{clip}=clip(V_{new}, V_{old}-\\phi,V_{old}+\\phi) $$ 2）critic拟合回报R： $$ vf\\_loss=\\frac{1}{2}\\cdot E_{\\tau\\sim\\pi_{old}^{RL}}E_{s_t\\sim\\tau}[max((V_{new}(s_t)-R_t)^2, (V_{clip}(s_t)-R_t)^2)] $$ 其中$\\tau$指的仅是“answer”部分内容，不包括“prompt”部分。\n1 2 3 4 5 6 7 def critic_loss_fn(self, values, old_values, returns, mask): # value loss values_clipped = torch.clamp(values, old_values - self.cliprange_value, old_values + self.cliprange_value) vf_loss1 = (values - returns) ** 2 vf_loss2 = (values_clipped - returns) ** 2 vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / mask.sum() return vf_loss 6. get_advantages_and_returns函数 优势advantages的计算，包括本框架在内的多数框架的advantages实现并非纯粹TD-error，而是在TD-error基础上结合MC方法，即GAE（广义优势估计）。具体来说，对于全长尾T的轨迹来说，其某个时间步t的优势为（$\\lambda=1$时，advantage完全使用MC方法；$\\lambda=0$时，advantage完全使用TD-error方法）：\n$$ \\hat A_t=\\delta_t+(\\gamma\\lambda)\\delta_{t+1}+(\\gamma\\lambda)^2\\delta_{t+2}+\\dots+(\\gamma\\lambda)^{T-t+1}\\delta_{T-1}\\\\ where\\ \\delta_t=r_{KL,t}+\\gamma\\cdot V_{old}(s_{t+1})-V_{old}(s_t) $$ 回报returns的计算，returns就是奖励reward的累计，对于全长为T的轨迹来说，其到达某个时间步$t$时的回报为： $$ R_t=\\hat A_t+V_t $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def get_advantages_and_returns(self, values, rewards, start): lastgaelam = 0 advantages_reversed = [] length = rewards.size(-1) # 反向遍历各个时间步的优势advantage for t in reversed(range(start, length)): # 获取下个时间步的价值估计V_{old}(s_{t+1}) nextvalues = values[:, t + 1] if t \u0026lt; length - 1 else 0.0 # 计算单步TD-error delta = rewards[:, t] + self.gamma * nextvalues - values[:, t] # 累计优势 lastgaelam = delta + self.gamma * self.lam * lastgaelam # 存储各个时间步的优势 advantages_reversed.append(lastgaelam) # 对逆序的优势列表进行正序处理，得到正常时间步排列的优势 advantages = torch.stack(advantages_reversed[::-1], dim=1) # (seq_bs, max_seq_len - 1 - start) # return_t = adv_t + v(s_t) # 通过优势计算得到回报 returns = advantages + values[:, start:] # (bs, max_seq_len - 1 - start) return advantages.detach(), returns 7. train_rlhf函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def train_rlhf(self, inputs): # inputs为一个ppo_batch的generate_experience函数返回值 prompts = inputs[\u0026#39;prompts\u0026#39;] log_probs = inputs[\u0026#39;logprobs\u0026#39;] ref_log_probs = inputs[\u0026#39;ref_logprobs\u0026#39;] reward_score = inputs[\u0026#39;rewards\u0026#39;] values = inputs[\u0026#39;value\u0026#39;] attention_mask = inputs[\u0026#39;attention_mask\u0026#39;] seq = inputs[\u0026#39;input_ids\u0026#39;] start = prompts.size()[-1] - 1 # max_prompt_len - 1 action_mask = attention_mask[:, 1:] # (ppo_bs, max_seq_len - 1) # 利用经验池中旧的logprobs, ref_logprobs以及reward_score计算KL-reward，并利用KL-reward和旧的values计算advantages和returns old_values = values # (ppo_bs, max_seq_len - 1) with torch.no_grad(): # old_rewards (ppo_bs, max_seq_len - 1) old_rewards = self.compute_rewards(prompts, log_probs, ref_log_probs, reward_score, action_mask) ends = start + action_mask[:, start:].sum(1) + 1 # 将reward和value中padding部分的值置零不然advantage和return计算会出错 for i in range(old_rewards.shape[0]): old_rewards[i, ends[i]:] = 0 old_values[i, end[i]:] = 0 # advantages (ppo_bs, max_seq_len - max_prompt_len) # returns (ppo_bs, max_seq_len - max_prompt_len) advantages, returns = self.get_advantages_and_returns(old_values, old_rewards, start) batch = {\u0026#39;input_ids\u0026#39;: seq, \u0026#39;attention_mask\u0026#39;: attention_mask} # 利用当前最新actor model计算最新logprob，计算actor_loss并更新actor model参数 actor_prob = self.actor_model(**batch, use_cache=False).logits actor_log_prob = gather_log_probs(actor_prob[:, :-1, :], seq[:, 1:]) actor_loss = self.actor_loss_fn(actor_log_prob[:, start:], log_probs[:, start:], advantages, action_mask[:, start:]) self.actor_model.backward(actor_loss) self.actor_model.step() # 利用当前最新critic model计算最新value，计算critic_loss并更新critic model参数，完成一个ppo batch数据的训练 value = self.critic_model.forward_value(**batch, return_value_only=True, use_cache=False)[:, :-1] critic_loss = self.critic_loss_fn(value[:, start:], old_values[:, start:], returns, action_mask[:, start:]) self.critic_model.backward(critic_loss) self.critic_model.step() return actor_loss, critic_loss PPO训练数据管理-MiniDataset /DeepSpeed-Chat/dschat/utils/data/data_utils.py\nMiniDataset是一个进一步划分ppo训练时数据的一个类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class MiniDataset: def __init__(self, max_size, small_batch_size): # max_size为进行划分ppo训练数据时的normal batch容量，比如max_size=2，batch=4则意味着当dataset中含有两个batch（8条数据）时，开始划分ppo batch。 # small_batch_size为ppo训练时的batch大小，即ppo_batch self.dataset = [] self.max_size = max_size self.small_batch_size = small_batch_size def seperate(self): # 当self.dataset长度达到max_size时，开始划分ppo_batch # 假设max_size=2, small_batch_size=3, normal batch_size=4 # 划分前self.dataset=[[d0,d1,d2,d3], [d4,d5,d6,d7]]，划分后的small_dataset应该为[[d0,d1,d2], [d3], [d4,d5,d5], [d7]] small_dataset = [] for large_batch in self.dataset: if type(large_batch) == list or type(large_batch) == tuple: # large_size即normal batch size large_size = len(large_batch[0]) elif type(large_batch) == dict: large_size = len(large_batch[list(large_batch.keys())[0]]) else: large_size = len(large_batch) for i in range(0, large_size, self.small_batch_size): if type(large_batch) == list or type(large_batch) == tuple: small_dataset.append([x[i:i + self.small_batch_size] for x in large_batch]) elif type(large_batch) == dict: small_dataset.append({k: v[i:i + self.small_batch_size] for k, v in large_batch.items()}) else: small_dataset.append(large_batch[i: i + self.small_batch_size]) self.free() return small_dataset def add(self, data): if len(self.dataset) \u0026lt; self.max_size: self.dataset.append(data) if len(self.dataset) == self.max_size: return self.seperate() else: return None else: raise ValueError(\u0026#39;xx\u0026#39;) def free(self): self.dataset = [] main.py中训练主循环 DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py\n不考虑unsupervised数据，记录rlhf训练主函数循环流程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 exp_mini_dataset = MiniDataset(args.generation_batches, args.per_device_training_batch_size) for epoch in range(args.num_train_epochs): for step, batch_prompt in enumerate(prompt_train_dataloader): batch_prompt = to_device(batch_prompt, device) # 计算当前batch prompt的经验数据 out = trainer.generate_experience(batch_prompt[\u0026#39;prompt\u0026#39;], batch_prompt[\u0026#39;prompt_att_mask\u0026#39;], step) # 添加当前批经验数据，达到args.generation_batches时划分成ppo_batch数据进行训练，否则继续添加 exp_dataset = exp_mini_dataset.add(out) if exp_dataset is not None: inner_iter = 0 actor_loss_sum, critic_loss_sum = 0, 0 average_reward = 0 if ppo_ep in range(args.ppo_epochs): for i, exp_data in enumerate(exp_dataset): actor_loss, critic_loss = trainer.train_rlhf(exp_data) actor_loss_sum += actor_loss.item() critic_loss_sum += critic_loss.item() average_reward += exp_data[\u0026#39;rewards\u0026#39;].mean() inner_iter += 1 random.shuffle(exp_dataset) print_rank_0(f\u0026#34;{epoch} | {step} | {ppo_ep+1} | {actor_loss_sum / inner_iter} | {critic_loss_sum / inner_iter}\u0026#34;) 总结就是每args.generation_batches个batch数据使用当前{actor, ref, critic, reward}模型生成一批经验数据，这批经验数据构建ppo_batch训练数据开始进行args.ppo_epochs轮训练，期间每个ppo_epoch的每个inner_iter对{actor, critic}模型做一步参数更新，每次完成当前经验数据全部ppo_epochs训练后打印平均{actor_loss, critic_loss, average_reward}。直到训练完prompt_dataloader中的prompt数据结束一个大epoch，基于此循环args.num_train_epochs次。\nReferences [1] InstructGPT高效实践——【DeepSpeed-Chat】源码详解(2/3)：Supervised Finetuning、Reward Model Finetuning\n[2] InstructGPT高效实践——【DeepSpeed-Chat】源码详解(3/3)：RLHF Finetuning\n","permalink":"https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/","summary":"\u003cp\u003e本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。\u003c/p\u003e\n\u003ch3 id=\"supervised-finetuning\"\u003eSupervised finetuning\u003c/h3\u003e\n\u003cp\u003e首先在主节点克隆\u003ca href=\"https://github.com/microsoft/DeepSpeedExamples\" class=\"entityLink\"\u003edeepspeed-chat\u003c/a\u003e仓库。\u003c/p\u003e\n\u003cp\u003e使用的主要环境：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install \u003cspan class=\"nv\"\u003etorch\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e1.13.0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install datasets\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install sentencepiece\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install \u003cspan class=\"nv\"\u003eprotobuf\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e3.20.3\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install accelerate\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install \u003cspan class=\"nv\"\u003edeepspeed\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e0.10.0\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install \u003cspan class=\"nv\"\u003etransformers\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e4.44.2\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install tensorboard\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epip install \u003cspan class=\"nv\"\u003enumpy\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e1.26.4\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003edeepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt update\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt install nvidia-cuda-toolkit\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e之后先跑通单节点，我用的是\u003ccode\u003estep1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh\u003c/code\u003e，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。\u003c/p\u003e\n\u003cp\u003e跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了\u003ccode\u003esynthetic-instruct-gptj-pairwise\u003c/code\u003e数据集，两个文件保存在主节点：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003edatasets\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    └── synthetic-instruct-gptj-pairwise\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        ├── dataset_infos.json\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        └── train-00000-of-00001-1e5d57b93c448e7a.parquet\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e在\u003ccode\u003edschat/utils/data/raw_datasets.py\u003c/code\u003e的数据集类\u003ccode\u003ePromptRawDataset\u003c/code\u003e上也做了对应修改:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e9\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003ePromptRawDataset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003eobject\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eoutput_path\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eseed\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003elocal_rank\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edataset_name\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoutput_path\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eoutput_path\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eseed\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eseed\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal_rank\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elocal_rank\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"s1\"\u003e\u0026#39;\u0026#39;\u0026#39;原始数据的读取，这里根据自己数据集作相应修改\u0026#39;\u0026#39;\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eraw_datasets\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eload_dataset\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;parquet\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edata_files\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edataset_name\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003e到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是\u003ccode\u003eFusedAdam\u003c/code\u003e，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成\u003ccode\u003eAdamW\u003c/code\u003e就跑通了。查了一下\u003ccode\u003eFusedAdam\u003c/code\u003e在需要大量计算资源的场景下有一定优势。\u003c/p\u003e","title":"Deepspeed多机多卡训练\u0026代码细节"},{"content":" 1. DPO Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}+\\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：\n$$ \\mathcal L_{DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})] $$ 2. Simple-DPO Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：\n$$ p_\\theta(y|x)=\\frac{1}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 因此Simple-DPO考虑将奖励函数表达式改为：\n$$ r_{SimPO}(x, y)=\\frac{\\beta}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\\gamma \\ (\\gamma\u0026gt;0)$，表达式如下：\n$$ p(y_w\u003ey_l|x)=\\sigma(r(x,y_w)-r(x,y_l)-\\gamma) $$ 从而，Simple-DPO的优化函数： $$ \\mathcal L_{SimPO}(\\pi_\\theta)=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma(\\frac{\\beta}{|y_w|}log\\ \\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}log\\ \\pi_\\theta(y_l|x)-\\gamma)] $$ 3. KTO KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。\n前景理论（prospect theory），Tversky \u0026amp; Kahneman用下面方程建模人类价值： $$ v(z,z_{ref};\\alpha,\\lambda) = \\begin{cases} (z-z_{ref})^\\alpha \u0026amp; \\text{if } z \\ge z_{ref} \\\\ -\\lambda(z_{ref}-z)^\\alpha \u0026amp; \\text{if } z \u0026lt; z_{ref} \\end{cases} $$ 价值函数$v:z\\rightarrow R$将一个输出$z$想对一个参考值$z_{ref}$映射到其感知价值，反映人类相比起相同大小回报，对损失的敏感性更大，其中$\\alpha$控制价值变化速度，$\\lambda$反应对损失的敏感程度。\n基于上述效用方程，作者做了一定修改使其更适合模型训练，损失函数如下：\n$$ \\mathcal L_{KTO}(\\pi_\\theta,\\pi_{ref})=\\mathbb E_{x,y\\sim\\mathcal D}[w(y)(1-v_{KTO}(x, y;\\beta))] $$ 其中 $$ r_{KTO}(x,y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\\\\\\\ z_{ref}=\\mathbb E_{x^\\prime\\sim\\mathcal D}[\\beta KL(\\pi_\\theta(y^\\prime|x^\\prime)||\\pi_{ref}(y^\\prime|x^\\prime))]\\\\\\\\ v_{KTO}(x)=\\begin{cases} \\sigma(r_{KTO}(x,y)-z_{ref})\u0026 if\\ y\\sim y_{desirable}|x\\\\ \\sigma(z_{ref}-r_{KTO}(x, y))\u0026 if\\ y\\sim y_{undesirable}|x \\end{cases}\\\\\\\\ w(y)=\\begin{cases} \\lambda_D\u0026 if\\ y\\sim y_{desirable}|x\\\\ \\lambda_U\u0026 if\\ y\\sim y_{undesirable}|x \\end{cases} $$ 4. Step-DPO Step-DPO (Lai et al. (2024))主要解决大模型处理数学这类需要较长reasoning过程效果不佳的问题，作者指出用传统DPO训练数学类偏好数据存在一定缺陷，即数学类问题的reasoning过程往往是在中间的一些step开始出现错误，而前面的step推理是正确的，因此直接将整条数据归类为win或者lose都不合适。基于此，作者提出Step-DPO方法，即在initial steps之后对数据划分成win和lose，具体损失函数如下：\n$$ \\mathcal L_{Step-DPO}(\\theta)=-\\mathbb E_{x,s_{1\\sim k-1},s_{win},s_{lose}\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(s_{win}|x;s_{1\\sim k-1})}{\\pi_{ref}(s_{win}|x;s_{1\\sim k-1})})-\\beta log\\frac{\\pi_\\theta(s_{lose}|x;s_{1\\sim k-1})}{\\pi_{ref}(s_{lose}|x;s_{1\\sim k-1})}] $$ 此外本文提出了一种偏好数据构建流水线，来获取偏好数据${x,s_{1\\sim k-1},s_{win},s_{lose}}$，具体流程如下：\nStep1: 收集数学问题$x$和相关答案$\\hat y$的数据集$D_0={(x,\\hat y)}$，用模型$\\pi_{ref}$使用问题$x$进行推理（添加CoT），得到每条问题的推理过程以及最终答案，从所有数据中挑选最终答案出错的数据得到子数据集$D_1={(x,\\hat y, y)}$，其中$y$为对应的错误答案的generations。\nStep2: 针对数据集$D_1$，其中$y=s1,s2,\\dots,s_n$，可以拆分成一系列推理步骤，通过人工或者GPT4逐条检测这些推理步骤直至发现推理出错的那一步$k$，将$s_k$作为$s_{lose}$，从而得到子数据集$D_2={(x,\\hat y,s_{1\\sim k-1},s_{lose})|x\\in D_1}$。\nStep3: 为了获取正确的推理步骤，通过$D_2$中的数据，使用$\\pi_{ref}$模型基于问题$x$和前面$k-1$步正确推理作为prompt进行inference得到$y_{cont}$: $$ y_{cont}\\sim\\pi_{ref}(y|x;s_{1\\sim k-1}) $$ 从得到正确答案的$y_{cont}$中，选择其中的第一个推理步骤作为$s_{win}$，得到最终的数据集$D_{final}={(x,s_{1\\sim k-1},s_{lose},s_{win})|x\\in D_2}$。\n需要注意的一点是，在Step3中，可能出现最终答案正确但是中间推理步骤错误的情况，因此这里需要人工或者GPT4对$s_{win}$做筛选。\n5. ORPO Hong et al. (2024)考虑到SFT不能预防模型生成lose response，同时考虑使用偏好对齐如DPO等方法需要reference model开销较大且流程繁琐，因此本文提出了一种在SFT方法上增加一个能兼顾DPO这类偏好对齐的损失，具体做法：\n对于一条训练数据${x, y}$，其对数似然概率：\n$$ logP_\\theta(y|x)=\\frac{1}{m}\\sum_{t=1}^mlog\\ P_\\theta(y_t|x,y_{\u003c t}) $$ 定义Odds Ratio $odds_\\theta(y|x)=\\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}$表示模型生成$y$相比不生成$y$的概率倍数，基于此定义$OR_\\theta(y_w, y_l)$： $$ OR_\\theta(y_w,y_l)=\\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)} $$ 表示模型在给定$x$条件下相比于生成$y_l$，更可能生成$y_w$的程度。最终改良后的SFT损失函数： $$ \\mathcal L_{ORPO}=\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[\\mathcal L_{SFT}+\\lambda\\cdot\\mathcal L_{OR}]\\\\\\\\ \\mathcal L_{OR}=-log\\ \\sigma(log\\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)}) $$ 6. R-DPO Park et al. (2024)考虑DPO在长度控制上的不足：容易生成过长啰嗦的文本，考虑在DPO的损失中引入对长度的约束，具体损失函数如下：\n$$ \\mathcal L_{R-DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma (\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})+\\alpha|y_w|-\\alpha|y_l|] $$ 7. CPO Xu et al. (2024)简化DPO (Rafailov et al. (2023))损失函数，将$\\pi_{ref}$用均匀分布替代，并在偏好对齐中再次引入SFT损失约束，具体损失函数如下：\n$$ \\mathcal L_{CPO}=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[ log\\ \\sigma(\\beta log\\ \\pi_\\theta(y_w|x)-\\beta log\\ \\pi_\\theta(y_l|x))+\\lambda\\ log\\ \\pi_\\theta(y_w|x)] $$ 8. sDPO Kim et al. (2024)提出了一个简单有效的DPO改进方案，作者发现DPO的损失优化存在理论下界\n$$ \\begin{align*} \\mathcal L_{DPO}(\\pi_\\theta, \\pi_{ref}) \u0026=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D} [log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]\\\\ \u0026=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D }[log\\ \\sigma(\\beta\\cdot(\\gamma_{\\pi_\\theta}(x, y_w, y_l) - \\gamma_{\\pi_{ref}}(x, y_w, y_l)))] \\end{align*} $$ 其中$\\gamma_\\pi(x, y_w, y_l)=log\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}$，即正例和负例句子的对数似然差值，理论上，优化DPO损失的过程最终导致$\\gamma_{\\pi_\\theta}\u003e\\gamma_{\\pi_{ref}}$。因此$\\gamma_{\\pi_{ref}}$可以看作reference model的下界。基于该motivation，作者做了初步验证试验，即使用不同的模型作为reference model进行DPO训练，发现reference model越强，最后训练出来的$\\pi_\\theta$性能越好，从而在实验上验证这一点。基于此作者提出step-wise DPO的方法，即对一批偏好数据，不要一次性训练完，而是将数据做切分，每次用一小批数据训练迭代一版模型，下一次训练的时候用上一轮训练好的$\\pi_\\theta$作为新的reference model训练，这样可以不断提高策略模型的能力下届。 关于偏好数据的切分上，作者提出的思路是，按照偏好数据对的偏好强弱程度划分（即训练的难易程度，类比课程学习，由易到难的学习），因此作者思路是使用一个extra reward model(文章用的应该是最开始的reference model，就是sft后的model)对所有偏好数据对打分（这里感觉是计算似然，毕竟如果是sft后的model也不是reward model），计算每个子数据集的预测准确率（即chosen的分数高于rejected的比例），文章DPO数据集是涵盖很多不同source的，所以这里直接按照不同的source进行划分。最终就是预测准确率最高的（即最容易区分的）先训练，由易到难。\n我个人想法是如果不是想文章中使用多个source的偏好数据，而是只有一个source的话，划分数据的方式可以改为使用一个extra reward model对每条偏好数据对打分，按照(chosen_score - rejected_score)的值由高到低进行排序，然后再划分成不同子数据集，也是一种比较直观的想法吧。总之这篇工作给人感觉确实是简单有效。\nReferences [1] Rafailov et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model” NeurIPS 2023.\n[2] Meng et al. “SimPO: Simple Preference Optimization with a Reference-Free Reward” arXiv preprint arXiv:2405.14734 (2024).\n[3] Ethayarajh et al. “KTO: Model Alignment as Prospect Theoretic Optimization” arXiv preprint arXiv:2402.01306 (2024).\n[4] Lai et al. “STEP-DPO: STEP-WISE PREFERENCE OPTIMIZATION FOR LONG-CHAIN REASONING OF LLMS” arXiv preprint arXiv:2406.18629 (2024).\n[5] Hong et al. “ORPO: Monolithic Preference Optimization without Reference Model” arXiv preprint arXiv:2403.07691 (2024).\n[6] Park et al. “Disentangling Length from Quality in Direct Preference Optimization” arXiv preprint arXiv:2403.19159 (2024).\n[7] Xu et al. “Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation” arXiv preprint arXiv:2401.08417 (2024).\n[8] Kim et al. “sDPO: Don’t Use Your Data All at Once” arXiv preprint axXiv:2403.19270 (2024).\n","permalink":"https://tqzhong.github.io/my-blog/posts/llm-post-training/","summary":"\u003c!--tips:--\u003e\n\u003c!--公式块里，如果加了class=scroll-container(滚轮滑块防止单行公式太长)，大小于号注意要与后面的字符隔开一个空格，否则无法正常编译--\u003e\n\u003c!--常规公式块里，换行要用\\\\\\\\而不是\\\\否则无法正常编译，但是在{cases}环境里，换行用\\\\即可（目前发现这个，后续有其他再补充）--\u003e\n\u003ch3 id=\"1-dpo\"\u003e1. DPO\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html\" class=\"entityLink\"\u003eRafailov et al. (2023)\u003c/a\u003e基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}+\\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\mathcal L_{DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]\n$$\n\u003c/div\u003e\n\u003ch3 id=\"2-simple-dpo\"\u003e2. Simple-DPO\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2405.14734\" class=\"entityLink\"\u003eMeng et al. (2024)\u003c/a\u003e考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\np_\\theta(y|x)=\\frac{1}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i})\n$$\n\u003c/div\u003e\n\u003cp\u003e因此Simple-DPO考虑将奖励函数表达式改为：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\nr_{SimPO}(x, y)=\\frac{\\beta}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i})\n$$\n\u003c/div\u003e\n\u003cp\u003e此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\\gamma \\ (\\gamma\u0026gt;0)$，表达式如下：\u003c/p\u003e\n\u003cdiv class=\"scroll-container\"\u003e\n$$\np(y_w\u003ey_l|x)=\\sigma(r(x,y_w)-r(x,y_l)-\\gamma)\n$$\n\u003c/div\u003e\n从而，Simple-DPO的优化函数：\n\u003cdiv class=\"scroll-container\"\u003e\n$$\n\\mathcal L_{SimPO}(\\pi_\\theta)=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma(\\frac{\\beta}{|y_w|}log\\ \\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}log\\ \\pi_\\theta(y_l|x)-\\gamma)]\n$$\n\u003c/div\u003e\n\u003ch3 id=\"3-kto\"\u003e3. KTO\u003c/h3\u003e\n\u003cp\u003eKTO loss (\u003ca href=\"https://arxiv.org/abs/2402.01306\" class=\"entityLink\"\u003eEthayarajh et al. (2024)\u003c/a\u003e)与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。\u003c/p\u003e","title":"大模型post-training方法"}]