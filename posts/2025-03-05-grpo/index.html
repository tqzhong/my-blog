<!DOCTYPE html>
<html lang="en" dir="auto">
<a href="https://yourpersonalwebsite.com" target="_blank">

<head>
    <script src="https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/@waline/client@v3/dist/waline.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js"></script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GRPO From Scratch | Rs&#39; Log</title>
<meta name="keywords" content="AI, NLP, LLM, RL">
<meta name="description" content="简介
本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。
GRPO
GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。

$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[ 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 &#43; \epsilon \right) A_i 
\right] 
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1&#43;\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$


$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} 
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1,
$$


$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$

GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。">
<meta name="author" content="Rs">
<link rel="canonical" href="https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link crossorigin="anonymous" href="/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">


<script defer crossorigin="anonymous" src="/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" type="image/x-icon" sizes="48x48" href="images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
<link rel="icon" sizes="512x512" href="images/android-chrome-512x512.png" type="image/png">
<link rel="icon" sizes="192x192" href="images/android-chrome-192x192.png" type="image/png">
<link rel="mask-icon" href="https://tqzhong.github.io/my-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/my-blog/css/custom.css?v=1.7"><meta property="og:title" content="GRPO From Scratch" />
<meta property="og:description" content="简介
本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。
GRPO
GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。

$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[ 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 &#43; \epsilon \right) A_i 
\right] 
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1&#43;\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$


$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} 
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1,
$$


$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$

GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GRPO From Scratch"/>
<meta name="twitter:description" content="简介
本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。
GRPO
GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。

$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[ 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 &#43; \epsilon \right) A_i 
\right] 
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1&#43;\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$


$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} 
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1,
$$


$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$

GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tqzhong.github.io/my-blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "GRPO From Scratch",
      "item": "https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GRPO From Scratch",
  "name": "GRPO From Scratch",
  "description": "简介 本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。\nGRPO GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right] - \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\theta} \\| \\pi_{\\text{ref}}] \\right\\}\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G\\frac{1}{\\vert o_i\\vert}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\} \\end{align*} $$ $$ D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - 1, $$ $$ \\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}. $$ GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\\mathbb D_{KL}(q\\Vert p)=r-1-\\log r$，其中$r=\\log\\frac{p(x)}{q(x)}$，该估计相比$-\\log r$具有更小的方差，比$\\frac{1}{2}(\\log r)^2$具有更小的偏差（无偏）。\n",
  "keywords": [
    "AI", "NLP", "LLM", "RL"
  ],
  "articleBody": "简介 本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。\nGRPO GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。\n$$ \\begin{align*} \\mathcal{J}_{\\text{GRPO}}(\\theta) \u0026= \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O|q)\\right]\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G \\left\\{ \\min \\left[ \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)} A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i | q)}{\\pi_{\\theta_{\\text{old}}}(o_i | q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right] - \\beta \\mathbb D_{\\text{KL}}[\\pi_{\\theta} \\| \\pi_{\\text{ref}}] \\right\\}\\\\ \u0026=\\frac{1}{G} \\sum_{i=1}^G\\frac{1}{\\vert o_i\\vert}\\sum_{t=1}^{\\vert o_i\\vert}\\left\\{\\min\\left[\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q, o_{i,\u003c t})}\\hat A_{i,t},\\ \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}\\vert q,o_{i,\u003c t})}{\\pi_{\\theta_{old}}(o_{i,t}\\vert q,o_{i, \u003c t})},1-\\epsilon,1+\\epsilon\\right)\\hat A_{i,t}\\right] - \\beta\\mathbb D_{KL}[\\pi_\\theta\\Vert\\pi_{\\text{ref}}]\\right\\} \\end{align*} $$ $$ D_{\\text{KL}}(\\pi_{\\theta} \\| \\pi_{\\text{ref}}) = \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i, t} | q, o_{i, \u003c t})}{\\pi_{\\theta}(o_{i, t} | q, o_{i, \u003c t})} - 1, $$ $$ \\hat A_{i,t}=A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\cdots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\cdots, r_G\\})}. $$ GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\\mathbb D_{KL}(q\\Vert p)=r-1-\\log r$，其中$r=\\log\\frac{p(x)}{q(x)}$，该估计相比$-\\log r$具有更小的方差，比$\\frac{1}{2}(\\log r)^2$具有更小的偏差（无偏）。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 import random import copy import re import os import numpy as np import wandb import torch import pdb import torch.nn as nn from torch.nn.utils.rnn import pad_sequence from transformers import AutoModelForCausalLM, AutoTokenizer from datasets import load_dataset, load_from_disk from tqdm import tqdm def set_random_seed(seed: int=42): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False set_random_seed(42) os.environ['WANDB_API_KEY'] = \"YOUR_API_KEY\" os.environ['WANDB_PROJECT'] = \"GRPO-Qwen-2.5-1.5B-Instruct\" # 设置系统prompt SYSTEM_PROMPT = \"\"\" Respond in the following format: ... ... \"\"\" def extract_answer_from_model_output(text): \"\"\" Extracts the value from the last tag in the text. Args: text (str): The model-generated text containing XML-style tags. Returns: str or None: The content inside the tags, or None if no valid answer is found. Explanation: 1. Splits the text on the tag to isolate content after the tag. 2. Checks if at least one tag exists in the text. 3. For the last segment: - Verifies it contains a closing tag. - Extracts only the content between the tags. 4. Returns None if the answer is empty (just \"...\") or if tags are missing. \"\"\" parts = text.split('') if len(parts) \u003c 2: # No tag found return None last_part = parts[-1] if '' not in last_part: return None answer = last_part.split('')[0].strip() return None if answer == \"...\" else answer def extract_answer_from_dataset(text): \"\"\" Extracts the answer from the GSM8K dataset examples. Args: text (str): The dataset example text containing a question and answer. Returns: str or None: The extracted answer part after the '####' delimiter, or None if not found. Explanation: 1. Checks if the text contains the '####' delimiter that separates question from answer. 2. If found, splits the text at this delimiter and returns the second part (the answer). 3. The answer is stripped of leading/trailing whitespace. 4. Returns None if no delimiter is present. \"\"\" if \"####\" not in text: return None return text.split(\"####\")[1].strip() def prepare_dataset(split=\"train\"): \"\"\" Load and prepare the GSM8K dataset for training with string prompts. Args: split (str): The dataset split to load (\"train\" or \"test\"). Defaults to \"train\". Returns: list: A list of formatted examples, each containing a prompt string and answer. Explanation: 1. Loads the GSM8K dataset from the Hugging Face datasets hub. 2. For each example in the dataset: - Creates a list of messages with system prompt and the question. - Converts this list into a single string prompt using build_prompt(). - Extracts the answer from the dataset example. - Creates a formatted example dictionary with prompt and answer. 3. Returns the list of formatted examples ready for model training or evaluation. \"\"\" # 从本地加载，服务器端连接不上huggingface，使用train部分的数据 data = load_from_disk('/data/ztq147/gsm8k')[split] # data = load_dataset('openai/gsm8k', 'main')[split] # 一个formatted数据包含“prompt”和“answer”，其中“prompt”格式为SYSTEM_PROMPT\\n QUESTION；“answer”格式为ANSWER formatted_data = [] for example in data: prompt_str = build_prompt( [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": example['question']} ] ) formatted_example = { \"prompt\": prompt_str, \"answer\": extract_answer_from_dataset(example[\"answer\"]) } formatted_data.append(formatted_example) return formatted_data def build_prompt(messages): \"\"\" Build a single prompt string from a list of messages. Args: messages (list): A list of message dictionaries, each with 'role' and 'content' keys. Returns: str: A concatenated string of all message contents. Explanation: 1. Takes a list of message dictionaries in the typical chat format. 2. Extracts the 'content' field from each message and strips whitespace. 3. Joins all content strings with newlines to create a single prompt. 4. This preserves the training format while converting from structured messages to a string. \"\"\" return \"\\n\".join([msg[\"content\"].strip() for msg in messages]) def extract_last_number(text): \"\"\" Extracts the last number appearing in the text. Args: text (str): The text to extract a number from. Returns: float or None: The last number in the text, or None if no number is found. Explanation: 1. Removes dollar signs and percent symbols from the text. 2. Uses regex to find a number that appears at the end of the text (possibly after whitespace). 3. The pattern matches numbers that appear at the end of the string, with or without decimal points. 4. Returns the found number as a float, or None if no match is found. \"\"\" text = text.replace('$', '').replace('%', '') pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$' match = re.search(pattern, text) return float(match.group(1)) if match else None def extract_single_number(text): \"\"\" Extracts the last number appearing in the text. Args: text (str): The text to extract a number from. Returns: float or None: The last number in the text, or None if no number is found. Explanation: 1. Removes dollar signs and percent symbols from the text. 2. Uses regex to find a number that appears at the end of the text (possibly after whitespace). 3. The pattern matches numbers that appear at the end of the string, with or without decimal points. 4. Returns the found number as a float, or None if no match is found. \"\"\" text = text.replace('$', '').replace('%', '') pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$' match = re.search(pattern, text) return float(match.group(1)) if match else None def evaluate_model(model, tokenizer, eval_examples, device): \"\"\" Evaluates the model on a set of examples and prints detailed results. Args: model: The language model to evaluate. tokenizer: The tokenizer for encoding inputs and decoding outputs. eval_examples (list): List of evaluation examples, each containing \"prompt\" and \"answer\". device: The device (CPU or GPU) to run evaluation on. Returns: float: The accuracy percentage (correct predictions / total examples * 100). Explanation: 1. Sets the model to evaluation mode. 2. For each example in the evaluation set: - Encodes the prompt and generates a response using the model. - Extracts the predicted answer from the generated response. - Compares the predicted answer with the expected answer using multiple methods: a. Exact string matching b. Single number extraction and comparison c. Last number extraction and comparison - Prints detailed information about each example. 3. Calculates and returns the overall accuracy. 4. Returns the model to training mode. \"\"\" model.eval() correct = 0 total = len(eval_examples) print(\"\\n\" + \"=\"*50) print(\"EVALUATION ON\", total, \"EXAMPLES\") print(\"=\"*50) for example in eval_examples: full_prompt = example[\"prompt\"] expected = example[\"answer\"] inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device) # early_stopping=False，表示模型会一直生直到达到最大新标记数（max_new_tokens）。 # forced_eos_token_id=tokenizer.eos_token_id，当生成到最大长度时，默认将最后一个token换成eos_token_id。 with torch.no_grad(): outputs = model.generate( inputs, max_new_tokens=512, temperature=0.7, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, forced_eos_token_id=tokenizer.eos_token_id, early_stopping=False, ) response = tokenizer.decode(outputs[0], skip_special_tokens=True) # 关于数学类问题评估的方法，这里不太了解，之前没做过 try: predicted = extract_answer_from_model_output(response) if predicted == expected: is_correct = True else: pred_num = extract_single_number(str(predicted)) exp_num = extract_single_number(str(expected)) if pred_num is not None and exp_num is not None and pred_num == exp_num: is_correct = True else: pred_num = extract_last_number(str(predicted)) exp_num = extract_last_number(str(expected)) is_correct = (pred_num is not None and exp_num is not None and pred_num == exp_num) if is_correct: correct += 1 print(\"\\nPrompt:\") print(full_prompt) print(\"\\nExpected Answer:\") print(expected) print(\"\\nExtracted Answer:\") print(predicted) print(\"\\nFull Generated Response:\") print(response) print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\") print(\"-\"*50) except Exception as e: print(\"\\nFailed to parse model output for prompt:\") print(full_prompt) print(\"Error:\", e) print(\"-\"*50) accuracy = (correct / total) * 100 print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\") print(\"=\"*50) model.train() return accuracy def correctness_reward(prompts, completions, answer, **kwargs): \"\"\" Assigns a reward based on the correctness of the model's answer. Args: prompts (list): List of input prompts. completions (list): List of model completions, each containing content. answer (list): List of expected answers. **kwargs: Additional keyword arguments. Returns: list: List of numerical rewards for each completion. Explanation: 1. Extracts the content from each completion. 2. Extracts the answer portion from each response using extract_answer_from_model_output. 3. Assigns rewards based on matching criteria: - 2.0 points for an exact match - 1.5 points for numeric equivalence (when values match but format differs) - 0.0 points for incorrect answers 4. Tracks completion lengths for analysis. \"\"\" responses = [completion[0]['content'] for completion in completions] extracted = [extract_answer_from_model_output(r) for r in responses] rewards = [] for r, a in zip(extracted, answer): if r == a: # Exact match case rewards.append(2.0) else: r_num = extract_single_number(str(r)) a_num = extract_single_number(str(a)) if r_num is not None and a_num is not None and r_num == a_num: rewards.append(1.5) else: rewards.append(0.0) completion_lengths = [len(response.split()) for response in responses] return rewards def format_reward(completions, **kwargs): \"\"\" Assigns a reward for adhering to the desired XML format. Args: completions (list): List of model completions, each containing content. **kwargs: Additional keyword arguments. Returns: list: List of format compliance scores for each completion. Explanation: 1. Extracts the content from each completion. 2. Evaluates format compliance by checking for required XML tags: - 0.2 points for each tag present (, , , ) - Maximum score of 0.8 for perfect format compliance 3. Stores and returns the format compliance scores. \"\"\" responses = [completion[0]['content'] for completion in completions] rewards = [] format_scores = [] for response in responses: score = 0.0 if \"\" in response: score += 0.2 if \"\" in response: score += 0.2 if \"\" in response: score += 0.2 if \"\" in response: score += 0.2 rewards.append(score) format_scores.append(score) return rewards def combined_reward(prompts, completions, answer): \"\"\" Combines correctness and format rewards. Args: prompts (list[str]): List of prompt texts completions (list[list[dict]]): List of completion dictionaries answer (list[str]): List of expected answers Returns: list[float]: Combined rewards for each prompt-completion pair Explanation: 1. Calculates separate rewards for correctness and format compliance. 2. Combines the rewards with the following weights: - Correctness score range: 0.0 to 2.0 - Format score range: 0.0 to 0.8 - Total possible range: 0.0 to 2.8 3. Returns the combined reward for each example. \"\"\" # Get individual rewards correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer) format_scores = format_reward(completions=completions) # Combine rewards - correctness is weighted more heavily combined_rewards = [] for c_score, f_score in zip(correctness_scores, format_scores): # Correctness score range: 0.0 to 2.0 # Format score range: 0.0 to 0.8 # Total range: 0.0 to 2.8 combined_rewards.append(c_score + f_score) return combined_rewards def selective_log_softmax(logits, input_ids): \"\"\" Computes log probabilities for specific tokens in the vocabulary. Args: logits (torch.Tensor): The raw logits output from the model. input_ids (torch.Tensor): The token IDs for which we want the log probabilities. Returns: torch.Tensor: Log probabilities of the selected tokens. Explanation: 1. Applies log softmax to convert logits to log probabilities over the vocabulary. 2. Uses gather to extract only the log probabilities corresponding to the input_ids. 3. Removes the extra dimension to match the original shape of input_ids. \"\"\" log_probs = nn.functional.log_softmax(logits, dim=-1) return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) def compute_log_probs(model, input_ids, attention_mask, logits_to_keep): \"\"\" Computes the log probabilities for a batch of tokens. Args: model: The language model. input_ids (torch.Tensor): Token IDs for input sequences. attention_mask (torch.Tensor): Attention mask for input sequences. logits_to_keep (int): Number of tokens to keep from the end of the sequence. Returns: torch.Tensor: Log probabilities of the selected tokens. Explanation: 1. Gets logits from the model for the input sequence. 2. Selects logits for all tokens except the last one (as we predict next tokens). 3. Selects only the last 'logits_to_keep' tokens from both logits and input_ids. 4. Computes log probabilities for these tokens using selective_log_softmax. \"\"\" logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :] input_ids = input_ids[:, -logits_to_keep:] # select the generation part, remove the prompt part logits = logits[:, -logits_to_keep:, :] # the same as input_ids return selective_log_softmax(logits, input_ids) def create_completion_mask(completion_ids, eos_token_id): \"\"\" Creates a mask for completion tokens that excludes tokens after the EOS token. Args: completion_ids (torch.Tensor): Token IDs of the generated completions. eos_token_id (int): The ID of the end-of-sequence token. Returns: torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token. Explanation: 1. Identifies positions where EOS tokens occur in each sequence. 2. Finds the index of the first EOS token in each sequence. 3. Creates a mask where positions before and including the first EOS are 1, others are 0. 4. If no EOS token is found in a sequence, all positions are set to 1. \"\"\" is_eos = completion_ids == eos_token_id # shape (bs, max_completion_length) eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device) # 表示每个completion的第一个Eos_token的位置，初始化全为max_completion_length. shape (bs, ) mask_exists = is_eos.any(dim=1) # 返回一个布尔向量 shape (bs, )，表示哪些序列包含至少一个Eos token eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists] # 对于包含Eos token的序列，找到第一个Eos token的位置，is_eos.int().argmax(dim=1)返回每个序列中第一个Eos token的索引。 sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1) # shape (bs, max_completion_length) return (sequence_indices \u003c= eos_idx.unsqueeze(1)).int() def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32): \"\"\" Generates multiple completions for each prompt. Args: model: The language model. tokenizer: The tokenizer for encoding and decoding text. prompts (list): List of text prompts. num_generations (int): Number of completions to generate per prompt. max_completion_length (int): Maximum number of tokens to generate. Returns: tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask. Explanation: 1. Encodes the prompts and moves them to the appropriate device. 2. Repeats each prompt num_generations times to generate multiple completions. 3. Generates completions using the model with specified parameters. 4. Extracts the completion IDs (excluding the prompt tokens). 5. Creates a mask for the completions using create_completion_mask. \"\"\" device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\") prompt_ids = inputs[\"input_ids\"].to(device) prompt_mask = inputs[\"attention_mask\"].to(device) print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\") prompt_length = prompt_ids.size(1) # .repeat_interleave 沿着dim=0重复num_generations次，使得prompt_ids和prompt_mask的维度都增加了num_generations倍 prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0) # shape (bs * num_generations, max_prompt_length) prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0) # shape (bs * num_generations, max_prompt_length) outputs = model.generate( prompt_ids, attention_mask=prompt_mask, max_new_tokens=max_completion_length, do_sample=True, temperature=1.0, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, early_stopping=False ) print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\") # completio_ids只包含模型生成的answer部分，长度为max_completion_length completion_ids = outputs[:, prompt_length:] # shape (bs * num_generations, max_completion_length) completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id) # shape (bs * num_generations, max_completion_length) return prompt_ids, prompt_mask, completion_ids, completion_mask def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length): \"\"\" Generates data for GRPO rollouts including completions and log probabilities. Args: model: The policy model being trained. ref_model: The reference model for KL divergence calculation. tokenizer: The tokenizer for encoding and decoding text. batch_samples (list): Batch of training samples. num_generations (int): Number of completions to generate per sample. max_completion_length (int): Maximum completion length. Returns: dict: Dictionary containing all data needed for GRPO updates. Explanation: 1. Extracts prompts and expected answers from the batch samples. 2. Generates completions using the current policy model. 3. Combines prompt and completion tokens. 4. Computes log probabilities from both the policy model and reference model. 5. Formats completions for reward calculation. 6. Repeats prompts and answers to match the number of generated completions. 7. Returns all data needed for GRPO loss calculation. \"\"\" device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") prompts = [sample[\"prompt\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples] answers = [sample[\"answer\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples] with torch.no_grad(): prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions( model, tokenizer, prompts, num_generations, max_completion_length ) input_ids = torch.cat([prompt_ids, completion_ids], dim=1) # shape (bs * num_generations, max_prompt_length + max_completion_length) attention_mask = torch.cat([prompt_mask, completion_mask], dim=1) # shape (bs * num_generations, max_prompt_length + max_completion_length) logits_to_keep = completion_ids.size(1) # max_completion_length old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep) # shape (bs * num_generations, max_completion_length) ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep) # shape (bs * num_generations, max_completion_length) formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids] repeated_prompts = [p for p in prompts for _ in range(num_generations)] repeated_answers = [a for a in answers for _ in range(num_generations)] return { \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"completion_mask\": completion_mask, \"old_log_probs\": old_log_probs, \"ref_log_probs\": ref_log_probs, \"formatted_completions\": formatted_completions, \"repeated_prompts\": repeated_prompts, \"repeated_answers\": repeated_answers, \"logits_to_keep\": logits_to_keep, \"batch_size\": len(prompts), \"num_generations\": num_generations } def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2): \"\"\" Computes the GRPO loss for updating the policy model. Args: model: The policy model being trained. ref_model: The reference model for KL divergence calculation. rollout_data (dict): Data generated by generate_rollout_data. tokenizer: The tokenizer for encoding and decoding text. reward_function: Function that calculates rewards for completions. beta (float): KL penalty coefficient. epsilon (float): Clipping parameter for PPO. Returns: torch.Tensor: The GRPO loss to be minimized. Explanation: 1. Computes current token log probabilities using the policy model. 2. Calculates the probability ratio between current and old policies. 3. Computes rewards using the provided reward_function. 4. Calculates advantages by standardizing rewards within each prompt. 5. Computes the PPO surrogate objective with clipping. 6. Calculates the KL divergence between reference and policy models. 7. Combines surrogate loss and KL penalty. 8. Averages the loss across all tokens and batches. \"\"\" device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") input_ids = rollout_data[\"input_ids\"] attention_mask = rollout_data[\"attention_mask\"] completion_mask = rollout_data[\"completion_mask\"] logits_to_keep = rollout_data[\"logits_to_keep\"] old_log_probs = rollout_data[\"old_log_probs\"] ref_log_probs = rollout_data[\"ref_log_probs\"] token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep) ratio = torch.exp(token_log_probs - old_log_probs) # shape (bs * num_generations, max_completion_length) rewards = torch.tensor( reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]), dtype=torch.float32, device=device ) # shape (bs * num_generations,) #print(f\"Rewards: {rewards}\") # Debug rewards batch_size = rollout_data[\"batch_size\"] num_generations = rollout_data[\"num_generations\"] rewards = rewards.view(batch_size, num_generations) avg_reward = rewards.mean().item() print(\"Average Reward:\", avg_reward) mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations) # shape (bs * num_generations,) std_rewards = rewards.std(dim=1).repeat_interleave(num_generations) # shape (bs * num_generations,) advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1) # shape (bs * num_generations, 1) surr1 = ratio * advantages # shape (bs * num_generations, max_completion_length) surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages # shape (bs * num_generations, max_completion_length) surrogate_loss = torch.min(surr1, surr2) # shape (bs * num_generations, max_completion_length) kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1 # shape (bs * num_generations, max_completion_length) per_token_loss = surrogate_loss - beta * kl # shape (bs * num_generations, max_completion_length) loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean() # completion_mask用于把生成部分中eos token之后的部分mask掉，只保留生成部分的loss，其中每个completion都会对生成有效长度的loss进行平均，然后再对batch求平均，得到batch的loss。这里添加负号因为GRPO优化目标是最大化改值，这里用梯度下降来优化，所以这里加负号 return loss, avg_reward def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4, num_generations=4, gradient_accumulation_steps=4, max_completion_length=128, beta=0.1, learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None): \"\"\" This function is your original working code (train_with_grpo_static) with an added outer loop for iterative GRPO updates per the pseudocode. Args: model: The language model to train. tokenizer: The tokenizer for encoding and decoding text. train_data (list): Training dataset. num_iterations (int): Number of outer iterations (reference model updates). num_steps (int): Number of batch updates per iteration. batch_size (int): Number of prompts per batch. num_generations (int): Number of completions per prompt. max_completion_length (int): Maximum token length for completions. beta (float): KL penalty coefficient. learning_rate (float): Learning rate for optimizer. mu (int): Number of policy updates per batch. epsilon (float): PPO clipping parameter. reward_function: Function that calculates rewards for completions. device_ids (list): List of GPU device IDs for DataParallel. Returns: The trained model. Explanation: 1. For each outer iteration: - Creates a reference model as a deep copy of the current policy model. - Reinitializes the optimizer for the policy model. - For each training step: a. Samples a batch of examples from the training data. b. Generates rollout data including completions and log probabilities. c. For mu iterations: i. Computes the GRPO loss. ii. Updates the policy model using gradient descent. - Monitors GPU memory usage and prints progress information. \"\"\" # assert device_ids is not None and len(device_ids) \u003e 1 # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # model = nn.DataParallel(model, device_ids=device_ids).cuda() # print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\") # num_iterations表示ref_model迭代次数 for iteration in range(num_iterations): print(f\"\\nIteration {iteration+1}/{num_iterations}\") # Create a reference model (deep copy) and set it to eval mode. ref_model = copy.deepcopy(model) ref_model.eval() for param in ref_model.parameters(): param.requires_grad = False print(\"Reference model created.\") optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) model.train() # pdb.set_trace() # Inner loop: your original training steps. for step in tqdm(range(num_steps)): batch_samples = random.sample(train_data, batch_size) with torch.no_grad(): # 生成经验数据 rollout_data = generate_rollout_data( model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length, ) # 对每批经验池数据学习的次数，一般设置mu=1 for grpo_iter in range(mu): loss, avg_reward = grpo_loss( model, ref_model, rollout_data, tokenizer, reward_function, beta=beta, epsilon=epsilon, ) loss.backward() if (step + 1) % gradient_accumulation_steps == 0: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1) optimizer.step() optimizer.zero_grad() wandb.log({ \"loss\": loss.item(), \"average_reward\": avg_reward, \"iteration\": iteration + 1, \"step\": step + 1, \"grpo_iter\": grpo_iter + 1 }) print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \" f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\") return model def optimize_model_memory(model): \"\"\" Optimizes the model to use less memory during training. Args: model: The language model to optimize. Returns: The optimized model. Explanation: 1. Sets the model to training mode. 2. Disables KV caching to save memory. 3. Enables gradient checkpointing to trade computation for memory. 4. Ensures that input embeddings require gradients: - Either uses the built-in method if available. - Or adds a forward hook to the input embeddings layer. 5. Returns the optimized model ready for memory-efficient training. \"\"\" model.train() model.config.use_cache = False # 不使用kv-cache缓存，减小显存消耗 # First ensure inputs will require gradients if hasattr(model, \"enable_input_require_grads\"): model.enable_input_require_grads() else: def make_inputs_require_grad(module, input, output): output.requires_grad_(True) model.get_input_embeddings().register_forward_hook(make_inputs_require_grad) # Then enable gradient checkpointing model.gradient_checkpointing_enable() return model if __name__ == \"__main__\": # Main execution device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model_name = \"/model/ztq147/Qwen/Qwen2.5-1.5B-Instruct\" output_dir = \"/data/ztq147/temp_models/math_solver_model\" if not os.path.exists(output_dir): os.makedirs(output_dir) print(\"Downloading model...\") model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.bfloat16, device_map='auto' ) print(\"Model downloaded\") tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model.config.pad_token_id = tokenizer.eos_token_id model.config.eos_token_id = tokenizer.eos_token_id # setting the number of device # device_ids = list(range(8)) all_data = prepare_dataset(\"train\") random.shuffle(all_data) size_of_eval_data = 30 eval_data = all_data[:size_of_eval_data] train_data = all_data[size_of_eval_data:] # print(\"\\nInital model evaluation before finetuning:\") # pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) # print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\") model = optimize_model_memory(model) print(\"\\nStarting RL fine-tuning using GRPO...\") training_config = { 'num_iterations': 1, 'num_steps': 500, 'batch_size': 2, 'num_generations': 8, 'gradient_accumulation_steps': 4, 'max_completion_length': 256, 'beta': 0.04, 'learning_rate': 5e-6, 'mu': 1, 'epsilon': 0.1 } wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True) print(\"Weights \u0026 Biases initialized.\") # pdb.set_trace() model = train_with_grpo( model=model, tokenizer=tokenizer, train_data=train_data, reward_function=combined_reward, # device_ids=device_ids, **training_config ) wandb.finish() print(\"Training completed and wandb run finished.\") print(\"\\nFinalmodel evaluation after GTPO RL fine-tuning:\") post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device) print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\") print(\"\\nSaving GTPO fine-tuned model...\") model.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) 单卡实验结果 作者使用一张3090GPU训练，实验结果如下：\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=3, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 23.33% (7/30)\n这里横轴是1500步因为设置了mu=3，所以每个step被记录了三次，这个实验主要想看看mu的影响，个人理解mu=1的情况下，ratio的计算结果应该永远是1。\nbatch_size=1, num_generations=8, max_completion_length=400, gradient_accumulation_steps=12, mu=1, num_steps=1500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 36.67% (11/30)\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=6, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 40.00% (12/30)\nbatch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1 最终的测试准确率：Accuracy: 46.67% (14/30)\n单卡实验小结 从上面简单的几次实验结果来看，最好的是batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1，准确率达到了46.67%，但是这个结果与原作者的90%的准确率有较大差距，原作者用8张80G显存的A100，batch_size设置为7（一张卡放1条数据，原文用了数据并行nn.DataParallel），最大生成长度设置为400，num_generations设置为12，mu=1，gradient_accumulation_steps=1，一共训练500步，其他参数与本文这个设置的其他参数一致。\n整体看对结果影响较大的参数有max_completino_length和num_generations。相反，设置gradient_accumulation_steps并没能带来大batch_size的效果（这个不确定是不是代码写的有问题），反而降低性能，mu的设置大于1也降低了性能。\n实验的loss曲线也没有原文那种稳定的上升趋势，虽然不理解为啥loss是上升的，本文贴的所有loss图均是设置了最大Y值，实际上会存在很多loss很高的脉冲（十几到几百），这个脉冲存在的原因也不太清楚。reward图和原文的有一定相似性。\nReferences [1] Shao et al. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models” arXiv preprint arXiv:2402.03300 (2024).\n[2] John Schulman. “Approximating KL Divergence” John Schulman’s Homepage 2020.\n[3] Andriy Burkov. “Coding GRPO from Scratch: A Guide to Distributed Implementation with Qwen2.5-1.5B-Instruct” github 2025.\n",
  "wordCount" : "4507",
  "inLanguage": "en",
  "datePublished": "2025-03-05T00:00:00Z",
  "dateModified": "2025-03-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Rs"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rs' Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tqzhong.github.io/my-blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    
    
    
    <nav class="nav">
        <div class="logo-logo-switches">
        <div class="logo">
            <a href="https://tqzhong.github.io/my-blog/" accesskey="h" title="Rs&#39; Log (Alt + H)">Rs&#39; Log</a>

            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch">
                </ul>
            </div>
        </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tqzhong.github.io/my-blog/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/faq/" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>


    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      GRPO From Scratch
    </h1>
    <div class="post-meta">


<i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-03-05 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;22 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span id="view-counter"></span>

<script src="https://cdn.jsdelivr.net/npm/leancloud-storage@4.12.0/dist/av-min.js"></script>
<script>
    
    AV.initialize("AokLJzaIvwVNJW0b2F0YTLLy-MdYXbMMI", "fwZpRfBG259O3LscJFPW3ViH");

    loadViewCount(location.pathname, "view-counter");

    
    
    
        
    
    
    
    
    
    
    
    
    
                
    
    
    
    
    


    var hasViewCounted = false;

    
    function loadViewCount(pageUrl, elementId) {
        if (hasViewCounted) return;
        hasViewCounted = true;       

        var counter = document.getElementById(elementId);
        var cachedViews = localStorage.getItem("view-count-" + pageUrl);

        
        if (cachedViews) {
            counter.innerText = cachedViews;
        } else {
            counter.innerText = "0"; 
        }

        
        var query = new AV.Query('Counter');
        query.equalTo("url", pageUrl);
        query.find().then(results => {
            if (results.length > 0) {
                var counterObj = results[0];
                counterObj.increment("views", 1);
                counterObj.save();
                var views = counterObj.get("views");
                counter.innerText = views;
                localStorage.setItem("view-count-" + pageUrl, views); 
            } else {
                var Counter = AV.Object.extend("Counter");
                var newCounter = new Counter();
                newCounter.set("url", pageUrl);
                newCounter.set("views", 1);
                newCounter.save().then(() => {
                    counter.innerText = "1";
                    localStorage.setItem("view-count-" + pageUrl, 1); 
                });
            }
        }).catch(function(error) {
            console.error("error:", error);
        });
    }
    
</script>



</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%ae%80%e4%bb%8b" aria-label="简介">简介</a></li>
                <li>
                    <a href="#grpo" aria-label="GRPO">GRPO</a></li>
                <li>
                    <a href="#%e4%bb%a3%e7%a0%81" aria-label="代码">代码</a></li>
                <li>
                    <a href="#%e5%8d%95%e5%8d%a1%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" aria-label="单卡实验结果">单卡实验结果</a></li>
                <li>
                    <a href="#%e5%8d%95%e5%8d%a1%e5%ae%9e%e9%aa%8c%e5%b0%8f%e7%bb%93" aria-label="单卡实验小结">单卡实验小结</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="简介">简介<a hidden class="anchor" aria-hidden="true" href="#简介">#</a></h3>
<p>本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：<a href="https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb" class="entityLink">GRPO_From-Scratch</a>以及<a href="https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb" class="entityLink">GRPO_Qwen-0_5_Instruct</a>。注：原作者使用8张80G A100完成实验。</p>
<h3 id="grpo">GRPO<a hidden class="anchor" aria-hidden="true" href="#grpo">#</a></h3>
<p>GRPO算法原理见<a href="https://matrixai.online/my-blog/posts/2025-01-27-deepseek-r1/#311-reinforcement-learning-algorithm" class="entityLink">alg-grpo</a>，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。</p>
<div class="scroll-container">
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[ 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i 
\right] 
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,< t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, < t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$
</div>
<div class="scroll-container">
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, < t})}{\pi_{\theta}(o_{i, t} | q, o_{i, < t})} 
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, < t})}{\pi_{\theta}(o_{i, t} | q, o_{i, < t})} - 1,
$$
</div>
<div class="scroll-container">
$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
</div>
<p>GRPO算法出自文章<a href="https://arxiv.org/abs/2402.03300" class="entityLink">DeepSeekMath (2024)</a>，其中KL散度的计算采用了<a href="http://joschu.net/blog/kl-approx.html" class="entityLink">Approximating KL Divergence</a>中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。</p>
<h3 id="代码">代码<a hidden class="anchor" aria-hidden="true" href="#代码">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span><span class="lnt">260
</span><span class="lnt">261
</span><span class="lnt">262
</span><span class="lnt">263
</span><span class="lnt">264
</span><span class="lnt">265
</span><span class="lnt">266
</span><span class="lnt">267
</span><span class="lnt">268
</span><span class="lnt">269
</span><span class="lnt">270
</span><span class="lnt">271
</span><span class="lnt">272
</span><span class="lnt">273
</span><span class="lnt">274
</span><span class="lnt">275
</span><span class="lnt">276
</span><span class="lnt">277
</span><span class="lnt">278
</span><span class="lnt">279
</span><span class="lnt">280
</span><span class="lnt">281
</span><span class="lnt">282
</span><span class="lnt">283
</span><span class="lnt">284
</span><span class="lnt">285
</span><span class="lnt">286
</span><span class="lnt">287
</span><span class="lnt">288
</span><span class="lnt">289
</span><span class="lnt">290
</span><span class="lnt">291
</span><span class="lnt">292
</span><span class="lnt">293
</span><span class="lnt">294
</span><span class="lnt">295
</span><span class="lnt">296
</span><span class="lnt">297
</span><span class="lnt">298
</span><span class="lnt">299
</span><span class="lnt">300
</span><span class="lnt">301
</span><span class="lnt">302
</span><span class="lnt">303
</span><span class="lnt">304
</span><span class="lnt">305
</span><span class="lnt">306
</span><span class="lnt">307
</span><span class="lnt">308
</span><span class="lnt">309
</span><span class="lnt">310
</span><span class="lnt">311
</span><span class="lnt">312
</span><span class="lnt">313
</span><span class="lnt">314
</span><span class="lnt">315
</span><span class="lnt">316
</span><span class="lnt">317
</span><span class="lnt">318
</span><span class="lnt">319
</span><span class="lnt">320
</span><span class="lnt">321
</span><span class="lnt">322
</span><span class="lnt">323
</span><span class="lnt">324
</span><span class="lnt">325
</span><span class="lnt">326
</span><span class="lnt">327
</span><span class="lnt">328
</span><span class="lnt">329
</span><span class="lnt">330
</span><span class="lnt">331
</span><span class="lnt">332
</span><span class="lnt">333
</span><span class="lnt">334
</span><span class="lnt">335
</span><span class="lnt">336
</span><span class="lnt">337
</span><span class="lnt">338
</span><span class="lnt">339
</span><span class="lnt">340
</span><span class="lnt">341
</span><span class="lnt">342
</span><span class="lnt">343
</span><span class="lnt">344
</span><span class="lnt">345
</span><span class="lnt">346
</span><span class="lnt">347
</span><span class="lnt">348
</span><span class="lnt">349
</span><span class="lnt">350
</span><span class="lnt">351
</span><span class="lnt">352
</span><span class="lnt">353
</span><span class="lnt">354
</span><span class="lnt">355
</span><span class="lnt">356
</span><span class="lnt">357
</span><span class="lnt">358
</span><span class="lnt">359
</span><span class="lnt">360
</span><span class="lnt">361
</span><span class="lnt">362
</span><span class="lnt">363
</span><span class="lnt">364
</span><span class="lnt">365
</span><span class="lnt">366
</span><span class="lnt">367
</span><span class="lnt">368
</span><span class="lnt">369
</span><span class="lnt">370
</span><span class="lnt">371
</span><span class="lnt">372
</span><span class="lnt">373
</span><span class="lnt">374
</span><span class="lnt">375
</span><span class="lnt">376
</span><span class="lnt">377
</span><span class="lnt">378
</span><span class="lnt">379
</span><span class="lnt">380
</span><span class="lnt">381
</span><span class="lnt">382
</span><span class="lnt">383
</span><span class="lnt">384
</span><span class="lnt">385
</span><span class="lnt">386
</span><span class="lnt">387
</span><span class="lnt">388
</span><span class="lnt">389
</span><span class="lnt">390
</span><span class="lnt">391
</span><span class="lnt">392
</span><span class="lnt">393
</span><span class="lnt">394
</span><span class="lnt">395
</span><span class="lnt">396
</span><span class="lnt">397
</span><span class="lnt">398
</span><span class="lnt">399
</span><span class="lnt">400
</span><span class="lnt">401
</span><span class="lnt">402
</span><span class="lnt">403
</span><span class="lnt">404
</span><span class="lnt">405
</span><span class="lnt">406
</span><span class="lnt">407
</span><span class="lnt">408
</span><span class="lnt">409
</span><span class="lnt">410
</span><span class="lnt">411
</span><span class="lnt">412
</span><span class="lnt">413
</span><span class="lnt">414
</span><span class="lnt">415
</span><span class="lnt">416
</span><span class="lnt">417
</span><span class="lnt">418
</span><span class="lnt">419
</span><span class="lnt">420
</span><span class="lnt">421
</span><span class="lnt">422
</span><span class="lnt">423
</span><span class="lnt">424
</span><span class="lnt">425
</span><span class="lnt">426
</span><span class="lnt">427
</span><span class="lnt">428
</span><span class="lnt">429
</span><span class="lnt">430
</span><span class="lnt">431
</span><span class="lnt">432
</span><span class="lnt">433
</span><span class="lnt">434
</span><span class="lnt">435
</span><span class="lnt">436
</span><span class="lnt">437
</span><span class="lnt">438
</span><span class="lnt">439
</span><span class="lnt">440
</span><span class="lnt">441
</span><span class="lnt">442
</span><span class="lnt">443
</span><span class="lnt">444
</span><span class="lnt">445
</span><span class="lnt">446
</span><span class="lnt">447
</span><span class="lnt">448
</span><span class="lnt">449
</span><span class="lnt">450
</span><span class="lnt">451
</span><span class="lnt">452
</span><span class="lnt">453
</span><span class="lnt">454
</span><span class="lnt">455
</span><span class="lnt">456
</span><span class="lnt">457
</span><span class="lnt">458
</span><span class="lnt">459
</span><span class="lnt">460
</span><span class="lnt">461
</span><span class="lnt">462
</span><span class="lnt">463
</span><span class="lnt">464
</span><span class="lnt">465
</span><span class="lnt">466
</span><span class="lnt">467
</span><span class="lnt">468
</span><span class="lnt">469
</span><span class="lnt">470
</span><span class="lnt">471
</span><span class="lnt">472
</span><span class="lnt">473
</span><span class="lnt">474
</span><span class="lnt">475
</span><span class="lnt">476
</span><span class="lnt">477
</span><span class="lnt">478
</span><span class="lnt">479
</span><span class="lnt">480
</span><span class="lnt">481
</span><span class="lnt">482
</span><span class="lnt">483
</span><span class="lnt">484
</span><span class="lnt">485
</span><span class="lnt">486
</span><span class="lnt">487
</span><span class="lnt">488
</span><span class="lnt">489
</span><span class="lnt">490
</span><span class="lnt">491
</span><span class="lnt">492
</span><span class="lnt">493
</span><span class="lnt">494
</span><span class="lnt">495
</span><span class="lnt">496
</span><span class="lnt">497
</span><span class="lnt">498
</span><span class="lnt">499
</span><span class="lnt">500
</span><span class="lnt">501
</span><span class="lnt">502
</span><span class="lnt">503
</span><span class="lnt">504
</span><span class="lnt">505
</span><span class="lnt">506
</span><span class="lnt">507
</span><span class="lnt">508
</span><span class="lnt">509
</span><span class="lnt">510
</span><span class="lnt">511
</span><span class="lnt">512
</span><span class="lnt">513
</span><span class="lnt">514
</span><span class="lnt">515
</span><span class="lnt">516
</span><span class="lnt">517
</span><span class="lnt">518
</span><span class="lnt">519
</span><span class="lnt">520
</span><span class="lnt">521
</span><span class="lnt">522
</span><span class="lnt">523
</span><span class="lnt">524
</span><span class="lnt">525
</span><span class="lnt">526
</span><span class="lnt">527
</span><span class="lnt">528
</span><span class="lnt">529
</span><span class="lnt">530
</span><span class="lnt">531
</span><span class="lnt">532
</span><span class="lnt">533
</span><span class="lnt">534
</span><span class="lnt">535
</span><span class="lnt">536
</span><span class="lnt">537
</span><span class="lnt">538
</span><span class="lnt">539
</span><span class="lnt">540
</span><span class="lnt">541
</span><span class="lnt">542
</span><span class="lnt">543
</span><span class="lnt">544
</span><span class="lnt">545
</span><span class="lnt">546
</span><span class="lnt">547
</span><span class="lnt">548
</span><span class="lnt">549
</span><span class="lnt">550
</span><span class="lnt">551
</span><span class="lnt">552
</span><span class="lnt">553
</span><span class="lnt">554
</span><span class="lnt">555
</span><span class="lnt">556
</span><span class="lnt">557
</span><span class="lnt">558
</span><span class="lnt">559
</span><span class="lnt">560
</span><span class="lnt">561
</span><span class="lnt">562
</span><span class="lnt">563
</span><span class="lnt">564
</span><span class="lnt">565
</span><span class="lnt">566
</span><span class="lnt">567
</span><span class="lnt">568
</span><span class="lnt">569
</span><span class="lnt">570
</span><span class="lnt">571
</span><span class="lnt">572
</span><span class="lnt">573
</span><span class="lnt">574
</span><span class="lnt">575
</span><span class="lnt">576
</span><span class="lnt">577
</span><span class="lnt">578
</span><span class="lnt">579
</span><span class="lnt">580
</span><span class="lnt">581
</span><span class="lnt">582
</span><span class="lnt">583
</span><span class="lnt">584
</span><span class="lnt">585
</span><span class="lnt">586
</span><span class="lnt">587
</span><span class="lnt">588
</span><span class="lnt">589
</span><span class="lnt">590
</span><span class="lnt">591
</span><span class="lnt">592
</span><span class="lnt">593
</span><span class="lnt">594
</span><span class="lnt">595
</span><span class="lnt">596
</span><span class="lnt">597
</span><span class="lnt">598
</span><span class="lnt">599
</span><span class="lnt">600
</span><span class="lnt">601
</span><span class="lnt">602
</span><span class="lnt">603
</span><span class="lnt">604
</span><span class="lnt">605
</span><span class="lnt">606
</span><span class="lnt">607
</span><span class="lnt">608
</span><span class="lnt">609
</span><span class="lnt">610
</span><span class="lnt">611
</span><span class="lnt">612
</span><span class="lnt">613
</span><span class="lnt">614
</span><span class="lnt">615
</span><span class="lnt">616
</span><span class="lnt">617
</span><span class="lnt">618
</span><span class="lnt">619
</span><span class="lnt">620
</span><span class="lnt">621
</span><span class="lnt">622
</span><span class="lnt">623
</span><span class="lnt">624
</span><span class="lnt">625
</span><span class="lnt">626
</span><span class="lnt">627
</span><span class="lnt">628
</span><span class="lnt">629
</span><span class="lnt">630
</span><span class="lnt">631
</span><span class="lnt">632
</span><span class="lnt">633
</span><span class="lnt">634
</span><span class="lnt">635
</span><span class="lnt">636
</span><span class="lnt">637
</span><span class="lnt">638
</span><span class="lnt">639
</span><span class="lnt">640
</span><span class="lnt">641
</span><span class="lnt">642
</span><span class="lnt">643
</span><span class="lnt">644
</span><span class="lnt">645
</span><span class="lnt">646
</span><span class="lnt">647
</span><span class="lnt">648
</span><span class="lnt">649
</span><span class="lnt">650
</span><span class="lnt">651
</span><span class="lnt">652
</span><span class="lnt">653
</span><span class="lnt">654
</span><span class="lnt">655
</span><span class="lnt">656
</span><span class="lnt">657
</span><span class="lnt">658
</span><span class="lnt">659
</span><span class="lnt">660
</span><span class="lnt">661
</span><span class="lnt">662
</span><span class="lnt">663
</span><span class="lnt">664
</span><span class="lnt">665
</span><span class="lnt">666
</span><span class="lnt">667
</span><span class="lnt">668
</span><span class="lnt">669
</span><span class="lnt">670
</span><span class="lnt">671
</span><span class="lnt">672
</span><span class="lnt">673
</span><span class="lnt">674
</span><span class="lnt">675
</span><span class="lnt">676
</span><span class="lnt">677
</span><span class="lnt">678
</span><span class="lnt">679
</span><span class="lnt">680
</span><span class="lnt">681
</span><span class="lnt">682
</span><span class="lnt">683
</span><span class="lnt">684
</span><span class="lnt">685
</span><span class="lnt">686
</span><span class="lnt">687
</span><span class="lnt">688
</span><span class="lnt">689
</span><span class="lnt">690
</span><span class="lnt">691
</span><span class="lnt">692
</span><span class="lnt">693
</span><span class="lnt">694
</span><span class="lnt">695
</span><span class="lnt">696
</span><span class="lnt">697
</span><span class="lnt">698
</span><span class="lnt">699
</span><span class="lnt">700
</span><span class="lnt">701
</span><span class="lnt">702
</span><span class="lnt">703
</span><span class="lnt">704
</span><span class="lnt">705
</span><span class="lnt">706
</span><span class="lnt">707
</span><span class="lnt">708
</span><span class="lnt">709
</span><span class="lnt">710
</span><span class="lnt">711
</span><span class="lnt">712
</span><span class="lnt">713
</span><span class="lnt">714
</span><span class="lnt">715
</span><span class="lnt">716
</span><span class="lnt">717
</span><span class="lnt">718
</span><span class="lnt">719
</span><span class="lnt">720
</span><span class="lnt">721
</span><span class="lnt">722
</span><span class="lnt">723
</span><span class="lnt">724
</span><span class="lnt">725
</span><span class="lnt">726
</span><span class="lnt">727
</span><span class="lnt">728
</span><span class="lnt">729
</span><span class="lnt">730
</span><span class="lnt">731
</span><span class="lnt">732
</span><span class="lnt">733
</span><span class="lnt">734
</span><span class="lnt">735
</span><span class="lnt">736
</span><span class="lnt">737
</span><span class="lnt">738
</span><span class="lnt">739
</span><span class="lnt">740
</span><span class="lnt">741
</span><span class="lnt">742
</span><span class="lnt">743
</span><span class="lnt">744
</span><span class="lnt">745
</span><span class="lnt">746
</span><span class="lnt">747
</span><span class="lnt">748
</span><span class="lnt">749
</span><span class="lnt">750
</span><span class="lnt">751
</span><span class="lnt">752
</span><span class="lnt">753
</span><span class="lnt">754
</span><span class="lnt">755
</span><span class="lnt">756
</span><span class="lnt">757
</span><span class="lnt">758
</span><span class="lnt">759
</span><span class="lnt">760
</span><span class="lnt">761
</span><span class="lnt">762
</span><span class="lnt">763
</span><span class="lnt">764
</span><span class="lnt">765
</span><span class="lnt">766
</span><span class="lnt">767
</span><span class="lnt">768
</span><span class="lnt">769
</span><span class="lnt">770
</span><span class="lnt">771
</span><span class="lnt">772
</span><span class="lnt">773
</span><span class="lnt">774
</span><span class="lnt">775
</span><span class="lnt">776
</span><span class="lnt">777
</span><span class="lnt">778
</span><span class="lnt">779
</span><span class="lnt">780
</span><span class="lnt">781
</span><span class="lnt">782
</span><span class="lnt">783
</span><span class="lnt">784
</span><span class="lnt">785
</span><span class="lnt">786
</span><span class="lnt">787
</span><span class="lnt">788
</span><span class="lnt">789
</span><span class="lnt">790
</span><span class="lnt">791
</span><span class="lnt">792
</span><span class="lnt">793
</span><span class="lnt">794
</span><span class="lnt">795
</span><span class="lnt">796
</span><span class="lnt">797
</span><span class="lnt">798
</span><span class="lnt">799
</span><span class="lnt">800
</span><span class="lnt">801
</span><span class="lnt">802
</span><span class="lnt">803
</span><span class="lnt">804
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">copy</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">re</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">wandb</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pdb</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">load_from_disk</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">set_random_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;WANDB_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;YOUR_API_KEY&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;WANDB_PROJECT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;GRPO-Qwen-2.5-1.5B-Instruct&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 设置系统prompt</span>
</span></span><span class="line"><span class="cl"><span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">Respond in the following format:
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;reasoning&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">...
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;/reasoning&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;answer&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">...
</span></span></span><span class="line"><span class="cl"><span class="s2">&lt;/answer&gt;
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">extract_answer_from_model_output</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">   Extracts the value from the last &lt;answer&gt; tag in the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       text (str): The model-generated text containing XML-style &lt;answer&gt; tags.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       str or None: The content inside the &lt;answer&gt; tags, or None if no valid answer is found.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Splits the text on the &lt;answer&gt; tag to isolate content after the tag.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Checks if at least one &lt;answer&gt; tag exists in the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. For the last &lt;answer&gt; segment:
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Verifies it contains a closing &lt;/answer&gt; tag.
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Extracts only the content between the tags.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. Returns None if the answer is empty (just &#34;...&#34;) or if tags are missing.
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">parts</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;&lt;answer&gt;&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span> <span class="c1"># No &lt;answer&gt; tag found</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">last_part</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;&lt;/answer&gt;&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">last_part</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">answer</span> <span class="o">=</span> <span class="n">last_part</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;&lt;/answer&gt;&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">answer</span> <span class="o">==</span> <span class="s2">&#34;...&#34;</span> <span class="k">else</span> <span class="n">answer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">extract_answer_from_dataset</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">   Extracts the answer from the GSM8K dataset examples.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       text (str): The dataset example text containing a question and answer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       str or None: The extracted answer part after the &#39;####&#39; delimiter, or None if not found.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Checks if the text contains the &#39;####&#39; delimiter that separates question from answer.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. If found, splits the text at this delimiter and returns the second part (the answer).
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. The answer is stripped of leading/trailing whitespace.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. Returns None if no delimiter is present.
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s2">&#34;####&#34;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;####&#34;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">prepare_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">   Load and prepare the GSM8K dataset for training with string prompts.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       split (str): The dataset split to load (&#34;train&#34; or &#34;test&#34;). Defaults to &#34;train&#34;.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       list: A list of formatted examples, each containing a prompt string and answer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Loads the GSM8K dataset from the Hugging Face datasets hub.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. For each example in the dataset:
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Creates a list of messages with system prompt and the question.
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Converts this list into a single string prompt using build_prompt().
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Extracts the answer from the dataset example.
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Creates a formatted example dictionary with prompt and answer.
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. Returns the list of formatted examples ready for model training or evaluation.
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 从本地加载，服务器端连接不上huggingface，使用train部分的数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">load_from_disk</span><span class="p">(</span><span class="s1">&#39;/data/ztq147/gsm8k&#39;</span><span class="p">)[</span><span class="n">split</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># data = load_dataset(&#39;openai/gsm8k&#39;, &#39;main&#39;)[split]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 一个formatted数据包含“prompt”和“answer”，其中“prompt”格式为SYSTEM_PROMPT\n QUESTION；“answer”格式为ANSWER</span>
</span></span><span class="line"><span class="cl">    <span class="n">formatted_data</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">prompt_str</span> <span class="o">=</span> <span class="n">build_prompt</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">SYSTEM_PROMPT</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">                <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;question&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">formatted_example</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;prompt&#34;</span><span class="p">:</span> <span class="n">prompt_str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;answer&#34;</span><span class="p">:</span> <span class="n">extract_answer_from_dataset</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s2">&#34;answer&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">formatted_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">formatted_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">formatted_data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">build_prompt</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">   Build a single prompt string from a list of messages.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       messages (list): A list of message dictionaries, each with &#39;role&#39; and &#39;content&#39; keys.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       str: A concatenated string of all message contents.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">   Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Takes a list of message dictionaries in the typical chat format.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Extracts the &#39;content&#39; field from each message and strips whitespace.
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. Joins all content strings with newlines to create a single prompt.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. This preserves the training format while converting from structured messages to a string.
</span></span></span><span class="line"><span class="cl"><span class="s2">   &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">msg</span><span class="p">[</span><span class="s2">&#34;content&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">extract_last_number</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Extracts the last number appearing in the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       text (str): The text to extract a number from.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       float or None: The last number in the text, or None if no number is found.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Removes dollar signs and percent symbols from the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Uses regex to find a number that appears at the end of the text (possibly after whitespace).
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. The pattern matches numbers that appear at the end of the string, with or without decimal points.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. Returns the found number as a float, or None if no match is found.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;(?:^|\s|=)\s*(-?\d*\.?\d+)\s*$&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="k">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="k">match</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">extract_single_number</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Extracts the last number appearing in the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       text (str): The text to extract a number from.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       float or None: The last number in the text, or None if no number is found.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Removes dollar signs and percent symbols from the text.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Uses regex to find a number that appears at the end of the text (possibly after whitespace).
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. The pattern matches numbers that appear at the end of the string, with or without decimal points.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. Returns the found number as a float, or None if no match is found.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;(?:^|\s|=)\s*(-?\d*\.?\d+)\s*$&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="k">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="k">match</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">eval_examples</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Evaluates the model on a set of examples and prints detailed results.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       model: The language model to evaluate.
</span></span></span><span class="line"><span class="cl"><span class="s2">       tokenizer: The tokenizer for encoding inputs and decoding outputs.
</span></span></span><span class="line"><span class="cl"><span class="s2">       eval_examples (list): List of evaluation examples, each containing &#34;prompt&#34; and &#34;answer&#34;.
</span></span></span><span class="line"><span class="cl"><span class="s2">       device: The device (CPU or GPU) to run evaluation on.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       float: The accuracy percentage (correct predictions / total examples * 100).
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Sets the model to evaluation mode.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. For each example in the evaluation set:
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Encodes the prompt and generates a response using the model.
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Extracts the predicted answer from the generated response.
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Compares the predicted answer with the expected answer using multiple methods:
</span></span></span><span class="line"><span class="cl"><span class="s2">            a. Exact string matching
</span></span></span><span class="line"><span class="cl"><span class="s2">            b. Single number extraction and comparison
</span></span></span><span class="line"><span class="cl"><span class="s2">            c. Last number extraction and comparison
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Prints detailed information about each example.
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. Calculates and returns the overall accuracy.
</span></span></span><span class="line"><span class="cl"><span class="s2">       4. Returns the model to training mode.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_examples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span> <span class="o">+</span> <span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;EVALUATION ON&#34;</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="s2">&#34;EXAMPLES&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">eval_examples</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_prompt</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s2">&#34;prompt&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">expected</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s2">&#34;answer&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">full_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># early_stopping=False，表示模型会一直生直到达到最大新标记数（max_new_tokens）。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># forced_eos_token_id=tokenizer.eos_token_id，当生成到最大长度时，默认将最后一个token换成eos_token_id。</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">inputs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">forced_eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 关于数学类问题评估的方法，这里不太了解，之前没做过</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">predicted</span> <span class="o">=</span> <span class="n">extract_answer_from_model_output</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">predicted</span> <span class="o">==</span> <span class="n">expected</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">is_correct</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">pred_num</span> <span class="o">=</span> <span class="n">extract_single_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">predicted</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">exp_num</span> <span class="o">=</span> <span class="n">extract_single_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">pred_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">exp_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pred_num</span> <span class="o">==</span> <span class="n">exp_num</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">is_correct</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pred_num</span> <span class="o">=</span> <span class="n">extract_last_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">predicted</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                    <span class="n">exp_num</span> <span class="o">=</span> <span class="n">extract_last_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                    <span class="n">is_correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">exp_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pred_num</span> <span class="o">==</span> <span class="n">exp_num</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">is_correct</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Prompt:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">full_prompt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Expected Answer:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">expected</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Extracted Answer:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Full Generated Response:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Correct:&#34;</span><span class="p">,</span> <span class="s2">&#34;✓&#34;</span> <span class="k">if</span> <span class="n">is_correct</span> <span class="k">else</span> <span class="s2">&#34;✗&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Failed to parse model output for prompt:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="n">full_prompt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Error:&#34;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% (</span><span class="si">{</span><span class="n">correct</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total</span><span class="si">}</span><span class="s2">)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;=&#34;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">accuracy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">correctness_reward</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="n">answer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Assigns a reward based on the correctness of the model&#39;s answer.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">    prompts (list): List of input prompts.
</span></span></span><span class="line"><span class="cl"><span class="s2">    completions (list): List of model completions, each containing content.
</span></span></span><span class="line"><span class="cl"><span class="s2">    answer (list): List of expected answers.
</span></span></span><span class="line"><span class="cl"><span class="s2">    **kwargs: Additional keyword arguments.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    list: List of numerical rewards for each completion.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">    1. Extracts the content from each completion.
</span></span></span><span class="line"><span class="cl"><span class="s2">    2. Extracts the answer portion from each response using extract_answer_from_model_output.
</span></span></span><span class="line"><span class="cl"><span class="s2">    3. Assigns rewards based on matching criteria:
</span></span></span><span class="line"><span class="cl"><span class="s2">      - 2.0 points for an exact match
</span></span></span><span class="line"><span class="cl"><span class="s2">      - 1.5 points for numeric equivalence (when values match but format differs)
</span></span></span><span class="line"><span class="cl"><span class="s2">      - 0.0 points for incorrect answers
</span></span></span><span class="line"><span class="cl"><span class="s2">    4. Tracks completion lengths for analysis.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="n">completion</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">completion</span> <span class="ow">in</span> <span class="n">completions</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">extracted</span> <span class="o">=</span> <span class="p">[</span><span class="n">extract_answer_from_model_output</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">extracted</span><span class="p">,</span> <span class="n">answer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">r</span> <span class="o">==</span> <span class="n">a</span><span class="p">:</span> <span class="c1"># Exact match case</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">r_num</span> <span class="o">=</span> <span class="n">extract_single_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">a_num</span> <span class="o">=</span> <span class="n">extract_single_number</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">r_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">a_num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">r_num</span> <span class="o">==</span> <span class="n">a_num</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">1.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">completion_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rewards</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">format_reward</span><span class="p">(</span><span class="n">completions</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Assigns a reward for adhering to the desired XML format.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       completions (list): List of model completions, each containing content.
</span></span></span><span class="line"><span class="cl"><span class="s2">       **kwargs: Additional keyword arguments.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       list: List of format compliance scores for each completion.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Extracts the content from each completion.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Evaluates format compliance by checking for required XML tags:
</span></span></span><span class="line"><span class="cl"><span class="s2">          - 0.2 points for each tag present (&lt;reasoning&gt;, &lt;/reasoning&gt;, &lt;answer&gt;, &lt;/answer&gt;)
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Maximum score of 0.8 for perfect format compliance
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. Stores and returns the format compliance scores.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">responses</span> <span class="o">=</span> <span class="p">[</span><span class="n">completion</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">completion</span> <span class="ow">in</span> <span class="n">completions</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">format_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">responses</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">score</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="s2">&#34;&lt;reasoning&gt;&#34;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span> <span class="n">score</span> <span class="o">+=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="s2">&#34;&lt;/reasoning&gt;&#34;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span> <span class="n">score</span> <span class="o">+=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="s2">&#34;&lt;answer&gt;&#34;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span> <span class="n">score</span> <span class="o">+=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="s2">&#34;&lt;/answer&gt;&#34;</span> <span class="ow">in</span> <span class="n">response</span><span class="p">:</span> <span class="n">score</span> <span class="o">+=</span> <span class="mf">0.2</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">format_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rewards</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">combined_reward</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">completions</span><span class="p">,</span> <span class="n">answer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Combines correctness and format rewards.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">       prompts (list[str]): List of prompt texts
</span></span></span><span class="line"><span class="cl"><span class="s2">       completions (list[list[dict]]): List of completion dictionaries
</span></span></span><span class="line"><span class="cl"><span class="s2">       answer (list[str]): List of expected answers
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">       list[float]: Combined rewards for each prompt-completion pair
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">       1. Calculates separate rewards for correctness and format compliance.
</span></span></span><span class="line"><span class="cl"><span class="s2">       2. Combines the rewards with the following weights:
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Correctness score range: 0.0 to 2.0
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Format score range: 0.0 to 0.8
</span></span></span><span class="line"><span class="cl"><span class="s2">          - Total possible range: 0.0 to 2.8
</span></span></span><span class="line"><span class="cl"><span class="s2">       3. Returns the combined reward for each example.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Get individual rewards</span>
</span></span><span class="line"><span class="cl">    <span class="n">correctness_scores</span> <span class="o">=</span> <span class="n">correctness_reward</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span> <span class="n">completions</span><span class="o">=</span><span class="n">completions</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="n">answer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">format_scores</span> <span class="o">=</span> <span class="n">format_reward</span><span class="p">(</span><span class="n">completions</span><span class="o">=</span><span class="n">completions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Combine rewards - correctness is weighted more heavily</span>
</span></span><span class="line"><span class="cl">    <span class="n">combined_rewards</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">c_score</span><span class="p">,</span> <span class="n">f_score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">correctness_scores</span><span class="p">,</span> <span class="n">format_scores</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Correctness score range: 0.0 to 2.0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Format score range: 0.0 to 0.8</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Total range: 0.0 to 2.8</span>
</span></span><span class="line"><span class="cl">        <span class="n">combined_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c_score</span> <span class="o">+</span> <span class="n">f_score</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">combined_rewards</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">selective_log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes log probabilities for specific tokens in the vocabulary.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        logits (torch.Tensor): The raw logits output from the model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        input_ids (torch.Tensor): The token IDs for which we want the log probabilities.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        torch.Tensor: Log probabilities of the selected tokens.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Applies log softmax to convert logits to log probabilities over the vocabulary.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Uses gather to extract only the log probabilities corresponding to the input_ids.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Removes the extra dimension to match the original shape of input_ids.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_log_probs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">logits_to_keep</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the log probabilities for a batch of tokens.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The language model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        input_ids (torch.Tensor): Token IDs for input sequences.
</span></span></span><span class="line"><span class="cl"><span class="s2">        attention_mask (torch.Tensor): Attention mask for input sequences.
</span></span></span><span class="line"><span class="cl"><span class="s2">        logits_to_keep (int): Number of tokens to keep from the end of the sequence.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        torch.Tensor: Log probabilities of the selected tokens.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Gets logits from the model for the input sequence.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Selects logits for all tokens except the last one (as we predict next tokens).
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Selects only the last &#39;logits_to_keep&#39; tokens from both logits and input_ids.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. Computes log probabilities for these tokens using selective_log_softmax.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="n">logits_to_keep</span><span class="p">:]</span>  <span class="c1"># select the generation part, remove the prompt part</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="n">logits_to_keep</span><span class="p">:,</span> <span class="p">:]</span>     <span class="c1"># the same as input_ids</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">selective_log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_completion_mask</span><span class="p">(</span><span class="n">completion_ids</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Creates a mask for completion tokens that excludes tokens after the EOS token.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        completion_ids (torch.Tensor): Token IDs of the generated completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        eos_token_id (int): The ID of the end-of-sequence token.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Identifies positions where EOS tokens occur in each sequence.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Finds the index of the first EOS token in each sequence.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Creates a mask where positions before and including the first EOS are 1, others are 0.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. If no EOS token is found in a sequence, all positions are set to 1.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_eos</span> <span class="o">=</span> <span class="n">completion_ids</span> <span class="o">==</span> <span class="n">eos_token_id</span> <span class="c1"># shape (bs, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">eos_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">is_eos</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),),</span> <span class="n">is_eos</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">completion_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># 表示每个completion的第一个Eos_token的位置，初始化全为max_completion_length. shape (bs, )</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask_exists</span> <span class="o">=</span> <span class="n">is_eos</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 返回一个布尔向量 shape (bs, )，表示哪些序列包含至少一个Eos token</span>
</span></span><span class="line"><span class="cl">    <span class="n">eos_idx</span><span class="p">[</span><span class="n">mask_exists</span><span class="p">]</span> <span class="o">=</span> <span class="n">is_eos</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">mask_exists</span><span class="p">]</span> <span class="c1"># 对于包含Eos token的序列，找到第一个Eos token的位置，is_eos.int().argmax(dim=1)返回每个序列中第一个Eos token的索引。</span>
</span></span><span class="line"><span class="cl">    <span class="n">sequence_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">is_eos</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">completion_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">is_eos</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># shape (bs, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">(</span><span class="n">sequence_indices</span> <span class="o">&lt;=</span> <span class="n">eos_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_completions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">num_generations</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_completion_length</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Generates multiple completions for each prompt.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The language model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokenizer: The tokenizer for encoding and decoding text.
</span></span></span><span class="line"><span class="cl"><span class="s2">        prompts (list): List of text prompts.
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_generations (int): Number of completions to generate per prompt.
</span></span></span><span class="line"><span class="cl"><span class="s2">        max_completion_length (int): Maximum number of tokens to generate.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Encodes the prompts and moves them to the appropriate device.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Repeats each prompt num_generations times to generate multiple completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Generates completions using the model with specified parameters.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. Extracts the completion IDs (excluding the prompt tokens).
</span></span></span><span class="line"><span class="cl"><span class="s2">        5. Creates a mask for the completions using create_completion_mask.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&#34;attention_mask&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Input batch size: </span><span class="si">{</span><span class="n">prompt_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">, Device before model: </span><span class="si">{</span><span class="n">prompt_ids</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_length</span> <span class="o">=</span> <span class="n">prompt_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># .repeat_interleave 沿着dim=0重复num_generations次，使得prompt_ids和prompt_mask的维度都增加了num_generations倍</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_ids</span> <span class="o">=</span> <span class="n">prompt_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_generations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># shape (bs * num_generations, max_prompt_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_mask</span> <span class="o">=</span> <span class="n">prompt_mask</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_generations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># shape (bs * num_generations, max_prompt_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">prompt_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="o">=</span><span class="n">prompt_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_completion_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Output batch size: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">, Device after model: </span><span class="si">{</span><span class="n">outputs</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># completio_ids只包含模型生成的answer部分，长度为max_completion_length</span>
</span></span><span class="line"><span class="cl">    <span class="n">completion_ids</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="n">prompt_length</span><span class="p">:]</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">completion_mask</span> <span class="o">=</span> <span class="n">create_completion_mask</span><span class="p">(</span><span class="n">completion_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">prompt_ids</span><span class="p">,</span> <span class="n">prompt_mask</span><span class="p">,</span> <span class="n">completion_ids</span><span class="p">,</span> <span class="n">completion_mask</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_rollout_data</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ref_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">batch_samples</span><span class="p">,</span> <span class="n">num_generations</span><span class="p">,</span> <span class="n">max_completion_length</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Generates data for GRPO rollouts including completions and log probabilities.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The policy model being trained.
</span></span></span><span class="line"><span class="cl"><span class="s2">        ref_model: The reference model for KL divergence calculation.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokenizer: The tokenizer for encoding and decoding text.
</span></span></span><span class="line"><span class="cl"><span class="s2">        batch_samples (list): Batch of training samples.
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_generations (int): Number of completions to generate per sample.
</span></span></span><span class="line"><span class="cl"><span class="s2">        max_completion_length (int): Maximum completion length.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dict: Dictionary containing all data needed for GRPO updates.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Extracts prompts and expected answers from the batch samples.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Generates completions using the current policy model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Combines prompt and completion tokens.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. Computes log probabilities from both the policy model and reference model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        5. Formats completions for reward calculation.
</span></span></span><span class="line"><span class="cl"><span class="s2">        6. Repeats prompts and answers to match the number of generated completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        7. Returns all data needed for GRPO loss calculation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample</span><span class="p">[</span><span class="s2">&#34;prompt&#34;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">sample</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">batch_samples</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">answers</span> <span class="o">=</span> <span class="p">[</span><span class="n">sample</span><span class="p">[</span><span class="s2">&#34;answer&#34;</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">sample</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">batch_samples</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">prompt_ids</span><span class="p">,</span> <span class="n">prompt_mask</span><span class="p">,</span> <span class="n">completion_ids</span><span class="p">,</span> <span class="n">completion_mask</span> <span class="o">=</span> <span class="n">generate_completions</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">num_generations</span><span class="p">,</span> <span class="n">max_completion_length</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prompt_ids</span><span class="p">,</span> <span class="n">completion_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_prompt_length + max_completion_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">prompt_mask</span><span class="p">,</span> <span class="n">completion_mask</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_prompt_length + max_completion_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits_to_keep</span> <span class="o">=</span> <span class="n">completion_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># max_completion_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_log_probs</span> <span class="o">=</span> <span class="n">compute_log_probs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">logits_to_keep</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ref_log_probs</span> <span class="o">=</span> <span class="n">compute_log_probs</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">logits_to_keep</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">formatted_completions</span> <span class="o">=</span> <span class="p">[[{</span><span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)}]</span> <span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">completion_ids</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">repeated_prompts</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_generations</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="n">repeated_answers</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">answers</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_generations</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;input_ids&#34;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;attention_mask&#34;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;completion_mask&#34;</span><span class="p">:</span> <span class="n">completion_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;old_log_probs&#34;</span><span class="p">:</span> <span class="n">old_log_probs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;ref_log_probs&#34;</span><span class="p">:</span> <span class="n">ref_log_probs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;formatted_completions&#34;</span><span class="p">:</span> <span class="n">formatted_completions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;repeated_prompts&#34;</span><span class="p">:</span> <span class="n">repeated_prompts</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;repeated_answers&#34;</span><span class="p">:</span> <span class="n">repeated_answers</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;logits_to_keep&#34;</span><span class="p">:</span> <span class="n">logits_to_keep</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;batch_size&#34;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;num_generations&#34;</span><span class="p">:</span> <span class="n">num_generations</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grpo_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ref_model</span><span class="p">,</span> <span class="n">rollout_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">reward_function</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Computes the GRPO loss for updating the policy model.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The policy model being trained.
</span></span></span><span class="line"><span class="cl"><span class="s2">        ref_model: The reference model for KL divergence calculation.
</span></span></span><span class="line"><span class="cl"><span class="s2">        rollout_data (dict): Data generated by generate_rollout_data.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokenizer: The tokenizer for encoding and decoding text.
</span></span></span><span class="line"><span class="cl"><span class="s2">        reward_function: Function that calculates rewards for completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        beta (float): KL penalty coefficient.
</span></span></span><span class="line"><span class="cl"><span class="s2">        epsilon (float): Clipping parameter for PPO.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        torch.Tensor: The GRPO loss to be minimized.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Computes current token log probabilities using the policy model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Calculates the probability ratio between current and old policies.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Computes rewards using the provided reward_function.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. Calculates advantages by standardizing rewards within each prompt.
</span></span></span><span class="line"><span class="cl"><span class="s2">        5. Computes the PPO surrogate objective with clipping.
</span></span></span><span class="line"><span class="cl"><span class="s2">        6. Calculates the KL divergence between reference and policy models.
</span></span></span><span class="line"><span class="cl"><span class="s2">        7. Combines surrogate loss and KL penalty.
</span></span></span><span class="line"><span class="cl"><span class="s2">        8. Averages the loss across all tokens and batches.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;attention_mask&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">completion_mask</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;completion_mask&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits_to_keep</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;logits_to_keep&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">old_log_probs</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;old_log_probs&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">ref_log_probs</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;ref_log_probs&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">token_log_probs</span> <span class="o">=</span> <span class="n">compute_log_probs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">logits_to_keep</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">token_log_probs</span> <span class="o">-</span> <span class="n">old_log_probs</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward_function</span><span class="p">(</span><span class="n">prompts</span><span class="o">=</span><span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;repeated_prompts&#34;</span><span class="p">],</span> <span class="n">completions</span><span class="o">=</span><span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;formatted_completions&#34;</span><span class="p">],</span> <span class="n">answer</span><span class="o">=</span><span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;repeated_answers&#34;</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="n">device</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>  <span class="c1"># shape (bs * num_generations,)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#print(f&#34;Rewards: {rewards}&#34;)  # Debug rewards</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;batch_size&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_generations</span> <span class="o">=</span> <span class="n">rollout_data</span><span class="p">[</span><span class="s2">&#34;num_generations&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_generations</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Average Reward:&#34;</span><span class="p">,</span> <span class="n">avg_reward</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean_rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_generations</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations,)</span>
</span></span><span class="line"><span class="cl">    <span class="n">std_rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_generations</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations,)</span>
</span></span><span class="line"><span class="cl">    <span class="n">advantages</span> <span class="o">=</span> <span class="p">((</span><span class="n">rewards</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std_rewards</span> <span class="o">+</span> <span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, 1)</span>
</span></span><span class="line"><span class="cl">    <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">advantages</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">advantages</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">surrogate_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">kl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ref_log_probs</span> <span class="o">-</span> <span class="n">token_log_probs</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">ref_log_probs</span> <span class="o">-</span> <span class="n">token_log_probs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">per_token_loss</span> <span class="o">=</span> <span class="n">surrogate_loss</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>  <span class="c1"># shape (bs * num_generations, max_completion_length)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">((</span><span class="n">per_token_loss</span> <span class="o">*</span> <span class="n">completion_mask</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">completion_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># completion_mask用于把生成部分中eos token之后的部分mask掉，只保留生成部分的loss，其中每个completion都会对生成有效长度的loss进行平均，然后再对batch求平均，得到batch的loss。这里添加负号因为GRPO优化目标是最大化改值，这里用梯度下降来优化，所以这里加负号</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">avg_reward</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_with_grpo</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_generations</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_completion_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-6</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">reward_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    This function is your original working code (train_with_grpo_static)
</span></span></span><span class="line"><span class="cl"><span class="s2">    with an added outer loop for iterative GRPO updates per the pseudocode.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The language model to train.
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokenizer: The tokenizer for encoding and decoding text.
</span></span></span><span class="line"><span class="cl"><span class="s2">        train_data (list): Training dataset.
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_iterations (int): Number of outer iterations (reference model updates).
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_steps (int): Number of batch updates per iteration.
</span></span></span><span class="line"><span class="cl"><span class="s2">        batch_size (int): Number of prompts per batch.
</span></span></span><span class="line"><span class="cl"><span class="s2">        num_generations (int): Number of completions per prompt.
</span></span></span><span class="line"><span class="cl"><span class="s2">        max_completion_length (int): Maximum token length for completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        beta (float): KL penalty coefficient.
</span></span></span><span class="line"><span class="cl"><span class="s2">        learning_rate (float): Learning rate for optimizer.
</span></span></span><span class="line"><span class="cl"><span class="s2">        mu (int): Number of policy updates per batch.
</span></span></span><span class="line"><span class="cl"><span class="s2">        epsilon (float): PPO clipping parameter.
</span></span></span><span class="line"><span class="cl"><span class="s2">        reward_function: Function that calculates rewards for completions.
</span></span></span><span class="line"><span class="cl"><span class="s2">        device_ids (list): List of GPU device IDs for DataParallel.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        The trained model.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. For each outer iteration:
</span></span></span><span class="line"><span class="cl"><span class="s2">           - Creates a reference model as a deep copy of the current policy model.
</span></span></span><span class="line"><span class="cl"><span class="s2">           - Reinitializes the optimizer for the policy model.
</span></span></span><span class="line"><span class="cl"><span class="s2">           - For each training step:
</span></span></span><span class="line"><span class="cl"><span class="s2">             a. Samples a batch of examples from the training data.
</span></span></span><span class="line"><span class="cl"><span class="s2">             b. Generates rollout data including completions and log probabilities.
</span></span></span><span class="line"><span class="cl"><span class="s2">             c. For mu iterations:
</span></span></span><span class="line"><span class="cl"><span class="s2">                i. Computes the GRPO loss.
</span></span></span><span class="line"><span class="cl"><span class="s2">                ii. Updates the policy model using gradient descent.
</span></span></span><span class="line"><span class="cl"><span class="s2">           - Monitors GPU memory usage and prints progress information.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># assert device_ids is not None and len(device_ids) &gt; 1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># model = nn.DataParallel(model, device_ids=device_ids).cuda()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># print(f&#34;Model wrapped with DataParallel across GPUs: {device_ids}&#34;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># num_iterations表示ref_model迭代次数</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_iterations</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Create a reference model (deep copy) and set it to eval mode.</span>
</span></span><span class="line"><span class="cl">        <span class="n">ref_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ref_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Reference model created.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Inner loop: your original training steps.</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch_samples</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 生成经验数据</span>
</span></span><span class="line"><span class="cl">                <span class="n">rollout_data</span> <span class="o">=</span> <span class="n">generate_rollout_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">ref_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tokenizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">batch_samples</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_generations</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">max_completion_length</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对每批经验池数据学习的次数，一般设置mu=1</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">grpo_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span><span class="p">,</span> <span class="n">avg_reward</span> <span class="o">=</span> <span class="n">grpo_loss</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">ref_model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">rollout_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tokenizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">reward_function</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;loss&#34;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;average_reward&#34;</span><span class="p">:</span> <span class="n">avg_reward</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;iteration&#34;</span><span class="p">:</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;step&#34;</span><span class="p">:</span> <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="s2">&#34;grpo_iter&#34;</span><span class="p">:</span> <span class="n">grpo_iter</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">                <span class="p">})</span>
</span></span><span class="line"><span class="cl">                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_iterations</span><span class="si">}</span><span class="s2">, Step </span><span class="si">{</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_steps</span><span class="si">}</span><span class="s2">, &#34;</span>
</span></span><span class="line"><span class="cl">                      <span class="sa">f</span><span class="s2">&#34;GRPO iter </span><span class="si">{</span><span class="n">grpo_iter</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">mu</span><span class="si">}</span><span class="s2">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">optimize_model_memory</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Optimizes the model to use less memory during training.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">        model: The language model to optimize.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">        The optimized model.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Explanation:
</span></span></span><span class="line"><span class="cl"><span class="s2">        1. Sets the model to training mode.
</span></span></span><span class="line"><span class="cl"><span class="s2">        2. Disables KV caching to save memory.
</span></span></span><span class="line"><span class="cl"><span class="s2">        3. Enables gradient checkpointing to trade computation for memory.
</span></span></span><span class="line"><span class="cl"><span class="s2">        4. Ensures that input embeddings require gradients:
</span></span></span><span class="line"><span class="cl"><span class="s2">           - Either uses the built-in method if available.
</span></span></span><span class="line"><span class="cl"><span class="s2">           - Or adds a forward hook to the input embeddings layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">        5. Returns the optimized model ready for memory-efficient training.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># 不使用kv-cache缓存，减小显存消耗</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># First ensure inputs will require gradients</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&#34;enable_input_require_grads&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">make_inputs_require_grad</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">make_inputs_require_grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Then enable gradient checkpointing</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Main execution</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda:0&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&#34;/model/ztq147/Qwen/Qwen2.5-1.5B-Instruct&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&#34;/data/ztq147/temp_models/math_solver_model&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Downloading model...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">device_map</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Model downloaded&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&#34;left&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># setting the number of device</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># device_ids = list(range(8))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">all_data</span> <span class="o">=</span> <span class="n">prepare_dataset</span><span class="p">(</span><span class="s2">&#34;train&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">all_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">size_of_eval_data</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">    <span class="n">eval_data</span> <span class="o">=</span> <span class="n">all_data</span><span class="p">[:</span><span class="n">size_of_eval_data</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_data</span> <span class="o">=</span> <span class="n">all_data</span><span class="p">[</span><span class="n">size_of_eval_data</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># print(&#34;\nInital model evaluation before finetuning:&#34;)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># print(f&#34;Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%&#34;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">optimize_model_memory</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Starting RL fine-tuning using GRPO...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">training_config</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;num_iterations&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;num_steps&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;num_generations&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;gradient_accumulation_steps&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;max_completion_length&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;beta&#39;</span><span class="p">:</span> <span class="mf">0.04</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">5e-6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;epsilon&#39;</span><span class="p">:</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">wandb</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;WANDB_PROJECT&#34;</span><span class="p">],</span> <span class="n">reinit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Weights &amp; Biases initialized.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">train_with_grpo</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_data</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward_function</span><span class="o">=</span><span class="n">combined_reward</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># device_ids=device_ids,</span>
</span></span><span class="line"><span class="cl">        <span class="o">**</span><span class="n">training_config</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">wandb</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Training completed and wandb run finished.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Finalmodel evaluation after GTPO RL fine-tuning:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">post_grpo_accuracy</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">eval_data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Post-GRPO Accuracy: </span><span class="si">{</span><span class="n">post_grpo_accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Saving GTPO fine-tuned model...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="单卡实验结果">单卡实验结果<a hidden class="anchor" aria-hidden="true" href="#单卡实验结果">#</a></h3>
<p>作者使用一张3090GPU训练，实验结果如下：</p>
<ul>
<li>batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=3, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1</li>
</ul>
<p>最终的测试准确率：Accuracy: 23.33% (7/30)</p>
<p>这里横轴是1500步因为设置了mu=3，所以每个step被记录了三次，这个实验主要想看看mu的影响，个人理解mu=1的情况下，ratio的计算结果应该永远是1。</p>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image1.jpg" alt="loss曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image2.jpg" alt="reward曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<ul>
<li>batch_size=1, num_generations=8, max_completion_length=400, gradient_accumulation_steps=12, mu=1, num_steps=1500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1</li>
</ul>
<p>最终的测试准确率：Accuracy: 36.67% (11/30)</p>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image3.jpg" alt="loss曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image4.jpg" alt="reward曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<ul>
<li>batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=6, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1</li>
</ul>
<p>最终的测试准确率：Accuracy: 40.00% (12/30)</p>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image5.jpg" alt="loss曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image6.jpg" alt="reward曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<ul>
<li>batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1</li>
</ul>
<p>最终的测试准确率：Accuracy: 46.67% (14/30)</p>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image7.jpg" alt="loss曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<p><img loading="lazy" src="/my-blog/images/2025-03-05-grpo/2025-03-05-image8.jpg" alt="reward曲线"  />
</p>
<div align='center' style="color: #999999"></div>
<h3 id="单卡实验小结">单卡实验小结<a hidden class="anchor" aria-hidden="true" href="#单卡实验小结">#</a></h3>
<p>从上面简单的几次实验结果来看，最好的是batch_size=2, num_generations=8, max_completion_length=256, gradient_accumulation_steps=1, mu=1, num_steps=500, num_iterations=1, beta=0.04, learning_rate=5e-6, epsilon=0.1，准确率达到了46.67%，但是这个结果与原作者的90%的准确率有较大差距，原作者用8张80G显存的A100，batch_size设置为7（一张卡放1条数据，原文用了数据并行nn.DataParallel），最大生成长度设置为400，num_generations设置为12，mu=1，gradient_accumulation_steps=1，一共训练500步，其他参数与本文这个设置的其他参数一致。</p>
<p>整体看对结果影响较大的参数有max_completino_length和num_generations。相反，设置gradient_accumulation_steps并没能带来大batch_size的效果（这个不确定是不是代码写的有问题），反而降低性能，mu的设置大于1也降低了性能。</p>
<p>实验的loss曲线也没有原文那种稳定的上升趋势，虽然不理解为啥loss是上升的，本文贴的所有loss图均是设置了最大Y值，实际上会存在很多loss很高的脉冲（十几到几百），这个脉冲存在的原因也不太清楚。reward图和原文的有一定相似性。</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<p>[1] Shao et al. <a href="https://arxiv.org/abs/2402.03300" class="entityLink">“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”</a> arXiv preprint arXiv:2402.03300 (2024).</p>
<p>[2] John Schulman. <a href="http://joschu.net/blog/kl-approx.html" class="entityLink">“Approximating KL Divergence”</a> John Schulman&rsquo;s Homepage 2020.</p>
<p>[3] Andriy Burkov. <a href="https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb" class="entityLink">“Coding GRPO from Scratch: A Guide to Distributed Implementation with Qwen2.5-1.5B-Instruct”</a> github 2025.</p>


  </div>

  <footer class="post-footer">
      <ul class="post-tags">
        <li><a href="https://tqzhong.github.io/my-blog/tags/ai/">AI</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/nlp/">NLP</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/llm/">LLM</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/rl/">RL</a></li>
      </ul>
<nav class="paginav">
  <a class="prev" href="https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/">
    <span class="title">« Prev</span>
    <br>
    <span>大模型post-training方法——强化学习篇</span>
  </a>
  <a class="next" href="https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/">
    <span class="title">Next »</span>
    <br>
    <span>DeepSeek-V3技术报告解读</span>
  </a>
</nav>

  </footer>
  
</article>




  <div class="social-icons">
    
        
            
        
    
        
            <a href="https://github.com/tqzhong" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-github"></i>
            </a>
        
    
        
            <a href="https://x.com/rs1047g" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-twitter"></i>
            </a>
        
    
        
            <a href="https://scholar.google.com/citations?hl=en&amp;user=UNNLJX4AAAAJ" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-google"></i>
            </a>
        
    
        
            <a href="https://tqzhong.github.io" target="_blank" rel="noopener noreferrer">
            <i class="fas fa-user"></i>  
            </a>
        
    
  </div>



<script src="https://utteranc.es/client.js"
        repo="tqzhong/my-blog"
        issue-term="pathname"
        label="hugo-comment"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>

    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://tqzhong.github.io/my-blog/">Rs&#39; Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>    

</body>

</html>
