<!DOCTYPE html>
<html lang="en" dir="auto">
<a href="https://yourpersonalwebsite.com" target="_blank">

<head>
    <script src="https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/@waline/client@v3/dist/waline.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js"></script>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deepspeed多机多卡训练&amp;代码细节 | Rs&#39; Log</title>
<meta name="keywords" content="AI, LLM, NLP, Deepspeed">
<meta name="description" content="本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。
Supervised finetuning
首先在主节点克隆deepspeed-chat仓库。
使用的主要环境：


1
2
3
4
5
6
7
8
9


pip install torch==1.13.0
pip install datasets
pip install sentencepiece
pip install protobuf==3.20.3
pip install accelerate
pip install deepspeed==0.10.0
pip install transformers==4.44.2
pip install tensorboard
pip install numpy==1.26.4


deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：


1
2


sudo apt update
sudo apt install nvidia-cuda-toolkit


之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。
跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：


1
2
3
4


datasets
    └── synthetic-instruct-gptj-pairwise
        ├── dataset_infos.json
        └── train-00000-of-00001-1e5d57b93c448e7a.parquet


在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:


1
2
3
4
5
6
7
8
9


class PromptRawDataset(object):
    def __init__(self, output_path, seed, local_rank, dataset_name):
        self.output_path = output_path
        self.seed = seed
        self.local_rank = local_rank
        &#39;&#39;&#39;原始数据的读取，这里根据自己数据集作相应修改&#39;&#39;&#39;
        self.raw_datasets = load_dataset(&#39;parquet&#39;, data_files=dataset_name)

    ...


到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g&#43;&#43;环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。">
<meta name="author" content="Rs">
<link rel="canonical" href="https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link crossorigin="anonymous" href="/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css" integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as="style">


<script defer crossorigin="anonymous" src="/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" type="image/x-icon" sizes="48x48" href="images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
<link rel="icon" sizes="512x512" href="images/android-chrome-512x512.png" type="image/png">
<link rel="icon" sizes="192x192" href="images/android-chrome-192x192.png" type="image/png">
<link rel="mask-icon" href="https://tqzhong.github.io/my-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="/my-blog/css/custom.css?v=1.7"><meta property="og:title" content="Deepspeed多机多卡训练&amp;代码细节" />
<meta property="og:description" content="本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。
Supervised finetuning
首先在主节点克隆deepspeed-chat仓库。
使用的主要环境：


1
2
3
4
5
6
7
8
9


pip install torch==1.13.0
pip install datasets
pip install sentencepiece
pip install protobuf==3.20.3
pip install accelerate
pip install deepspeed==0.10.0
pip install transformers==4.44.2
pip install tensorboard
pip install numpy==1.26.4


deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：


1
2


sudo apt update
sudo apt install nvidia-cuda-toolkit


之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。
跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：


1
2
3
4


datasets
    └── synthetic-instruct-gptj-pairwise
        ├── dataset_infos.json
        └── train-00000-of-00001-1e5d57b93c448e7a.parquet


在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:


1
2
3
4
5
6
7
8
9


class PromptRawDataset(object):
    def __init__(self, output_path, seed, local_rank, dataset_name):
        self.output_path = output_path
        self.seed = seed
        self.local_rank = local_rank
        &#39;&#39;&#39;原始数据的读取，这里根据自己数据集作相应修改&#39;&#39;&#39;
        self.raw_datasets = load_dataset(&#39;parquet&#39;, data_files=dataset_name)

    ...


到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g&#43;&#43;环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-30T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deepspeed多机多卡训练&amp;代码细节"/>
<meta name="twitter:description" content="本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。
Supervised finetuning
首先在主节点克隆deepspeed-chat仓库。
使用的主要环境：


1
2
3
4
5
6
7
8
9


pip install torch==1.13.0
pip install datasets
pip install sentencepiece
pip install protobuf==3.20.3
pip install accelerate
pip install deepspeed==0.10.0
pip install transformers==4.44.2
pip install tensorboard
pip install numpy==1.26.4


deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：


1
2


sudo apt update
sudo apt install nvidia-cuda-toolkit


之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。
跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：


1
2
3
4


datasets
    └── synthetic-instruct-gptj-pairwise
        ├── dataset_infos.json
        └── train-00000-of-00001-1e5d57b93c448e7a.parquet


在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:


1
2
3
4
5
6
7
8
9


class PromptRawDataset(object):
    def __init__(self, output_path, seed, local_rank, dataset_name):
        self.output_path = output_path
        self.seed = seed
        self.local_rank = local_rank
        &#39;&#39;&#39;原始数据的读取，这里根据自己数据集作相应修改&#39;&#39;&#39;
        self.raw_datasets = load_dataset(&#39;parquet&#39;, data_files=dataset_name)

    ...


到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g&#43;&#43;环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tqzhong.github.io/my-blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deepspeed多机多卡训练\u0026代码细节",
      "item": "https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deepspeed多机多卡训练\u0026代码细节",
  "name": "Deepspeed多机多卡训练\u0026代码细节",
  "description": "本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。\nSupervised finetuning 首先在主节点克隆deepspeed-chat仓库。\n使用的主要环境：\n1 2 3 4 5 6 7 8 9 pip install torch==1.13.0 pip install datasets pip install sentencepiece pip install protobuf==3.20.3 pip install accelerate pip install deepspeed==0.10.0 pip install transformers==4.44.2 pip install tensorboard pip install numpy==1.26.4 deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：\n1 2 sudo apt update sudo apt install nvidia-cuda-toolkit 之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。\n跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：\n1 2 3 4 datasets └── synthetic-instruct-gptj-pairwise ├── dataset_infos.json └── train-00000-of-00001-1e5d57b93c448e7a.parquet 在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:\n1 2 3 4 5 6 7 8 9 class PromptRawDataset(object): def __init__(self, output_path, seed, local_rank, dataset_name): self.output_path = output_path self.seed = seed self.local_rank = local_rank \u0026#39;\u0026#39;\u0026#39;原始数据的读取，这里根据自己数据集作相应修改\u0026#39;\u0026#39;\u0026#39; self.raw_datasets = load_dataset(\u0026#39;parquet\u0026#39;, data_files=dataset_name) ... 到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。\n",
  "keywords": [
    "AI", "LLM", "NLP", "Deepspeed"
  ],
  "articleBody": "本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。\nSupervised finetuning 首先在主节点克隆deepspeed-chat仓库。\n使用的主要环境：\n1 2 3 4 5 6 7 8 9 pip install torch==1.13.0 pip install datasets pip install sentencepiece pip install protobuf==3.20.3 pip install accelerate pip install deepspeed==0.10.0 pip install transformers==4.44.2 pip install tensorboard pip install numpy==1.26.4 deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：\n1 2 sudo apt update sudo apt install nvidia-cuda-toolkit 之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。\n跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：\n1 2 3 4 datasets └── synthetic-instruct-gptj-pairwise ├── dataset_infos.json └── train-00000-of-00001-1e5d57b93c448e7a.parquet 在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:\n1 2 3 4 5 6 7 8 9 class PromptRawDataset(object): def __init__(self, output_path, seed, local_rank, dataset_name): self.output_path = output_path self.seed = seed self.local_rank = local_rank '''原始数据的读取，这里根据自己数据集作相应修改''' self.raw_datasets = load_dataset('parquet', data_files=dataset_name) ... 到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。\n单节点跑通之后就开始多节点训练，多节点训练首先每个节点需要安装pdsh工具：\n1 sudo apt install pdsh 其次多节点需要在deepspeed启动命令添加--hostfile参数以及配置NCCL参数，hostfile文件形式如下：\n1 2 3 1.2.3.4 slots=8 1.2.3.5 slots=8 ... 第一列是节点ip，第二列是该节点的gpu数量。NCCL参数配置如下：\n1 OPTIONS_NCCL=\"NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1\" 其中NCCL_SOCKET_IFNAME是服务器上可用的网络接口，可以通过ip addr show命令查看。\n然后对于每个子节点，都要配置相同的环境（见上）以及相同的代码路径结构，模型文件每个节点都要保存（这里我直接把deepspeed目录打包scp到各个节点了），数据集文件主需要存在主节点上即可。这里卡的比较久的地方是子节点训练环境的位置问题，起初我把训练环境都装在每个节点的一个conda虚拟环境里，主节点进入虚拟环境启动训练脚本，但是当通信到子节点的时候报错提示找不到相关环境：\n1 /usr/bin/python3: Error while finding module specification for 'deepspeed.launcher.launch' (ModuleNotFoundError: No module named 'deepspeed') 问题在于这里通信到子节点不会访问对应conda虚拟环境，后来我在子节点conda base下装训练环境也还是不行。最后解决方法是得在linux默认环境下（不带base）把训练依赖装好，这下马上就跑通了。应该是有在conda下也能运行的方法，后续了解了再补充。\n最终的训练脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 #!/bin/bash # Copyright (c) Microsoft Corporation. # SPDX-License-Identifier: Apache-2.0 # DeepSpeed Team OUTPUT=$1 ZERO_STAGE=$2 if [ \"$OUTPUT\" == \"\" ]; then OUTPUT=./output fi if [ \"$ZERO_STAGE\" == \"\" ]; then ZERO_STAGE=3 fi mkdir -p $OUTPUT num_nodes=3 num_gpus=8 OPTIONS_NCCL=\"NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1\" model_name_or_path=/home/iaiustc/sft-rlhf/models/opt-1.3 data_output_path=/home/iaiustc/sft-rlhf/output/cache_dir/ds_chat batch_size=2 max_seq_len=512 learning_rate=9.65e-6 weight_decay=0. num_train_epochs=1 gradient_accumulation_steps=4 lr_scheduler_type=cosine num_warmup_steps=0 seed=1234 eval_interval=100 save_interval=100 print_interval=10 ARGS=\" \\ --data_path /home/iaiustc/sft-rlhf/datasets/synthetic-instruct-gptj-pairwise/train-00000-of-00001-1e5d57b93c448e7a.parquet \\ --data_output_path ${data_output_path} \\ --data_split 2,4,4 \\ --model_name_or_path ${model_name_or_path} \\ --per_device_train_batch_size ${batch_size} \\ --per_device_eval_batch_size ${batch_size} \\ --max_seq_len ${max_seq_len} \\ --learning_rate ${learning_rate} \\ --weight_decay ${weight_decay} \\ --num_train_epochs ${num_train_epochs} \\ --gradient_accumulation_steps ${gradient_accumulation_steps} \\ --lr_scheduler_type ${lr_scheduler_type} \\ --num_warmup_steps ${num_warmup_steps} \\ --seed ${seed} \\ --zero_stage $ZERO_STAGE \\ --deepspeed \\ --enable_tensorboard \\ --tensorboard_path $OUTPUT \\ --output_dir $OUTPUT \\ --eval_interval ${eval_interval} --save_interval ${save_interval} --print_interval ${print_interval} \" if [[ ${num_nodes} -gt 1 ]]; then # create hostfile if num_nodes \u003e 1 python create_hostfile.py hostfile_arg=\"--hostfile ./output/hostfile\" else hostfile_arg=\"\" fi deepspeed --num_nodes ${num_nodes} --num_gpus ${num_gpus} \\ ${hostfile_arg} --master_port 12346 \\ main.py \"$@\" ${ARGS} 2\u003e\u00261 | tee \"${OUTPUT}/training2.log\" 至此关于使用deepspeed进行多机多卡做sft训练就完成了，后续关于reward model以及rlhf的训练应该差不多，等实现完后更新。\nReward Model Reward Model本质上就是base model添加一个projction_head头得到的，projction_head头是把base model最后一层输出的hidden_states投影到1维上。因此在多机多卡的训练执行所需基本调整和Supervised Finetuning一样，这里主要记录一下RewardModel类的几个主要功能函数实现细节。\n1. forward函数 DeepSpeed-Chat/dschat/utils/model/reward_model.py\nforward函数用于RM训练计算训练损失以及训练chosen数据和rejected数据的平均得分，也是一种训练参考指标。RM训练损失函数\n$$ \\mathcal L_R=-\\mathbb E_{(x,y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(r_\\phi(x, y_w)-r_\\phi(x,y_l))] $$ 具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class RewardModel(nn.Model): def __init__(self, ...): ... ... def forward(self, input_ids=None, ...): transformer_outputs = self.rwtransformer(...) # 输出最后一层特征 # hidden_states.shape (bs*2, max_seq_len, hidden_size)，数据是前一半为chosen部分，后一半为rejected部分 hidden_states = transformer_outputs[0] # rewards.shape: (bs*2, max_seq_len) rewards = self.v_head(hidden_states).squeeze(-1) bs = input_ids.shape[0] // 2 chosen_ids = input_ids[:bs] rejected_ids = input_ids[bs:] chosen_rewards = rewards[:bs] rejected_rewards = rewards[bs:] chosen_mean_scores = [] rejected_mean_scores = [] loss = 0. for i in range(bs): # (max_seq_len, ) chosen_id = chosen_ids[i] rejected_id = rejected_ids[i] # (bs, max_seq_len) chosen_reward = chosen_rewards[i] rejected_reward = rejected_rewards[i] # 得到chosen_id张量中元素为0的坐标，例如a = torch.tensor([1,2,3,0,0,0]),(a == 0).nonzero()为torch.tensor([[3],[4],[5]]);如果a = torch.tensor([[1,2,3,0,0,0]]), (a == 0).nonzero()为torch.tensor([[0,3],[0,4],[0,5]]) c_inds = (chosen_id == self.PAD_ID).nonzero() # c_ind为chosen_sentence的answer后的第一个pad_token的index，例如chosen_id=torch.tensor([1,2,3,0,0,0]) 那么c_ind=3 # num_padding_at_beginning这个参数主的出现主要由于opt系列模型在input前有一个固定数量（等于1）的padding token (，和bert有点像)，对于其他autoregression模型没有这种。源码用了opt模型因此这里num_padding_at_beginning设置为1 c_ind = c_inds[self.num_padding_at_beginning].item() if len(c_inds) \u003e self.num_padding_at_beginning else seq_len check_divergence = (chosen_id != rejected_id).nonzero() # 如果当前chosen_sentence和rejected_sentence完全相同,这对数据只计算末位的损失(?) if len(check_divergence) == 0: end_ind = rejected_reward.size(-1) divergence_ind = end_ind - 1 r_ind = c_ind else: r_inds = (rejected_id == self.PAD_ID).nonzero() r_ind = r_inds[self.num_padding_at_beginning].item() if len(r_inds) \u003e self.num_padding_at_beginning else seq_len # end_ind 为c_ind,r_ind两者大值，即计算损失的有效末尾index end_ind = max(c_ind, r_ind) # divergence_ind为chosen_sentence和reject_sentence两者answer的第一个token的index，即计算损失的有效起始index divergence_ind = check_divergence[0] assert divergence_ind \u003e 0 c_truncated_reward = chosen_reward[divergence_ind:end_ind] r_truncated_reward = rejected_reward[divergence_ind:end_ind] # 这两个mean_scores只保留有效answer部分末尾的reward chosen_mean_scores.append(chosen_reward[c_ind - 1]) rejected_mean_scores.append(rejected_reward[r_ind - 1]) loss += -torch.nn.functional.logsigmoid(c_truncated_reward - r_truncated_reward).mean() loss = loss / bs # (bs, ) chosen_mean_scores = torch.stack(chosen_mean_scores) rejected_mean_scores = torch.stack(rejected_mean_scores) return { \"loss\": loss, \"chosen_mean_scores\": chosen_mean_scores, \"rejected_mean_scores\": rejected_mean_scores, } 2. forward_value函数 DeepSpeed-Chat/dschat/utils/model/reward_model.py\nforward_value函数主要用于rlhf阶段reward model和critic model前向计算reward和value，因此这里的input_ids输入不再是chosen和rejected一半一半，而是基于prompt生成的sequence。具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class RewardModel(nn.Model): def __init__(self, ...): ... ... def forward_value(self, input_ids, ...): transformer_outputs = self.rwtransformer(input_ids, ...) hidden_states = transformer_outputs[0] # (bs, max_seq_len) values = self.v_head(hidden_states).squeeze(-1) if return_value_only: return values else: assert prompt_length \u003e 1, \"prompt_length must be greater than 1 to help select the end score\" bs = values.size(0) seq_len = input_ids.shape[1] chosen_end_scores = [] for i in range(bs): input_id = input_ids[i] value = values[i] # c_ind和forward中含义一样，也是有效answer后的第一个pad_token的index，这里之所以先去除prompt_length部分的id进行计算最后再加上prompt_length，主要因为在rlhf阶段，prompt数据集也是按batch从dataloader中获取，所以prompt中也存在padding（padding + true_prompt），所以这里是为了去除prompt中padding的干扰 c_inds = (input_id[prompt_length:] == self.PAD_ID).nonzero() c_ind = c_inds[0].item() + prompt_length if len(c_inds) \u003e 0 else seq_len chosen_end_scores.append(value[c_ind - 1]) return { \"values\": values, # (bs, max_seq_len) \"chosen_end_scores\": torch.stack(chosen_end_scores), # (bs,) } RLHF RLHF阶段在代码跑通上与Supervised Finetuning和Reward Model训练的设置一致，前面跑通了，这一阶段基本改一下输入参数就可以直接跑，因此这里主要记录deepspeed关于rlhf部分的实现细节。主要函数均在在DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py脚本的DeepSpeedPPOTrainer类中。\n1. _generate_sequence函数 _generate_sequence函数输入一个batch（bs）的prompts数据，生成一个seq_bs的sequence，这里面主要对生成answer的长度做了过滤，生成的sequence的answer部分长度小于等于1的数据会被扔掉，所以输出sequence的维度变为seq_bs。\n2. generate_experience函数 generate_experience函数用于生成经验数据，输入是一个batch（bs）的prompts数据，输出包括reference model的logprobs，actor model的logprobs，reward model的rewards，critic model的value，以及sequence的input_ids和attention_mask\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def generate_experience(self, prompts, mask): # 所有4个模型进入eval模式 self.eval() seq = self._generate_sequence(prompts, mask, step) if seq is None: assert self.last_generated_experience is not None, f'Invalid generated experience at step={step}' prompts = self.last_generated_experience['prompts'] seq = self.last_generated_experience['seq'] else: self.last_generated_experience = {'prompts': prompts, 'seq': seq} # actor model和critic model进入train模式 self.train() pad_token_id = self.tokenizer.pad_token_id attention_mask = seq.not_equal(pad_token_id).long() with torch.no_grad(): output = self.actor_model(seq, attention_mask=attention_mask) output_ref = self.ref_model(seq, attention_mask=attention_mask) reward_score = self.reward_model.forward_value(seq, attention_mask, prompt_length=self.prompt_length)['chosen_end_scores'].detach() values = self.critic_model.forward_value(seq, attention_mask, return_value_only=True).detach()[:, :-1] logits = output.logits logits_ref = output_ref.logits return { 'prompt': prompts, # (bs, max_prompt_len) 'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]), # (seq_bs, max_seq_len - 1) 'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:, 1:]), # (seq_bs, max_seq_len - 1) 'value': values, # (seq_bs, max_seq_len - 1) 'rewards': reward_score, # (seq_bs, ) 'input_ids': seq, # (seq_bs, max_seq_len - 1) 'attention_mask': attention_mask # (seq_bs, max_seq_len) } 3. compute_rewards函数 首先介绍rlhf中的reward计算公式\n$$ r_{KL}=r(x,y)-\\beta log\\frac{\\pi^{RL}_{old}(y|x)}{\\pi^{SFT}(y|x)} $$ 具体代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_rewards(self, prompts, log_probs, ref_log_probs, reward_score, action_mask): kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs) rewards = kl_divergence_estimate # (bs, max_seq_len - 1) start = prompts.shape[1] - 1 # ends为batch中各个数据的最后一个有效token的index，是一个数组 # 这里分开prompt部分单独计算也是由于prompts中存在padding ends = start + action_mask[:, start:].sum(1) + 1 # RM得到的奖励值限定在一定范围 reward_clip = torch.clamp(reward_score, -self.clip_reward_value, self.clip_reward_value) batch_size = log_probs.shape[0] for j in range(batch_size): rewards[j, start:end[j]][-1] += reward_clip[j] return rewards # (bs, max_seq_len - 1) 4. actor_loss_fn函数 在一个ppo_batch中，actor损失计算公式\n$$ pg\\_loss=E_{\\tau\\sim\\pi_{old}^{RL}}E_{(s_t,a_t)\\sim\\tau}[max(-\\hat A_t\\cdot\\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},-\\hat A_t\\cdot clip(\\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},1-\\epsilon,1+\\epsilon))] $$ 其中$\\tau$指的仅是“answer”部分内容，不包括“prompt”部分。\n1 2 3 4 5 6 7 8 9 def actor_loss_fn(self, logprobs, old_logprobs, advantages, mask): # policy gradient loss # 重要性采样权重计算 ratio = exp(log(new) - log(old)) log_ratio = (logprobs - old_logprobs) * mask ratio = torch.exp(log_ratio) pg_loss1 = -advantages * ratio pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange, 1.0 + self.cliprange) pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum() return pg_loss 5. critic_loss_fn函数 在一个ppo_batch中，critic的损失计算公式：1）裁剪新价值估计$V_{new}$，使其不至于太偏离采集经验时的旧价值估计，使得经验回放仍能有效：\n$$ V_{clip}=clip(V_{new}, V_{old}-\\phi,V_{old}+\\phi) $$ 2）critic拟合回报R： $$ vf\\_loss=\\frac{1}{2}\\cdot E_{\\tau\\sim\\pi_{old}^{RL}}E_{s_t\\sim\\tau}[max((V_{new}(s_t)-R_t)^2, (V_{clip}(s_t)-R_t)^2)] $$ 其中$\\tau$指的仅是“answer”部分内容，不包括“prompt”部分。\n1 2 3 4 5 6 7 def critic_loss_fn(self, values, old_values, returns, mask): # value loss values_clipped = torch.clamp(values, old_values - self.cliprange_value, old_values + self.cliprange_value) vf_loss1 = (values - returns) ** 2 vf_loss2 = (values_clipped - returns) ** 2 vf_loss = 0.5 * torch.sum(torch.max(vf_loss1, vf_loss2) * mask) / mask.sum() return vf_loss 6. get_advantages_and_returns函数 优势advantages的计算，包括本框架在内的多数框架的advantages实现并非纯粹TD-error，而是在TD-error基础上结合MC方法，即GAE（广义优势估计）。具体来说，对于全长尾T的轨迹来说，其某个时间步t的优势为（$\\lambda=1$时，advantage完全使用MC方法；$\\lambda=0$时，advantage完全使用TD-error方法）：\n$$ \\hat A_t=\\delta_t+(\\gamma\\lambda)\\delta_{t+1}+(\\gamma\\lambda)^2\\delta_{t+2}+\\dots+(\\gamma\\lambda)^{T-t+1}\\delta_{T-1}\\\\ where\\ \\delta_t=r_{KL,t}+\\gamma\\cdot V_{old}(s_{t+1})-V_{old}(s_t) $$ 回报returns的计算，returns就是奖励reward的累计，对于全长为T的轨迹来说，其到达某个时间步$t$时的回报为： $$ R_t=\\hat A_t+V_t $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def get_advantages_and_returns(self, values, rewards, start): lastgaelam = 0 advantages_reversed = [] length = rewards.size(-1) # 反向遍历各个时间步的优势advantage for t in reversed(range(start, length)): # 获取下个时间步的价值估计V_{old}(s_{t+1}) nextvalues = values[:, t + 1] if t \u003c length - 1 else 0.0 # 计算单步TD-error delta = rewards[:, t] + self.gamma * nextvalues - values[:, t] # 累计优势 lastgaelam = delta + self.gamma * self.lam * lastgaelam # 存储各个时间步的优势 advantages_reversed.append(lastgaelam) # 对逆序的优势列表进行正序处理，得到正常时间步排列的优势 advantages = torch.stack(advantages_reversed[::-1], dim=1) # (seq_bs, max_seq_len - 1 - start) # return_t = adv_t + v(s_t) # 通过优势计算得到回报 returns = advantages + values[:, start:] # (bs, max_seq_len - 1 - start) return advantages.detach(), returns 7. train_rlhf函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def train_rlhf(self, inputs): # inputs为一个ppo_batch的generate_experience函数返回值 prompts = inputs['prompts'] log_probs = inputs['logprobs'] ref_log_probs = inputs['ref_logprobs'] reward_score = inputs['rewards'] values = inputs['value'] attention_mask = inputs['attention_mask'] seq = inputs['input_ids'] start = prompts.size()[-1] - 1 # max_prompt_len - 1 action_mask = attention_mask[:, 1:] # (ppo_bs, max_seq_len - 1) # 利用经验池中旧的logprobs, ref_logprobs以及reward_score计算KL-reward，并利用KL-reward和旧的values计算advantages和returns old_values = values # (ppo_bs, max_seq_len - 1) with torch.no_grad(): # old_rewards (ppo_bs, max_seq_len - 1) old_rewards = self.compute_rewards(prompts, log_probs, ref_log_probs, reward_score, action_mask) ends = start + action_mask[:, start:].sum(1) + 1 # 将reward和value中padding部分的值置零不然advantage和return计算会出错 for i in range(old_rewards.shape[0]): old_rewards[i, ends[i]:] = 0 old_values[i, end[i]:] = 0 # advantages (ppo_bs, max_seq_len - max_prompt_len) # returns (ppo_bs, max_seq_len - max_prompt_len) advantages, returns = self.get_advantages_and_returns(old_values, old_rewards, start) batch = {'input_ids': seq, 'attention_mask': attention_mask} # 利用当前最新actor model计算最新logprob，计算actor_loss并更新actor model参数 actor_prob = self.actor_model(**batch, use_cache=False).logits actor_log_prob = gather_log_probs(actor_prob[:, :-1, :], seq[:, 1:]) actor_loss = self.actor_loss_fn(actor_log_prob[:, start:], log_probs[:, start:], advantages, action_mask[:, start:]) self.actor_model.backward(actor_loss) self.actor_model.step() # 利用当前最新critic model计算最新value，计算critic_loss并更新critic model参数，完成一个ppo batch数据的训练 value = self.critic_model.forward_value(**batch, return_value_only=True, use_cache=False)[:, :-1] critic_loss = self.critic_loss_fn(value[:, start:], old_values[:, start:], returns, action_mask[:, start:]) self.critic_model.backward(critic_loss) self.critic_model.step() return actor_loss, critic_loss PPO训练数据管理-MiniDataset /DeepSpeed-Chat/dschat/utils/data/data_utils.py\nMiniDataset是一个进一步划分ppo训练时数据的一个类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class MiniDataset: def __init__(self, max_size, small_batch_size): # max_size为进行划分ppo训练数据时的normal batch容量，比如max_size=2，batch=4则意味着当dataset中含有两个batch（8条数据）时，开始划分ppo batch。 # small_batch_size为ppo训练时的batch大小，即ppo_batch self.dataset = [] self.max_size = max_size self.small_batch_size = small_batch_size def seperate(self): # 当self.dataset长度达到max_size时，开始划分ppo_batch # 假设max_size=2, small_batch_size=3, normal batch_size=4 # 划分前self.dataset=[[d0,d1,d2,d3], [d4,d5,d6,d7]]，划分后的small_dataset应该为[[d0,d1,d2], [d3], [d4,d5,d5], [d7]] small_dataset = [] for large_batch in self.dataset: if type(large_batch) == list or type(large_batch) == tuple: # large_size即normal batch size large_size = len(large_batch[0]) elif type(large_batch) == dict: large_size = len(large_batch[list(large_batch.keys())[0]]) else: large_size = len(large_batch) for i in range(0, large_size, self.small_batch_size): if type(large_batch) == list or type(large_batch) == tuple: small_dataset.append([x[i:i + self.small_batch_size] for x in large_batch]) elif type(large_batch) == dict: small_dataset.append({k: v[i:i + self.small_batch_size] for k, v in large_batch.items()}) else: small_dataset.append(large_batch[i: i + self.small_batch_size]) self.free() return small_dataset def add(self, data): if len(self.dataset) \u003c self.max_size: self.dataset.append(data) if len(self.dataset) == self.max_size: return self.seperate() else: return None else: raise ValueError('xx') def free(self): self.dataset = [] main.py中训练主循环 DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py\n不考虑unsupervised数据，记录rlhf训练主函数循环流程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 exp_mini_dataset = MiniDataset(args.generation_batches, args.per_device_training_batch_size) for epoch in range(args.num_train_epochs): for step, batch_prompt in enumerate(prompt_train_dataloader): batch_prompt = to_device(batch_prompt, device) # 计算当前batch prompt的经验数据 out = trainer.generate_experience(batch_prompt['prompt'], batch_prompt['prompt_att_mask'], step) # 添加当前批经验数据，达到args.generation_batches时划分成ppo_batch数据进行训练，否则继续添加 exp_dataset = exp_mini_dataset.add(out) if exp_dataset is not None: inner_iter = 0 actor_loss_sum, critic_loss_sum = 0, 0 average_reward = 0 if ppo_ep in range(args.ppo_epochs): for i, exp_data in enumerate(exp_dataset): actor_loss, critic_loss = trainer.train_rlhf(exp_data) actor_loss_sum += actor_loss.item() critic_loss_sum += critic_loss.item() average_reward += exp_data['rewards'].mean() inner_iter += 1 random.shuffle(exp_dataset) print_rank_0(f\"{epoch} | {step} | {ppo_ep+1} | {actor_loss_sum / inner_iter} | {critic_loss_sum / inner_iter}\") 总结就是每args.generation_batches个batch数据使用当前{actor, ref, critic, reward}模型生成一批经验数据，这批经验数据构建ppo_batch训练数据开始进行args.ppo_epochs轮训练，期间每个ppo_epoch的每个inner_iter对{actor, critic}模型做一步参数更新，每次完成当前经验数据全部ppo_epochs训练后打印平均{actor_loss, critic_loss, average_reward}。直到训练完prompt_dataloader中的prompt数据结束一个大epoch，基于此循环args.num_train_epochs次。\nReferences [1] InstructGPT高效实践——【DeepSpeed-Chat】源码详解(2/3)：Supervised Finetuning、Reward Model Finetuning\n[2] InstructGPT高效实践——【DeepSpeed-Chat】源码详解(3/3)：RLHF Finetuning\n",
  "wordCount" : "1867",
  "inLanguage": "en",
  "datePublished": "2024-10-30T00:00:00Z",
  "dateModified": "2024-10-30T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Rs"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rs' Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tqzhong.github.io/my-blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true
          },
          svg: {
            fontCache: 'global'
          }
        };
    </script>
    
    
    
    <nav class="nav">
        <div class="logo-logo-switches">
        <div class="logo">
            <a href="https://tqzhong.github.io/my-blog/" accesskey="h" title="Rs&#39; Log (Alt + H)">Rs&#39; Log</a>

            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch">
                </ul>
            </div>
        </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tqzhong.github.io/my-blog/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://tqzhong.github.io/my-blog/faq/" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>


    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Deepspeed多机多卡训练&amp;代码细节
    </h1>
    <div class="post-meta">


<i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2024-10-30 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;9 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span id="view-counter"></span>

<script src="https://cdn.jsdelivr.net/npm/leancloud-storage@4.12.0/dist/av-min.js"></script>
<script>
    
    AV.initialize("AokLJzaIvwVNJW0b2F0YTLLy-MdYXbMMI", "fwZpRfBG259O3LscJFPW3ViH");

    loadViewCount(location.pathname, "view-counter");

    
    
    
        
    
    
    
    
    
    
    
    
    
                
    
    
    
    
    


    var hasViewCounted = false;

    
    function loadViewCount(pageUrl, elementId) {
        if (hasViewCounted) return;
        hasViewCounted = true;       

        var counter = document.getElementById(elementId);
        var cachedViews = localStorage.getItem("view-count-" + pageUrl);

        
        if (cachedViews) {
            counter.innerText = cachedViews;
        } else {
            counter.innerText = "0"; 
        }

        
        var query = new AV.Query('Counter');
        query.equalTo("url", pageUrl);
        query.find().then(results => {
            if (results.length > 0) {
                var counterObj = results[0];
                counterObj.increment("views", 1);
                counterObj.save();
                var views = counterObj.get("views");
                counter.innerText = views;
                localStorage.setItem("view-count-" + pageUrl, views); 
            } else {
                var Counter = AV.Object.extend("Counter");
                var newCounter = new Counter();
                newCounter.set("url", pageUrl);
                newCounter.set("views", 1);
                newCounter.save().then(() => {
                    counter.innerText = "1";
                    localStorage.setItem("view-count-" + pageUrl, 1); 
                });
            }
        }).catch(function(error) {
            console.error("error:", error);
        });
    }
    
</script>



</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#supervised-finetuning" aria-label="Supervised finetuning">Supervised finetuning</a></li>
                <li>
                    <a href="#reward-model" aria-label="Reward Model">Reward Model</a><ul>
                        
                <li>
                    <a href="#1-forward%e5%87%bd%e6%95%b0" aria-label="1. forward函数">1. forward函数</a></li>
                <li>
                    <a href="#2-forward_value%e5%87%bd%e6%95%b0" aria-label="2. forward_value函数">2. forward_value函数</a></li></ul>
                </li>
                <li>
                    <a href="#rlhf" aria-label="RLHF">RLHF</a><ul>
                        
                <li>
                    <a href="#1-_generate_sequence%e5%87%bd%e6%95%b0" aria-label="1. _generate_sequence函数">1. _generate_sequence函数</a></li>
                <li>
                    <a href="#2-generate_experience%e5%87%bd%e6%95%b0" aria-label="2. generate_experience函数">2. generate_experience函数</a></li>
                <li>
                    <a href="#3-compute_rewards%e5%87%bd%e6%95%b0" aria-label="3. compute_rewards函数">3. compute_rewards函数</a></li>
                <li>
                    <a href="#4-actor_loss_fn%e5%87%bd%e6%95%b0" aria-label="4. actor_loss_fn函数">4. actor_loss_fn函数</a></li>
                <li>
                    <a href="#5-critic_loss_fn%e5%87%bd%e6%95%b0" aria-label="5. critic_loss_fn函数">5. critic_loss_fn函数</a></li>
                <li>
                    <a href="#6-get_advantages_and_returns%e5%87%bd%e6%95%b0" aria-label="6. get_advantages_and_returns函数">6. get_advantages_and_returns函数</a></li>
                <li>
                    <a href="#7-train_rlhf%e5%87%bd%e6%95%b0" aria-label="7. train_rlhf函数">7. train_rlhf函数</a></li>
                <li>
                    <a href="#ppo%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e7%ae%a1%e7%90%86-minidataset" aria-label="PPO训练数据管理-MiniDataset">PPO训练数据管理-MiniDataset</a></li>
                <li>
                    <a href="#mainpy%e4%b8%ad%e8%ae%ad%e7%bb%83%e4%b8%bb%e5%be%aa%e7%8e%af" aria-label="main.py中训练主循环">main.py中训练主循环</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。</p>
<h3 id="supervised-finetuning">Supervised finetuning<a hidden class="anchor" aria-hidden="true" href="#supervised-finetuning">#</a></h3>
<p>首先在主节点克隆<a href="https://github.com/microsoft/DeepSpeedExamples" class="entityLink">deepspeed-chat</a>仓库。</p>
<p>使用的主要环境：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install <span class="nv">torch</span><span class="o">==</span>1.13.0
</span></span><span class="line"><span class="cl">pip install datasets
</span></span><span class="line"><span class="cl">pip install sentencepiece
</span></span><span class="line"><span class="cl">pip install <span class="nv">protobuf</span><span class="o">==</span>3.20.3
</span></span><span class="line"><span class="cl">pip install accelerate
</span></span><span class="line"><span class="cl">pip install <span class="nv">deepspeed</span><span class="o">==</span>0.10.0
</span></span><span class="line"><span class="cl">pip install <span class="nv">transformers</span><span class="o">==</span>4.44.2
</span></span><span class="line"><span class="cl">pip install tensorboard
</span></span><span class="line"><span class="cl">pip install <span class="nv">numpy</span><span class="o">==</span>1.26.4
</span></span></code></pre></td></tr></table>
</div>
</div><p>deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt update
</span></span><span class="line"><span class="cl">sudo apt install nvidia-cuda-toolkit
</span></span></code></pre></td></tr></table>
</div>
</div><p>之后先跑通单节点，我用的是<code>step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh</code>，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。</p>
<p>跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了<code>synthetic-instruct-gptj-pairwise</code>数据集，两个文件保存在主节点：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">datasets
</span></span><span class="line"><span class="cl">    └── synthetic-instruct-gptj-pairwise
</span></span><span class="line"><span class="cl">        ├── dataset_infos.json
</span></span><span class="line"><span class="cl">        └── train-00000-of-00001-1e5d57b93c448e7a.parquet
</span></span></code></pre></td></tr></table>
</div>
</div><p>在<code>dschat/utils/data/raw_datasets.py</code>的数据集类<code>PromptRawDataset</code>上也做了对应修改:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PromptRawDataset</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_path</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">local_rank</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">output_path</span> <span class="o">=</span> <span class="n">output_path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="n">local_rank</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;原始数据的读取，这里根据自己数据集作相应修改&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;parquet&#39;</span><span class="p">,</span> <span class="n">data_files</span><span class="o">=</span><span class="n">dataset_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是<code>FusedAdam</code>，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成<code>AdamW</code>就跑通了。查了一下<code>FusedAdam</code>在需要大量计算资源的场景下有一定优势。</p>
<p>单节点跑通之后就开始多节点训练，多节点训练首先每个节点需要安装pdsh工具：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt install pdsh
</span></span></code></pre></td></tr></table>
</div>
</div><p>其次多节点需要在deepspeed启动命令添加<code>--hostfile</code>参数以及配置NCCL参数，hostfile文件形式如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">1.2.3.4 <span class="nv">slots</span><span class="o">=</span><span class="m">8</span>
</span></span><span class="line"><span class="cl">1.2.3.5 <span class="nv">slots</span><span class="o">=</span><span class="m">8</span>
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>第一列是节点ip，第二列是该节点的gpu数量。NCCL参数配置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">OPTIONS_NCCL</span><span class="o">=</span><span class="s2">&#34;NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中NCCL_SOCKET_IFNAME是服务器上可用的网络接口，可以通过<code>ip addr show</code>命令查看。</p>
<p>然后对于每个子节点，都要配置相同的环境（见上）以及相同的代码路径结构，模型文件每个节点都要保存（这里我直接把deepspeed目录打包scp到各个节点了），数据集文件主需要存在主节点上即可。这里卡的比较久的地方是子节点训练环境的位置问题，起初我把训练环境都装在每个节点的一个conda虚拟环境里，主节点进入虚拟环境启动训练脚本，但是当通信到子节点的时候报错提示找不到相关环境：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/python3: Error <span class="k">while</span> finding module specification <span class="k">for</span> <span class="s1">&#39;deepspeed.launcher.launch&#39;</span> <span class="o">(</span>ModuleNotFoundError: No module named <span class="s1">&#39;deepspeed&#39;</span><span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>问题在于这里通信到子节点不会访问对应conda虚拟环境，后来我在子节点conda base下装训练环境也还是不行。最后解决方法是得在linux默认环境下（不带base）把训练依赖装好，这下马上就跑通了。应该是有在conda下也能运行的方法，后续了解了再补充。</p>
<p>最终的训练脚本：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="cp">#!/bin/bash
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="c1"># Copyright (c) Microsoft Corporation.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># SPDX-License-Identifier: Apache-2.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># DeepSpeed Team</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">OUTPUT</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl"><span class="nv">ZERO_STAGE</span><span class="o">=</span><span class="nv">$2</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$OUTPUT</span><span class="s2">&#34;</span> <span class="o">==</span> <span class="s2">&#34;&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OUTPUT</span><span class="o">=</span>./output
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$ZERO_STAGE</span><span class="s2">&#34;</span> <span class="o">==</span> <span class="s2">&#34;&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ZERO_STAGE</span><span class="o">=</span><span class="m">3</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">mkdir -p <span class="nv">$OUTPUT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">num_nodes</span><span class="o">=</span><span class="m">3</span>
</span></span><span class="line"><span class="cl"><span class="nv">num_gpus</span><span class="o">=</span><span class="m">8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">OPTIONS_NCCL</span><span class="o">=</span><span class="s2">&#34;NCCL_DEBUG=warn NCCL_SOCKET_IFNAME=enp59s0f0 NCCL_IB_GID_INDEX=3 NCCL_IB_HCA=mlx5_2:1,mlx5_2:1 NCCL_IB_SL=3 NCCL_CHECKS_DISABLE=1 NCCL_P2P_DISABLE=0 NCCL_LL_THRESHOLD=16384 NCCL_IB_CUDA_SUPPORT=1&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">model_name_or_path</span><span class="o">=</span>/home/iaiustc/sft-rlhf/models/opt-1.3
</span></span><span class="line"><span class="cl"><span class="nv">data_output_path</span><span class="o">=</span>/home/iaiustc/sft-rlhf/output/cache_dir/ds_chat
</span></span><span class="line"><span class="cl"><span class="nv">batch_size</span><span class="o">=</span><span class="m">2</span>
</span></span><span class="line"><span class="cl"><span class="nv">max_seq_len</span><span class="o">=</span><span class="m">512</span>
</span></span><span class="line"><span class="cl"><span class="nv">learning_rate</span><span class="o">=</span>9.65e-6
</span></span><span class="line"><span class="cl"><span class="nv">weight_decay</span><span class="o">=</span>0.
</span></span><span class="line"><span class="cl"><span class="nv">num_train_epochs</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">gradient_accumulation_steps</span><span class="o">=</span><span class="m">4</span>
</span></span><span class="line"><span class="cl"><span class="nv">lr_scheduler_type</span><span class="o">=</span>cosine
</span></span><span class="line"><span class="cl"><span class="nv">num_warmup_steps</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl"><span class="nv">seed</span><span class="o">=</span><span class="m">1234</span>
</span></span><span class="line"><span class="cl"><span class="nv">eval_interval</span><span class="o">=</span><span class="m">100</span>
</span></span><span class="line"><span class="cl"><span class="nv">save_interval</span><span class="o">=</span><span class="m">100</span>
</span></span><span class="line"><span class="cl"><span class="nv">print_interval</span><span class="o">=</span><span class="m">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">ARGS</span><span class="o">=</span><span class="s2">&#34; \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --data_path /home/iaiustc/sft-rlhf/datasets/synthetic-instruct-gptj-pairwise/train-00000-of-00001-1e5d57b93c448e7a.parquet \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --data_output_path </span><span class="si">${</span><span class="nv">data_output_path</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --data_split 2,4,4 \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --model_name_or_path </span><span class="si">${</span><span class="nv">model_name_or_path</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --per_device_train_batch_size </span><span class="si">${</span><span class="nv">batch_size</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --per_device_eval_batch_size </span><span class="si">${</span><span class="nv">batch_size</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --max_seq_len </span><span class="si">${</span><span class="nv">max_seq_len</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --learning_rate </span><span class="si">${</span><span class="nv">learning_rate</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --weight_decay </span><span class="si">${</span><span class="nv">weight_decay</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --num_train_epochs </span><span class="si">${</span><span class="nv">num_train_epochs</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --gradient_accumulation_steps </span><span class="si">${</span><span class="nv">gradient_accumulation_steps</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --lr_scheduler_type </span><span class="si">${</span><span class="nv">lr_scheduler_type</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --num_warmup_steps </span><span class="si">${</span><span class="nv">num_warmup_steps</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --seed </span><span class="si">${</span><span class="nv">seed</span><span class="si">}</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --zero_stage </span><span class="nv">$ZERO_STAGE</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --deepspeed \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --enable_tensorboard \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --tensorboard_path </span><span class="nv">$OUTPUT</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --output_dir </span><span class="nv">$OUTPUT</span><span class="s2"> \
</span></span></span><span class="line"><span class="cl"><span class="s2">    --eval_interval </span><span class="si">${</span><span class="nv">eval_interval</span><span class="si">}</span><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    --save_interval </span><span class="si">${</span><span class="nv">save_interval</span><span class="si">}</span><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    --print_interval </span><span class="si">${</span><span class="nv">print_interval</span><span class="si">}</span><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">num_nodes</span><span class="si">}</span> -gt <span class="m">1</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># create hostfile if num_nodes &gt; 1</span>
</span></span><span class="line"><span class="cl">    python create_hostfile.py
</span></span><span class="line"><span class="cl">    <span class="nv">hostfile_arg</span><span class="o">=</span><span class="s2">&#34;--hostfile ./output/hostfile&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span>
</span></span><span class="line"><span class="cl">    <span class="nv">hostfile_arg</span><span class="o">=</span><span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">deepspeed --num_nodes <span class="si">${</span><span class="nv">num_nodes</span><span class="si">}</span> --num_gpus <span class="si">${</span><span class="nv">num_gpus</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>          <span class="si">${</span><span class="nv">hostfile_arg</span><span class="si">}</span> --master_port <span class="m">12346</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>          main.py <span class="s2">&#34;</span><span class="nv">$@</span><span class="s2">&#34;</span> <span class="si">${</span><span class="nv">ARGS</span><span class="si">}</span> 2&gt;<span class="p">&amp;</span><span class="m">1</span> <span class="p">|</span> tee <span class="s2">&#34;</span><span class="si">${</span><span class="nv">OUTPUT</span><span class="si">}</span><span class="s2">/training2.log&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>至此关于使用deepspeed进行多机多卡做sft训练就完成了，后续关于reward model以及rlhf的训练应该差不多，等实现完后更新。</p>
<h3 id="reward-model">Reward Model<a hidden class="anchor" aria-hidden="true" href="#reward-model">#</a></h3>
<p>Reward Model本质上就是base model添加一个projction_head头得到的，projction_head头是把base model最后一层输出的hidden_states投影到1维上。因此在多机多卡的训练执行所需基本调整和Supervised Finetuning一样，这里主要记录一下RewardModel类的几个主要功能函数实现细节。</p>
<h4 id="1-forward函数">1. forward函数<a hidden class="anchor" aria-hidden="true" href="#1-forward函数">#</a></h4>
<p>DeepSpeed-Chat/dschat/utils/model/reward_model.py</p>
<p>forward函数用于RM训练计算训练损失以及训练chosen数据和rejected数据的平均得分，也是一种训练参考指标。RM训练损失函数</p>
<div class="scroll-container">
$$
\mathcal L_R=-\mathbb E_{(x,y_w, y_l)\sim\mathcal D}[log\ \sigma(r_\phi(x, y_w)-r_\phi(x,y_l))]
$$
</div>
<p>具体实现</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RewardModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rwtransformer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出最后一层特征</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># hidden_states.shape (bs*2, max_seq_len, hidden_size)，数据是前一半为chosen部分，后一半为rejected部分</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># rewards.shape: (bs*2, max_seq_len)</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">chosen_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:</span><span class="n">bs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">rejected_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">bs</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">chosen_rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[:</span><span class="n">bs</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">rejected_rewards</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">bs</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">chosen_mean_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">rejected_mean_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># (max_seq_len, )</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_id</span> <span class="o">=</span> <span class="n">chosen_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">rejected_id</span> <span class="o">=</span> <span class="n">rejected_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># (bs, max_seq_len)</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_reward</span> <span class="o">=</span> <span class="n">chosen_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">rejected_reward</span> <span class="o">=</span> <span class="n">rejected_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 得到chosen_id张量中元素为0的坐标，例如a = torch.tensor([1,2,3,0,0,0]),(a == 0).nonzero()为torch.tensor([[3],[4],[5]]);如果a = torch.tensor([[1,2,3,0,0,0]]), (a == 0).nonzero()为torch.tensor([[0,3],[0,4],[0,5]])</span>
</span></span><span class="line"><span class="cl">            <span class="n">c_inds</span> <span class="o">=</span> <span class="p">(</span><span class="n">chosen_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">PAD_ID</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># c_ind为chosen_sentence的answer后的第一个pad_token的index，例如chosen_id=torch.tensor([1,2,3,0,0,0]) 那么c_ind=3</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># num_padding_at_beginning这个参数主的出现主要由于opt系列模型在input前有一个固定数量（等于1）的padding token (&lt;/s&gt;，和bert有点像)，对于其他autoregression模型没有这种。源码用了opt模型因此这里num_padding_at_beginning设置为1</span>
</span></span><span class="line"><span class="cl">            <span class="n">c_ind</span> <span class="o">=</span> <span class="n">c_inds</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_padding_at_beginning</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_inds</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_padding_at_beginning</span> <span class="k">else</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">check_divergence</span> <span class="o">=</span> <span class="p">(</span><span class="n">chosen_id</span> <span class="o">!=</span> <span class="n">rejected_id</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果当前chosen_sentence和rejected_sentence完全相同,这对数据只计算末位的损失(?)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">check_divergence</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">end_ind</span> <span class="o">=</span> <span class="n">rejected_reward</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">divergence_ind</span> <span class="o">=</span> <span class="n">end_ind</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">                <span class="n">r_ind</span> <span class="o">=</span> <span class="n">c_ind</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">r_inds</span> <span class="o">=</span> <span class="p">(</span><span class="n">rejected_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">PAD_ID</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">r_ind</span> <span class="o">=</span> <span class="n">r_inds</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_padding_at_beginning</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">r_inds</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_padding_at_beginning</span> <span class="k">else</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># end_ind 为c_ind,r_ind两者大值，即计算损失的有效末尾index</span>
</span></span><span class="line"><span class="cl">                <span class="n">end_ind</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">c_ind</span><span class="p">,</span> <span class="n">r_ind</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># divergence_ind为chosen_sentence和reject_sentence两者answer的第一个token的index，即计算损失的有效起始index</span>
</span></span><span class="line"><span class="cl">                <span class="n">divergence_ind</span> <span class="o">=</span> <span class="n">check_divergence</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">divergence_ind</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">c_truncated_reward</span> <span class="o">=</span> <span class="n">chosen_reward</span><span class="p">[</span><span class="n">divergence_ind</span><span class="p">:</span><span class="n">end_ind</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">r_truncated_reward</span> <span class="o">=</span> <span class="n">rejected_reward</span><span class="p">[</span><span class="n">divergence_ind</span><span class="p">:</span><span class="n">end_ind</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 这两个mean_scores只保留有效answer部分末尾的reward</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_mean_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chosen_reward</span><span class="p">[</span><span class="n">c_ind</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">rejected_mean_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rejected_reward</span><span class="p">[</span><span class="n">r_ind</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">logsigmoid</span><span class="p">(</span><span class="n">c_truncated_reward</span> <span class="o">-</span> <span class="n">r_truncated_reward</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">bs</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (bs, )</span>
</span></span><span class="line"><span class="cl">        <span class="n">chosen_mean_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">chosen_mean_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">rejected_mean_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">rejected_mean_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;loss&#34;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;chosen_mean_scores&#34;</span><span class="p">:</span> <span class="n">chosen_mean_scores</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;rejected_mean_scores&#34;</span><span class="p">:</span> <span class="n">rejected_mean_scores</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="2-forward_value函数">2. forward_value函数<a hidden class="anchor" aria-hidden="true" href="#2-forward_value函数">#</a></h4>
<p>DeepSpeed-Chat/dschat/utils/model/reward_model.py</p>
<p>forward_value函数主要用于rlhf阶段reward model和critic model前向计算reward和value，因此这里的input_ids输入不再是chosen和rejected一半一半，而是基于prompt生成的sequence。具体实现</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RewardModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rwtransformer</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">transformer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># (bs, max_seq_len)</span>
</span></span><span class="line"><span class="cl">        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">return_value_only</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">values</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">prompt_length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;prompt_length must be greater than 1 to help select the end score&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">bs</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">chosen_end_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">input_id</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">value</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="c1"># c_ind和forward中含义一样，也是有效answer后的第一个pad_token的index，这里之所以先去除prompt_length部分的id进行计算最后再加上prompt_length，主要因为在rlhf阶段，prompt数据集也是按batch从dataloader中获取，所以prompt中也存在padding（padding + true_prompt），所以这里是为了去除prompt中padding的干扰</span>
</span></span><span class="line"><span class="cl">                <span class="n">c_inds</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_id</span><span class="p">[</span><span class="n">prompt_length</span><span class="p">:]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">PAD_ID</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">c_ind</span> <span class="o">=</span> <span class="n">c_inds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="n">prompt_length</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_inds</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">                <span class="n">chosen_end_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="n">c_ind</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;values&#34;</span><span class="p">:</span> <span class="n">values</span><span class="p">,</span> <span class="c1"># (bs, max_seq_len)</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;chosen_end_scores&#34;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">chosen_end_scores</span><span class="p">),</span> <span class="c1"># (bs,)</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="rlhf">RLHF<a hidden class="anchor" aria-hidden="true" href="#rlhf">#</a></h3>
<p>RLHF阶段在代码跑通上与Supervised Finetuning和Reward Model训练的设置一致，前面跑通了，这一阶段基本改一下输入参数就可以直接跑，因此这里主要记录deepspeed关于rlhf部分的实现细节。主要函数均在在DeepSpeed-Chat/dschat/rlhf/ppo_trainer.py脚本的DeepSpeedPPOTrainer类中。</p>
<h4 id="1-_generate_sequence函数">1. _generate_sequence函数<a hidden class="anchor" aria-hidden="true" href="#1-_generate_sequence函数">#</a></h4>
<p>_generate_sequence函数输入一个batch（bs）的prompts数据，生成一个seq_bs的sequence，这里面主要对生成answer的长度做了过滤，生成的sequence的answer部分长度小于等于1的数据会被扔掉，所以输出sequence的维度变为seq_bs。</p>
<h4 id="2-generate_experience函数">2. generate_experience函数<a hidden class="anchor" aria-hidden="true" href="#2-generate_experience函数">#</a></h4>
<p>generate_experience函数用于生成经验数据，输入是一个batch（bs）的prompts数据，输出包括reference model的logprobs，actor model的logprobs，reward model的rewards，critic model的value，以及sequence的input_ids和attention_mask</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_experience</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 所有4个模型进入eval模式</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_sequence</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">seq</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_generated_experience</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Invalid generated experience at step=</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s1">&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="n">prompts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_generated_experience</span><span class="p">[</span><span class="s1">&#39;prompts&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_generated_experience</span><span class="p">[</span><span class="s1">&#39;seq&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">last_generated_experience</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;prompts&#39;</span><span class="p">:</span> <span class="n">prompts</span><span class="p">,</span> <span class="s1">&#39;seq&#39;</span><span class="p">:</span> <span class="n">seq</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># actor model和critic model进入train模式</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">seq</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_model</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_ref</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_model</span><span class="o">.</span><span class="n">forward_value</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">prompt_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prompt_length</span><span class="p">)[</span><span class="s1">&#39;chosen_end_scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_model</span><span class="o">.</span><span class="n">forward_value</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">return_value_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits_ref</span> <span class="o">=</span> <span class="n">output_ref</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;prompt&#39;</span><span class="p">:</span> <span class="n">prompts</span><span class="p">,</span> <span class="c1"># (bs, max_prompt_len)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;logprobs&#39;</span><span class="p">:</span> <span class="n">gather_log_probs</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">seq</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="c1"># (seq_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;ref_logprobs&#39;</span><span class="p">:</span> <span class="n">gather_log_probs</span><span class="p">(</span><span class="n">logits_ref</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">seq</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="c1"># (seq_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">values</span><span class="p">,</span> <span class="c1"># (seq_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="n">reward_score</span><span class="p">,</span> <span class="c1"># (seq_bs, )</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">seq</span><span class="p">,</span> <span class="c1"># (seq_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">attention_mask</span> <span class="c1"># (seq_bs, max_seq_len)</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="3-compute_rewards函数">3. compute_rewards函数<a hidden class="anchor" aria-hidden="true" href="#3-compute_rewards函数">#</a></h4>
<p>首先介绍rlhf中的reward计算公式</p>
<div class="scroll-container">
$$
r_{KL}=r(x,y)-\beta log\frac{\pi^{RL}_{old}(y|x)}{\pi^{SFT}(y|x)}
$$
</div>
<p>具体代码实现</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_rewards</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">ref_log_probs</span><span class="p">,</span> <span class="n">reward_score</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">kl_divergence_estimate</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">kl_ctl</span> <span class="o">*</span> <span class="p">(</span><span class="n">log_probs</span> <span class="o">-</span> <span class="n">ref_log_probs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span> <span class="o">=</span> <span class="n">kl_divergence_estimate</span> <span class="c1"># (bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">prompts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ends为batch中各个数据的最后一个有效token的index，是一个数组</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这里分开prompt部分单独计算也是由于prompts中存在padding</span>
</span></span><span class="line"><span class="cl">    <span class="n">ends</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">action_mask</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># RM得到的奖励值限定在一定范围</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward_clip</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">reward_score</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clip_reward_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_reward_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewards</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">[</span><span class="n">j</span><span class="p">]][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward_clip</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rewards</span> <span class="c1"># (bs, max_seq_len - 1)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="4-actor_loss_fn函数">4. actor_loss_fn函数<a hidden class="anchor" aria-hidden="true" href="#4-actor_loss_fn函数">#</a></h4>
<p>在一个ppo_batch中，actor损失计算公式</p>
<div class="scroll-container">
$$
pg\_loss=E_{\tau\sim\pi_{old}^{RL}}E_{(s_t,a_t)\sim\tau}[max(-\hat A_t\cdot\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},-\hat A_t\cdot clip(\frac{p_{new}^{RL}(a_t|s_t)}{p_{old}^{RL}(a_t|s_t)},1-\epsilon,1+\epsilon))]
$$
</div>
<p>其中$\tau$指的仅是“answer”部分内容，不包括“prompt”部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">actor_loss_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logprobs</span><span class="p">,</span> <span class="n">old_logprobs</span><span class="p">,</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># policy gradient loss</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 重要性采样权重计算 ratio = exp(log(new) - log(old))</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_ratio</span> <span class="o">=</span> <span class="p">(</span><span class="n">logprobs</span> <span class="o">-</span> <span class="n">old_logprobs</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
</span></span><span class="line"><span class="cl">    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_ratio</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pg_loss1</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">ratio</span>
</span></span><span class="line"><span class="cl">    <span class="n">pg_loss2</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cliprange</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cliprange</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pg_loss1</span><span class="p">,</span> <span class="n">pg_loss2</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">pg_loss</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="5-critic_loss_fn函数">5. critic_loss_fn函数<a hidden class="anchor" aria-hidden="true" href="#5-critic_loss_fn函数">#</a></h4>
<p>在一个ppo_batch中，critic的损失计算公式：1）裁剪新价值估计$V_{new}$，使其不至于太偏离采集经验时的旧价值估计，使得经验回放仍能有效：</p>
<div class="scroll-container">
$$
V_{clip}=clip(V_{new}, V_{old}-\phi,V_{old}+\phi)
$$
</div>
2）critic拟合回报R：
<div class="scroll-container">
$$
vf\_loss=\frac{1}{2}\cdot E_{\tau\sim\pi_{old}^{RL}}E_{s_t\sim\tau}[max((V_{new}(s_t)-R_t)^2, (V_{clip}(s_t)-R_t)^2)]
$$
</div>
<p>其中$\tau$指的仅是“answer”部分内容，不包括“prompt”部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">critic_loss_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">old_values</span><span class="p">,</span> <span class="n">returns</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># value loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">values_clipped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">old_values</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cliprange_value</span><span class="p">,</span> <span class="n">old_values</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cliprange_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">vf_loss1</span> <span class="o">=</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">vf_loss2</span> <span class="o">=</span> <span class="p">(</span><span class="n">values_clipped</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">vf_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vf_loss1</span><span class="p">,</span> <span class="n">vf_loss2</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">vf_loss</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="6-get_advantages_and_returns函数">6. get_advantages_and_returns函数<a hidden class="anchor" aria-hidden="true" href="#6-get_advantages_and_returns函数">#</a></h4>
<p>优势advantages的计算，包括本框架在内的多数框架的advantages实现并非纯粹TD-error，而是在TD-error基础上结合MC方法，即GAE（广义优势估计）。具体来说，对于全长尾T的轨迹来说，其某个时间步t的优势为（$\lambda=1$时，advantage完全使用MC方法；$\lambda=0$时，advantage完全使用TD-error方法）：</p>
<div class="scroll-container">
$$
\hat A_t=\delta_t+(\gamma\lambda)\delta_{t+1}+(\gamma\lambda)^2\delta_{t+2}+\dots+(\gamma\lambda)^{T-t+1}\delta_{T-1}\\
where\ \delta_t=r_{KL,t}+\gamma\cdot V_{old}(s_{t+1})-V_{old}(s_t)
$$
</div>
回报returns的计算，returns就是奖励reward的累计，对于全长为T的轨迹来说，其到达某个时间步$t$时的回报为：
<div class="scroll-container">
$$
R_t=\hat A_t+V_t
$$
</div>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_advantages_and_returns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">lastgaelam</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">advantages_reversed</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">length</span> <span class="o">=</span> <span class="n">rewards</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 反向遍历各个时间步的优势advantage</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 获取下个时间步的价值估计V_{old}(s_{t+1})</span>
</span></span><span class="line"><span class="cl">        <span class="n">nextvalues</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">length</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算单步TD-error</span>
</span></span><span class="line"><span class="cl">        <span class="n">delta</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">nextvalues</span> <span class="o">-</span> <span class="n">values</span><span class="p">[:,</span> <span class="n">t</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 累计优势</span>
</span></span><span class="line"><span class="cl">        <span class="n">lastgaelam</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lam</span> <span class="o">*</span> <span class="n">lastgaelam</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 存储各个时间步的优势</span>
</span></span><span class="line"><span class="cl">        <span class="n">advantages_reversed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lastgaelam</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 对逆序的优势列表进行正序处理，得到正常时间步排列的优势</span>
</span></span><span class="line"><span class="cl">    <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">advantages_reversed</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (seq_bs, max_seq_len - 1 - start)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># return_t = adv_t + v(s_t)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 通过优势计算得到回报</span>
</span></span><span class="line"><span class="cl">    <span class="n">returns</span> <span class="o">=</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">values</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:]</span> <span class="c1"># (bs, max_seq_len - 1 - start)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">advantages</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">returns</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="7-train_rlhf函数">7. train_rlhf函数<a hidden class="anchor" aria-hidden="true" href="#7-train_rlhf函数">#</a></h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_rlhf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># inputs为一个ppo_batch的generate_experience函数返回值</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompts</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;prompts&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">ref_log_probs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;ref_logprobs&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward_score</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">values</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">prompts</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># max_prompt_len - 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">action_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="c1"># (ppo_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 利用经验池中旧的logprobs, ref_logprobs以及reward_score计算KL-reward，并利用KL-reward和旧的values计算advantages和returns</span>
</span></span><span class="line"><span class="cl">    <span class="n">old_values</span> <span class="o">=</span> <span class="n">values</span> <span class="c1"># (ppo_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># old_rewards (ppo_bs, max_seq_len - 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_rewards</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">ref_log_probs</span><span class="p">,</span> <span class="n">reward_score</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">ends</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">action_mask</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将reward和value中padding部分的值置零不然advantage和return计算会出错</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">old_rewards</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">            <span class="n">old_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">ends</span><span class="p">[</span><span class="n">i</span><span class="p">]:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">old_values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span><span class="p">[</span><span class="n">i</span><span class="p">]:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># advantages (ppo_bs, max_seq_len - max_prompt_len)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># returns (ppo_bs, max_seq_len - max_prompt_len)</span>
</span></span><span class="line"><span class="cl">        <span class="n">advantages</span><span class="p">,</span> <span class="n">returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_advantages_and_returns</span><span class="p">(</span><span class="n">old_values</span><span class="p">,</span> <span class="n">old_rewards</span><span class="p">,</span> <span class="n">start</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">seq</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 利用当前最新actor model计算最新logprob，计算actor_loss并更新actor model参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">actor_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">    <span class="n">actor_log_prob</span> <span class="o">=</span> <span class="n">gather_log_probs</span><span class="p">(</span><span class="n">actor_prob</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">seq</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="n">actor_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_loss_fn</span><span class="p">(</span><span class="n">actor_log_prob</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:],</span> <span class="n">log_probs</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:],</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">actor_model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">actor_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">actor_model</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 利用当前最新critic model计算最新value，计算critic_loss并更新critic model参数，完成一个ppo batch数据的训练</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_model</span><span class="o">.</span><span class="n">forward_value</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">,</span> <span class="n">return_value_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">critic_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_loss_fn</span><span class="p">(</span><span class="n">value</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:],</span> <span class="n">old_values</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:],</span> <span class="n">returns</span><span class="p">,</span> <span class="n">action_mask</span><span class="p">[:,</span> <span class="n">start</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">critic_model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">critic_loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">critic_model</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="ppo训练数据管理-minidataset">PPO训练数据管理-MiniDataset<a hidden class="anchor" aria-hidden="true" href="#ppo训练数据管理-minidataset">#</a></h4>
<p>/DeepSpeed-Chat/dschat/utils/data/data_utils.py</p>
<p>MiniDataset是一个进一步划分ppo训练时数据的一个类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MiniDataset</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_size</span><span class="p">,</span> <span class="n">small_batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># max_size为进行划分ppo训练数据时的normal batch容量，比如max_size=2，batch=4则意味着当dataset中含有两个batch（8条数据）时，开始划分ppo batch。</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># small_batch_size为ppo训练时的batch大小，即ppo_batch</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span> <span class="o">=</span> <span class="n">max_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">small_batch_size</span> <span class="o">=</span> <span class="n">small_batch_size</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">seperate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 当self.dataset长度达到max_size时，开始划分ppo_batch</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 假设max_size=2, small_batch_size=3, normal batch_size=4</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 划分前self.dataset=[[d0,d1,d2,d3], [d4,d5,d6,d7]]，划分后的small_dataset应该为[[d0,d1,d2], [d3], [d4,d5,d5], [d7]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">small_dataset</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">large_batch</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># large_size即normal batch size</span>
</span></span><span class="line"><span class="cl">                <span class="n">large_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">large_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_batch</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">large_batch</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">large_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">large_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">small_batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">small_dataset</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">small_batch_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">large_batch</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">large_batch</span><span class="p">)</span> <span class="o">==</span> <span class="nb">dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">small_dataset</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">small_batch_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">large_batch</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">small_dataset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">large_batch</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">small_batch_size</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">free</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">small_dataset</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_size</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">seperate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;xx&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">free</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="mainpy中训练主循环">main.py中训练主循环<a hidden class="anchor" aria-hidden="true" href="#mainpy中训练主循环">#</a></h4>
<p>DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py</p>
<p>不考虑unsupervised数据，记录rlhf训练主函数循环流程</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">exp_mini_dataset</span> <span class="o">=</span> <span class="n">MiniDataset</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">generation_batches</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">per_device_training_batch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_train_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch_prompt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">prompt_train_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_prompt</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">batch_prompt</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前batch prompt的经验数据</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">generate_experience</span><span class="p">(</span><span class="n">batch_prompt</span><span class="p">[</span><span class="s1">&#39;prompt&#39;</span><span class="p">],</span> <span class="n">batch_prompt</span><span class="p">[</span><span class="s1">&#39;prompt_att_mask&#39;</span><span class="p">],</span> <span class="n">step</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 添加当前批经验数据，达到args.generation_batches时划分成ppo_batch数据进行训练，否则继续添加</span>
</span></span><span class="line"><span class="cl">        <span class="n">exp_dataset</span> <span class="o">=</span> <span class="n">exp_mini_dataset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">exp_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">inner_iter</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">actor_loss_sum</span><span class="p">,</span> <span class="n">critic_loss_sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="n">average_reward</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">ppo_ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ppo_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">exp_data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">exp_dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">actor_loss</span><span class="p">,</span> <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train_rlhf</span><span class="p">(</span><span class="n">exp_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">actor_loss_sum</span> <span class="o">+=</span> <span class="n">actor_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">critic_loss_sum</span> <span class="o">+=</span> <span class="n">critic_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">average_reward</span> <span class="o">+=</span> <span class="n">exp_data</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="n">inner_iter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">exp_dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">print_rank_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">ppo_ep</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">actor_loss_sum</span> <span class="o">/</span> <span class="n">inner_iter</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">critic_loss_sum</span> <span class="o">/</span> <span class="n">inner_iter</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>总结就是每args.generation_batches个batch数据使用当前{actor, ref, critic, reward}模型生成一批经验数据，这批经验数据构建ppo_batch训练数据开始进行args.ppo_epochs轮训练，期间每个ppo_epoch的每个inner_iter对{actor, critic}模型做一步参数更新，每次完成当前经验数据全部ppo_epochs训练后打印平均{actor_loss, critic_loss, average_reward}。直到训练完prompt_dataloader中的prompt数据结束一个大epoch，基于此循环args.num_train_epochs次。</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<p>[1] <a href="https://blog.csdn.net/Remixa/article/details/130655635" class="entityLink">InstructGPT高效实践——【DeepSpeed-Chat】源码详解(2/3)：Supervised Finetuning、Reward Model Finetuning</a></p>
<p>[2] <a href="https://blog.csdn.net/Remixa/article/details/130666878" class="entityLink">InstructGPT高效实践——【DeepSpeed-Chat】源码详解(3/3)：RLHF Finetuning</a></p>


  </div>

  <footer class="post-footer">
      <ul class="post-tags">
        <li><a href="https://tqzhong.github.io/my-blog/tags/ai/">AI</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/llm/">LLM</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/nlp/">NLP</a></li>
        <li><a href="https://tqzhong.github.io/my-blog/tags/deepspeed/">Deepspeed</a></li>
      </ul>
<nav class="paginav">
  <a class="prev" href="https://tqzhong.github.io/my-blog/posts/2024-11-21-reinforcement-learning/">
    <span class="title">« Prev</span>
    <br>
    <span>强化学习笔记</span>
  </a>
  <a class="next" href="https://tqzhong.github.io/my-blog/posts/llm-post-training/">
    <span class="title">Next »</span>
    <br>
    <span>大模型post-training方法</span>
  </a>
</nav>

  </footer>
  
</article>




  <div class="social-icons">
    
        
            
        
    
        
            <a href="https://github.com/tqzhong" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-github"></i>
            </a>
        
    
        
            <a href="https://x.com/rs1047g" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-twitter"></i>
            </a>
        
    
        
            <a href="https://scholar.google.com/citations?hl=en&amp;user=UNNLJX4AAAAJ" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-google"></i>
            </a>
        
    
        
            <a href="https://tqzhong.github.io" target="_blank" rel="noopener noreferrer">
            <i class="fas fa-user"></i>  
            </a>
        
    
  </div>



<script src="https://utteranc.es/client.js"
        repo="tqzhong/my-blog"
        issue-term="pathname"
        label="hugo-comment"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>

    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://tqzhong.github.io/my-blog/">Rs&#39; Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>    

</body>

</html>
