<!doctype html><html lang=en dir=auto><a href=https://yourpersonalwebsite.com target=_blank><head><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>大模型post-training方法 | Rs' Log</title>
<meta name=keywords content="AI,LLM,NLP,post-training"><meta name=description content="


1. DPO
Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：

$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$

2. Simple-DPO
Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：

$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

因此Simple-DPO考虑将奖励函数表达式改为：

$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma>0)$，表达式如下：

$$
p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$

从而，Simple-DPO的优化函数：

$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$

3. KTO
KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。"><meta name=author content="Rs"><link rel=canonical href=https://tqzhong.github.io/my-blog/posts/llm-post-training/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css><link crossorigin=anonymous href=/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon type=image/x-icon sizes=48x48 href=images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=images/favicon-32x32.png><link rel=apple-touch-icon href=images/apple-touch-icon.png><link rel=icon sizes=512x512 href=images/android-chrome-512x512.png type=image/png><link rel=icon sizes=192x192 href=images/android-chrome-192x192.png type=image/png><link rel=mask-icon href=https://tqzhong.github.io/my-blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tqzhong.github.io/my-blog/posts/llm-post-training/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href="/my-blog/css/custom.css?v=1.7"><meta property="og:title" content="大模型post-training方法"><meta property="og:description" content="


1. DPO
Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：

$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$

2. Simple-DPO
Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：

$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

因此Simple-DPO考虑将奖励函数表达式改为：

$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma>0)$，表达式如下：

$$
p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$

从而，Simple-DPO的优化函数：

$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$

3. KTO
KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。"><meta property="og:type" content="article"><meta property="og:url" content="https://tqzhong.github.io/my-blog/posts/llm-post-training/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="大模型post-training方法"><meta name=twitter:description content="


1. DPO
Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：

$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$

2. Simple-DPO
Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：

$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

因此Simple-DPO考虑将奖励函数表达式改为：

$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$

此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma>0)$，表达式如下：

$$
p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$

从而，Simple-DPO的优化函数：

$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$

3. KTO
KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tqzhong.github.io/my-blog/posts/"},{"@type":"ListItem","position":2,"name":"大模型post-training方法","item":"https://tqzhong.github.io/my-blog/posts/llm-post-training/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"大模型post-training方法","name":"大模型post-training方法","description":" 1. DPO Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}+\\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：\n$$ \\mathcal L_{DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})] $$ 2. Simple-DPO Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：\n$$ p_\\theta(y|x)=\\frac{1}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 因此Simple-DPO考虑将奖励函数表达式改为：\n$$ r_{SimPO}(x, y)=\\frac{\\beta}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\\gamma \\ (\\gamma\u0026gt;0)$，表达式如下：\n$$ p(y_w\u003ey_l|x)=\\sigma(r(x,y_w)-r(x,y_l)-\\gamma) $$ 从而，Simple-DPO的优化函数： $$ \\mathcal L_{SimPO}(\\pi_\\theta)=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma(\\frac{\\beta}{|y_w|}log\\ \\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}log\\ \\pi_\\theta(y_l|x)-\\gamma)] $$ 3. KTO KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。\n","keywords":["AI","LLM","NLP","post-training"],"articleBody":" 1. DPO Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}+\\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：\n$$ \\mathcal L_{DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})] $$ 2. Simple-DPO Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：\n$$ p_\\theta(y|x)=\\frac{1}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{1}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 因此Simple-DPO考虑将奖励函数表达式改为：\n$$ r_{SimPO}(x, y)=\\frac{\\beta}{|y|}log\\ \\pi_\\theta(y|x)=\\frac{\\beta}{|y|}\\sum_{i=1}^{|y|}log\\ \\pi_\\theta(y_i|x,y_{\u003c i}) $$ 此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\\gamma \\ (\\gamma\u003e0)$，表达式如下：\n$$ p(y_w\u003ey_l|x)=\\sigma(r(x,y_w)-r(x,y_l)-\\gamma) $$ 从而，Simple-DPO的优化函数： $$ \\mathcal L_{SimPO}(\\pi_\\theta)=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma(\\frac{\\beta}{|y_w|}log\\ \\pi_\\theta(y_w|x)-\\frac{\\beta}{|y_l|}log\\ \\pi_\\theta(y_l|x)-\\gamma)] $$ 3. KTO KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。\n前景理论（prospect theory），Tversky \u0026 Kahneman用下面方程建模人类价值： $$ v(z,z_{ref};\\alpha,\\lambda) = \\begin{cases} (z-z_{ref})^\\alpha \u0026 \\text{if } z \\ge z_{ref} \\\\ -\\lambda(z_{ref}-z)^\\alpha \u0026 \\text{if } z \u003c z_{ref} \\end{cases} $$ 价值函数$v:z\\rightarrow R$将一个输出$z$想对一个参考值$z_{ref}$映射到其感知价值，反映人类相比起相同大小回报，对损失的敏感性更大，其中$\\alpha$控制价值变化速度，$\\lambda$反应对损失的敏感程度。\n基于上述效用方程，作者做了一定修改使其更适合模型训练，损失函数如下：\n$$ \\mathcal L_{KTO}(\\pi_\\theta,\\pi_{ref})=\\mathbb E_{x,y\\sim\\mathcal D}[w(y)(1-v_{KTO}(x, y;\\beta))] $$ 其中 $$ r_{KTO}(x,y)=\\beta log\\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}\\\\\\\\ z_{ref}=\\mathbb E_{x^\\prime\\sim\\mathcal D}[\\beta KL(\\pi_\\theta(y^\\prime|x^\\prime)||\\pi_{ref}(y^\\prime|x^\\prime))]\\\\\\\\ v_{KTO}(x)=\\begin{cases} \\sigma(r_{KTO}(x,y)-z_{ref})\u0026 if\\ y\\sim y_{desirable}|x\\\\ \\sigma(z_{ref}-r_{KTO}(x, y))\u0026 if\\ y\\sim y_{undesirable}|x \\end{cases}\\\\\\\\ w(y)=\\begin{cases} \\lambda_D\u0026 if\\ y\\sim y_{desirable}|x\\\\ \\lambda_U\u0026 if\\ y\\sim y_{undesirable}|x \\end{cases} $$ 4. Step-DPO Step-DPO (Lai et al. (2024))主要解决大模型处理数学这类需要较长reasoning过程效果不佳的问题，作者指出用传统DPO训练数学类偏好数据存在一定缺陷，即数学类问题的reasoning过程往往是在中间的一些step开始出现错误，而前面的step推理是正确的，因此直接将整条数据归类为win或者lose都不合适。基于此，作者提出Step-DPO方法，即在initial steps之后对数据划分成win和lose，具体损失函数如下：\n$$ \\mathcal L_{Step-DPO}(\\theta)=-\\mathbb E_{x,s_{1\\sim k-1},s_{win},s_{lose}\\sim\\mathcal D}[log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(s_{win}|x;s_{1\\sim k-1})}{\\pi_{ref}(s_{win}|x;s_{1\\sim k-1})})-\\beta log\\frac{\\pi_\\theta(s_{lose}|x;s_{1\\sim k-1})}{\\pi_{ref}(s_{lose}|x;s_{1\\sim k-1})}] $$ 此外本文提出了一种偏好数据构建流水线，来获取偏好数据${x,s_{1\\sim k-1},s_{win},s_{lose}}$，具体流程如下：\nStep1: 收集数学问题$x$和相关答案$\\hat y$的数据集$D_0={(x,\\hat y)}$，用模型$\\pi_{ref}$使用问题$x$进行推理（添加CoT），得到每条问题的推理过程以及最终答案，从所有数据中挑选最终答案出错的数据得到子数据集$D_1={(x,\\hat y, y)}$，其中$y$为对应的错误答案的generations。\nStep2: 针对数据集$D_1$，其中$y=s1,s2,\\dots,s_n$，可以拆分成一系列推理步骤，通过人工或者GPT4逐条检测这些推理步骤直至发现推理出错的那一步$k$，将$s_k$作为$s_{lose}$，从而得到子数据集$D_2={(x,\\hat y,s_{1\\sim k-1},s_{lose})|x\\in D_1}$。\nStep3: 为了获取正确的推理步骤，通过$D_2$中的数据，使用$\\pi_{ref}$模型基于问题$x$和前面$k-1$步正确推理作为prompt进行inference得到$y_{cont}$: $$ y_{cont}\\sim\\pi_{ref}(y|x;s_{1\\sim k-1}) $$ 从得到正确答案的$y_{cont}$中，选择其中的第一个推理步骤作为$s_{win}$，得到最终的数据集$D_{final}={(x,s_{1\\sim k-1},s_{lose},s_{win})|x\\in D_2}$。\n需要注意的一点是，在Step3中，可能出现最终答案正确但是中间推理步骤错误的情况，因此这里需要人工或者GPT4对$s_{win}$做筛选。\n5. ORPO Hong et al. (2024)考虑到SFT不能预防模型生成lose response，同时考虑使用偏好对齐如DPO等方法需要reference model开销较大且流程繁琐，因此本文提出了一种在SFT方法上增加一个能兼顾DPO这类偏好对齐的损失，具体做法：\n对于一条训练数据${x, y}$，其对数似然概率：\n$$ logP_\\theta(y|x)=\\frac{1}{m}\\sum_{t=1}^mlog\\ P_\\theta(y_t|x,y_{\u003c t}) $$ 定义Odds Ratio $odds_\\theta(y|x)=\\frac{P_\\theta(y|x)}{1-P_\\theta(y|x)}$表示模型生成$y$相比不生成$y$的概率倍数，基于此定义$OR_\\theta(y_w, y_l)$： $$ OR_\\theta(y_w,y_l)=\\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)} $$ 表示模型在给定$x$条件下相比于生成$y_l$，更可能生成$y_w$的程度。最终改良后的SFT损失函数： $$ \\mathcal L_{ORPO}=\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[\\mathcal L_{SFT}+\\lambda\\cdot\\mathcal L_{OR}]\\\\\\\\ \\mathcal L_{OR}=-log\\ \\sigma(log\\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)}) $$ 6. R-DPO Park et al. (2024)考虑DPO在长度控制上的不足：容易生成过长啰嗦的文本，考虑在DPO的损失中引入对长度的约束，具体损失函数如下：\n$$ \\mathcal L_{R-DPO}(\\pi_\\theta;\\pi_{ref})=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[log\\ \\sigma (\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-\\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})+\\alpha|y_w|-\\alpha|y_l|] $$ 7. CPO Xu et al. (2024)简化DPO (Rafailov et al. (2023))损失函数，将$\\pi_{ref}$用均匀分布替代，并在偏好对齐中再次引入SFT损失约束，具体损失函数如下：\n$$ \\mathcal L_{CPO}=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D}[ log\\ \\sigma(\\beta log\\ \\pi_\\theta(y_w|x)-\\beta log\\ \\pi_\\theta(y_l|x))+\\lambda\\ log\\ \\pi_\\theta(y_w|x)] $$ 8. sDPO Kim et al. (2024)提出了一个简单有效的DPO改进方案，作者发现DPO的损失优化存在理论下界\n$$ \\begin{align*} \\mathcal L_{DPO}(\\pi_\\theta, \\pi_{ref}) \u0026=-\\mathbb E_{(x, y_w, y_l)\\sim\\mathcal D} [log\\ \\sigma(\\beta log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]\\\\ \u0026=-\\mathbb E_{(x,y_w,y_l)\\sim\\mathcal D }[log\\ \\sigma(\\beta\\cdot(\\gamma_{\\pi_\\theta}(x, y_w, y_l) - \\gamma_{\\pi_{ref}}(x, y_w, y_l)))] \\end{align*} $$ 其中$\\gamma_\\pi(x, y_w, y_l)=log\\frac{\\pi(y_w|x)}{\\pi(y_l|x)}$，即正例和负例句子的对数似然差值，理论上，优化DPO损失的过程最终导致$\\gamma_{\\pi_\\theta}\u003e\\gamma_{\\pi_{ref}}$。因此$\\gamma_{\\pi_{ref}}$可以看作reference model的下界。基于该motivation，作者做了初步验证试验，即使用不同的模型作为reference model进行DPO训练，发现reference model越强，最后训练出来的$\\pi_\\theta$性能越好，从而在实验上验证这一点。基于此作者提出step-wise DPO的方法，即对一批偏好数据，不要一次性训练完，而是将数据做切分，每次用一小批数据训练迭代一版模型，下一次训练的时候用上一轮训练好的$\\pi_\\theta$作为新的reference model训练，这样可以不断提高策略模型的能力下届。 关于偏好数据的切分上，作者提出的思路是，按照偏好数据对的偏好强弱程度划分（即训练的难易程度，类比课程学习，由易到难的学习），因此作者思路是使用一个extra reward model(文章用的应该是最开始的reference model，就是sft后的model)对所有偏好数据对打分（这里感觉是计算似然，毕竟如果是sft后的model也不是reward model），计算每个子数据集的预测准确率（即chosen的分数高于rejected的比例），文章DPO数据集是涵盖很多不同source的，所以这里直接按照不同的source进行划分。最终就是预测准确率最高的（即最容易区分的）先训练，由易到难。\n我个人想法是如果不是想文章中使用多个source的偏好数据，而是只有一个source的话，划分数据的方式可以改为使用一个extra reward model对每条偏好数据对打分，按照(chosen_score - rejected_score)的值由高到低进行排序，然后再划分成不同子数据集，也是一种比较直观的想法吧。总之这篇工作给人感觉确实是简单有效。\nReferences [1] Rafailov et al. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model” NeurIPS 2023.\n[2] Meng et al. “SimPO: Simple Preference Optimization with a Reference-Free Reward” arXiv preprint arXiv:2405.14734 (2024).\n[3] Ethayarajh et al. “KTO: Model Alignment as Prospect Theoretic Optimization” arXiv preprint arXiv:2402.01306 (2024).\n[4] Lai et al. “STEP-DPO: STEP-WISE PREFERENCE OPTIMIZATION FOR LONG-CHAIN REASONING OF LLMS” arXiv preprint arXiv:2406.18629 (2024).\n[5] Hong et al. “ORPO: Monolithic Preference Optimization without Reference Model” arXiv preprint arXiv:2403.07691 (2024).\n[6] Park et al. “Disentangling Length from Quality in Direct Preference Optimization” arXiv preprint arXiv:2403.19159 (2024).\n[7] Xu et al. “Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation” arXiv preprint arXiv:2401.08417 (2024).\n[8] Kim et al. “sDPO: Don’t Use Your Data All at Once” arXiv preprint axXiv:2403.19270 (2024).\n","wordCount":"437","inLanguage":"en","datePublished":"2024-10-09T00:00:00Z","dateModified":"2024-10-09T00:00:00Z","author":{"@type":"Person","name":"Rs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tqzhong.github.io/my-blog/posts/llm-post-training/"},"publisher":{"@type":"Organization","name":"Rs' Log","logo":{"@type":"ImageObject","url":"https://tqzhong.github.io/my-blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},svg:{fontCache:"global"}}</script><nav class=nav><div class=logo-logo-switches><div class=logo><a href=https://tqzhong.github.io/my-blog/ accesskey=h title="Rs' Log (Alt + H)">Rs' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div></div><ul id=menu><li><a href=https://tqzhong.github.io/my-blog/ title=Posts><span>Posts</span></a></li><li><a href=https://tqzhong.github.io/my-blog/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://tqzhong.github.io/my-blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tqzhong.github.io/my-blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://tqzhong.github.io/my-blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://tqzhong.github.io/my-blog/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>大模型post-training方法</h1><div class=post-meta><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2024-10-09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;3 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span id=view-counter></span>
<script src=https://cdn.jsdelivr.net/npm/leancloud-storage@4.12.0/dist/av-min.js></script><script>AV.initialize("AokLJzaIvwVNJW0b2F0YTLLy-MdYXbMMI","fwZpRfBG259O3LscJFPW3ViH"),loadViewCount(location.pathname,"view-counter");var hasViewCounted=!1;function loadViewCount(e,t){if(hasViewCounted)return;hasViewCounted=!0;var s,n=document.getElementById(t),o=localStorage.getItem("view-count-"+e);o?n.innerText=o:n.innerText="0",s=new AV.Query("Counter"),s.equalTo("url",e),s.find().then(t=>{if(t.length>0){var s,i,a,o=t[0];o.increment("views",1),o.save(),i=o.get("views"),n.innerText=i,localStorage.setItem("view-count-"+e,i)}else a=AV.Object.extend("Counter"),s=new a,s.set("url",e),s.set("views",1),s.save().then(()=>{n.innerText="1",localStorage.setItem("view-count-"+e,1)})}).catch(function(e){console.error("error:",e)})}</script></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-dpo aria-label="1. DPO">1. DPO</a></li><li><a href=#2-simple-dpo aria-label="2. Simple-DPO">2. Simple-DPO</a></li><li><a href=#3-kto aria-label="3. KTO">3. KTO</a></li><li><a href=#4-step-dpo aria-label="4. Step-DPO">4. Step-DPO</a></li><li><a href=#5-orpo aria-label="5. ORPO">5. ORPO</a></li><li><a href=#6-r-dpo aria-label="6. R-DPO">6. R-DPO</a></li><li><a href=#7-cpo aria-label="7. CPO">7. CPO</a></li><li><a href=#8-sdpo aria-label="8. sDPO">8. sDPO</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h3 id=1-dpo>1. DPO<a hidden class=anchor aria-hidden=true href=#1-dpo>#</a></h3><p><a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html class=entityLink>Rafailov et al. (2023)</a>基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：</p><div class=scroll-container>$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$</div><h3 id=2-simple-dpo>2. Simple-DPO<a hidden class=anchor aria-hidden=true href=#2-simple-dpo>#</a></h3><p><a href=https://arxiv.org/abs/2405.14734 class=entityLink>Meng et al. (2024)</a>考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：</p><div class=scroll-container>$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$</div><p>因此Simple-DPO考虑将奖励函数表达式改为：</p><div class=scroll-container>$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{< i})
$$</div><p>此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma>0)$，表达式如下：</p><div class=scroll-container>$$
p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$</div>从而，Simple-DPO的优化函数：<div class=scroll-container>$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$</div><h3 id=3-kto>3. KTO<a hidden class=anchor aria-hidden=true href=#3-kto>#</a></h3><p>KTO loss (<a href=https://arxiv.org/abs/2402.01306 class=entityLink>Ethayarajh et al. (2024)</a>)与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。</p><p>前景理论（prospect theory），Tversky & Kahneman用下面方程建模人类价值：
$$
v(z,z_{ref};\alpha,\lambda) = \begin{cases}
(z-z_{ref})^\alpha & \text{if } z \ge z_{ref}
\\
-\lambda(z_{ref}-z)^\alpha & \text{if } z &lt; z_{ref}
\end{cases}
$$
价值函数$v:z\rightarrow R$将一个输出$z$想对一个参考值$z_{ref}$映射到其感知价值，反映人类相比起相同大小回报，对损失的敏感性更大，其中$\alpha$控制价值变化速度，$\lambda$反应对损失的敏感程度。</p><p>基于上述效用方程，作者做了一定修改使其更适合模型训练，损失函数如下：</p><div class=scroll-container>$$
\mathcal L_{KTO}(\pi_\theta,\pi_{ref})=\mathbb E_{x,y\sim\mathcal D}[w(y)(1-v_{KTO}(x, y;\beta))]
$$</div>其中<div class=scroll-container>$$
r_{KTO}(x,y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}\\\\
z_{ref}=\mathbb E_{x^\prime\sim\mathcal D}[\beta KL(\pi_\theta(y^\prime|x^\prime)||\pi_{ref}(y^\prime|x^\prime))]\\\\
v_{KTO}(x)=\begin{cases}
\sigma(r_{KTO}(x,y)-z_{ref})& if\ y\sim y_{desirable}|x\\
\sigma(z_{ref}-r_{KTO}(x, y))& if\ y\sim y_{undesirable}|x
\end{cases}\\\\
w(y)=\begin{cases}
\lambda_D& if\ y\sim y_{desirable}|x\\
\lambda_U& if\ y\sim y_{undesirable}|x
\end{cases}
$$</div><h3 id=4-step-dpo>4. Step-DPO<a hidden class=anchor aria-hidden=true href=#4-step-dpo>#</a></h3><p>Step-DPO (<a href=https://arxiv.org/abs/2406.18629 class=entityLink>Lai et al. (2024)</a>)主要解决大模型处理数学这类需要较长reasoning过程效果不佳的问题，作者指出用传统DPO训练数学类偏好数据存在一定缺陷，即数学类问题的reasoning过程往往是在中间的一些step开始出现错误，而前面的step推理是正确的，因此直接将整条数据归类为win或者lose都不合适。基于此，作者提出Step-DPO方法，即在initial steps之后对数据划分成win和lose，具体损失函数如下：</p><div class=scroll-container>$$
\mathcal L_{Step-DPO}(\theta)=-\mathbb E_{x,s_{1\sim k-1},s_{win},s_{lose}\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(s_{win}|x;s_{1\sim k-1})}{\pi_{ref}(s_{win}|x;s_{1\sim k-1})})-\beta log\frac{\pi_\theta(s_{lose}|x;s_{1\sim k-1})}{\pi_{ref}(s_{lose}|x;s_{1\sim k-1})}]
$$</div><p>此外本文提出了一种偏好数据构建流水线，来获取偏好数据${x,s_{1\sim k-1},s_{win},s_{lose}}$，具体流程如下：</p><ul><li><p>Step1: 收集数学问题$x$和相关答案$\hat y$的数据集$D_0={(x,\hat y)}$，用模型$\pi_{ref}$使用问题$x$进行推理（添加CoT），得到每条问题的推理过程以及最终答案，从所有数据中挑选最终答案出错的数据得到子数据集$D_1={(x,\hat y, y)}$，其中$y$为对应的错误答案的generations。</p></li><li><p>Step2: 针对数据集$D_1$，其中$y=s1,s2,\dots,s_n$，可以拆分成一系列推理步骤，通过人工或者GPT4逐条检测这些推理步骤直至发现推理出错的那一步$k$，将$s_k$作为$s_{lose}$，从而得到子数据集$D_2={(x,\hat y,s_{1\sim k-1},s_{lose})|x\in D_1}$。</p></li><li><p>Step3: 为了获取正确的推理步骤，通过$D_2$中的数据，使用$\pi_{ref}$模型基于问题$x$和前面$k-1$步正确推理作为prompt进行inference得到$y_{cont}$:
$$
y_{cont}\sim\pi_{ref}(y|x;s_{1\sim k-1})
$$
从得到正确答案的$y_{cont}$中，选择其中的第一个推理步骤作为$s_{win}$，得到最终的数据集$D_{final}={(x,s_{1\sim k-1},s_{lose},s_{win})|x\in D_2}$。</p></li></ul><p>需要注意的一点是，在Step3中，可能出现最终答案正确但是中间推理步骤错误的情况，因此这里需要人工或者GPT4对$s_{win}$做筛选。</p><h3 id=5-orpo>5. ORPO<a hidden class=anchor aria-hidden=true href=#5-orpo>#</a></h3><p><a href=https://arxiv.org/abs/2403.07691 class=entityLink>Hong et al. (2024)</a>考虑到SFT不能预防模型生成lose response，同时考虑使用偏好对齐如DPO等方法需要reference model开销较大且流程繁琐，因此本文提出了一种在SFT方法上增加一个能兼顾DPO这类偏好对齐的损失，具体做法：</p><p>对于一条训练数据${x, y}$，其对数似然概率：</p><div class=scroll-container>$$
logP_\theta(y|x)=\frac{1}{m}\sum_{t=1}^mlog\ P_\theta(y_t|x,y_{< t})
$$</div>定义Odds Ratio $odds_\theta(y|x)=\frac{P_\theta(y|x)}{1-P_\theta(y|x)}$表示模型生成$y$相比不生成$y$的概率倍数，基于此定义$OR_\theta(y_w, y_l)$：<div class=scroll-container>$$
OR_\theta(y_w,y_l)=\frac{odds_\theta(y_w|x)}{odds_\theta(y_l|x)}
$$</div>表示模型在给定$x$条件下相比于生成$y_l$，更可能生成$y_w$的程度。最终改良后的SFT损失函数：<div class=scroll-container>$$
\mathcal L_{ORPO}=\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[\mathcal L_{SFT}+\lambda\cdot\mathcal L_{OR}]\\\\
\mathcal L_{OR}=-log\ \sigma(log\frac{odds_\theta(y_w|x)}{odds_\theta(y_l|x)})
$$</div><h3 id=6-r-dpo>6. R-DPO<a hidden class=anchor aria-hidden=true href=#6-r-dpo>#</a></h3><p><a href=https://arxiv.org/abs/2403.19159 class=entityLink>Park et al. (2024)</a>考虑DPO在长度控制上的不足：容易生成过长啰嗦的文本，考虑在DPO的损失中引入对长度的约束，具体损失函数如下：</p><div class=scroll-container>$$
\mathcal L_{R-DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma (\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})+\alpha|y_w|-\alpha|y_l|]
$$</div><h3 id=7-cpo>7. CPO<a hidden class=anchor aria-hidden=true href=#7-cpo>#</a></h3><p><a href=https://arxiv.org/abs/2401.08417 class=entityLink>Xu et al. (2024)</a>简化DPO (<a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html class=entityLink>Rafailov et al. (2023)</a>)损失函数，将$\pi_{ref}$用均匀分布替代，并在偏好对齐中再次引入SFT损失约束，具体损失函数如下：</p><div class=scroll-container>$$
\mathcal L_{CPO}=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[ log\ \sigma(\beta log\ \pi_\theta(y_w|x)-\beta log\ \pi_\theta(y_l|x))+\lambda\ log\ \pi_\theta(y_w|x)]
$$</div><h3 id=8-sdpo>8. sDPO<a hidden class=anchor aria-hidden=true href=#8-sdpo>#</a></h3><p><a href=https://arxiv.org/pdf/2403.19270 class=entityLink>Kim et al. (2024)</a>提出了一个简单有效的DPO改进方案，作者发现DPO的损失优化存在理论下界</p><div class=scroll-container>$$
\begin{align*}
\mathcal L_{DPO}(\pi_\theta, \pi_{ref})
&=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D} [log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]\\
&=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D }[log\ \sigma(\beta\cdot(\gamma_{\pi_\theta}(x, y_w, y_l) - \gamma_{\pi_{ref}}(x, y_w, y_l)))]
\end{align*}
$$</div>其中$\gamma_\pi(x, y_w, y_l)=log\frac{\pi(y_w|x)}{\pi(y_l|x)}$，即正例和负例句子的对数似然差值，理论上，优化DPO损失的过程最终导致$\gamma_{\pi_\theta}>\gamma_{\pi_{ref}}$。因此$\gamma_{\pi_{ref}}$可以看作reference model的下界。基于该motivation，作者做了初步验证试验，即使用不同的模型作为reference model进行DPO训练，发现reference model越强，最后训练出来的$\pi_\theta$性能越好，从而在实验上验证这一点。基于此作者提出step-wise DPO的方法，即对一批偏好数据，不要一次性训练完，而是将数据做切分，每次用一小批数据训练迭代一版模型，下一次训练的时候用上一轮训练好的$\pi_\theta$作为新的reference model训练，这样可以不断提高策略模型的能力下届。<p>关于偏好数据的切分上，作者提出的思路是，按照偏好数据对的偏好强弱程度划分（即训练的难易程度，类比课程学习，由易到难的学习），因此作者思路是使用一个extra reward model(文章用的应该是最开始的reference model，就是sft后的model)对所有偏好数据对打分（这里感觉是计算似然，毕竟如果是sft后的model也不是reward model），计算每个子数据集的预测准确率（即chosen的分数高于rejected的比例），文章DPO数据集是涵盖很多不同source的，所以这里直接按照不同的source进行划分。最终就是预测准确率最高的（即最容易区分的）先训练，由易到难。</p><p>我个人想法是如果不是想文章中使用多个source的偏好数据，而是只有一个source的话，划分数据的方式可以改为使用一个extra reward model对每条偏好数据对打分，按照(chosen_score - rejected_score)的值由高到低进行排序，然后再划分成不同子数据集，也是一种比较直观的想法吧。总之这篇工作给人感觉确实是简单有效。</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Rafailov et al. <a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html class=entityLink>“Direct Preference Optimization: Your Language Model is Secretly a Reward Model”</a> NeurIPS 2023.</p><p>[2] Meng et al. <a href=https://arxiv.org/abs/2405.14734 class=entityLink>“SimPO: Simple Preference Optimization with a Reference-Free Reward”</a> arXiv preprint arXiv:2405.14734 (2024).</p><p>[3] Ethayarajh et al. <a href=https://arxiv.org/abs/2402.01306 class=entityLink>“KTO: Model Alignment as Prospect Theoretic Optimization”</a> arXiv preprint arXiv:2402.01306 (2024).</p><p>[4] Lai et al. <a href=https://arxiv.org/abs/2406.18629 class=entityLink>“STEP-DPO: STEP-WISE PREFERENCE OPTIMIZATION FOR LONG-CHAIN REASONING OF LLMS”</a> arXiv preprint arXiv:2406.18629 (2024).</p><p>[5] Hong et al. <a href=https://arxiv.org/abs/2403.07691 class=entityLink>“ORPO: Monolithic Preference Optimization without Reference Model”</a> arXiv preprint arXiv:2403.07691 (2024).</p><p>[6] Park et al. <a href=https://arxiv.org/abs/2403.19159 class=entityLink>“Disentangling Length from Quality in Direct Preference Optimization”</a> arXiv preprint arXiv:2403.19159 (2024).</p><p>[7] Xu et al. <a href=https://arxiv.org/abs/2401.08417 class=entityLink>“Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation”</a> arXiv preprint arXiv:2401.08417 (2024).</p><p>[8] Kim et al. <a href=https://arxiv.org/pdf/2403.19270 class=entityLink>“sDPO: Don’t Use Your Data All at Once”</a> arXiv preprint axXiv:2403.19270 (2024).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tqzhong.github.io/my-blog/tags/ai/>AI</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/llm/>LLM</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/nlp/>NLP</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/post-training/>Post-Training</a></li></ul><nav class=paginav><a class=prev href=https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/><span class=title>« Prev</span><br><span>Deepspeed多机多卡训练&代码细节</span></a></nav></footer></article><div class=social-icons><a href=https://github.com/tqzhong target=_blank rel="noopener noreferrer"><i class="fab fa-github"></i>
</a><a href=https://x.com/rs1047g target=_blank rel="noopener noreferrer"><i class="fab fa-twitter"></i>
</a><a href="https://scholar.google.com/citations?hl=en&amp;user=UNNLJX4AAAAJ" target=_blank rel="noopener noreferrer"><i class="fab fa-google"></i>
</a><a href=https://tqzhong.github.io target=_blank rel="noopener noreferrer"><i class="fas fa-user"></i></a></div><script src=https://utteranc.es/client.js repo=tqzhong/my-blog issue-term=pathname label=hugo-comment theme=github-dark crossorigin=anonymous async></script></main><footer class=footer><span>&copy; 2025 <a href=https://tqzhong.github.io/my-blog/>Rs' Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}]})})</script></body></html>