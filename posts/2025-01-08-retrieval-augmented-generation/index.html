<!doctype html><html lang=en dir=auto><a href=https://yourpersonalwebsite.com target=_blank><head><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RAG路线 | Rs' Log</title>
<meta name=keywords content="AI,RAG,NLP"><meta name=description content="Retrieval-Augmented Generation for Large Language Models: A Survey
1. Overview of RAG
典型的RAG模型如图1所示


图1: 经典RAG模型
1.1 Naive RAG
Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。

索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。
检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。
生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。

1.2 Advanced RAG
Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。

pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。

优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。
优化初始query：常用的策略有query transformation，query expansion等。


post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。

chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。
chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。



1.3 Modular RAG
模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。

引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等
引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。



图2: 三类不同RAG模型流程示意图
2. Retrieval Part
2.1 检索资源
从检索内容的数据上来看包含以下几种：

无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等
半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。
结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。
LLMs生成内容

从检索的粒度来看，包含以下几种：

对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章
对于知识图谱，检索粒度包含：实体，三元组，子图

2.2 索引的优化
在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。"><meta name=author content="Rs"><link rel=canonical href=https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css><link crossorigin=anonymous href=/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon type=image/x-icon sizes=48x48 href=images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=images/favicon-32x32.png><link rel=apple-touch-icon href=images/apple-touch-icon.png><link rel=icon sizes=512x512 href=images/android-chrome-512x512.png type=image/png><link rel=icon sizes=192x192 href=images/android-chrome-192x192.png type=image/png><link rel=mask-icon href=https://tqzhong.github.io/my-blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href="/my-blog/css/custom.css?v=1.7"><meta property="og:title" content="RAG路线"><meta property="og:description" content="Retrieval-Augmented Generation for Large Language Models: A Survey
1. Overview of RAG
典型的RAG模型如图1所示


图1: 经典RAG模型
1.1 Naive RAG
Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。

索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。
检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。
生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。

1.2 Advanced RAG
Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。

pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。

优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。
优化初始query：常用的策略有query transformation，query expansion等。


post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。

chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。
chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。



1.3 Modular RAG
模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。

引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等
引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。



图2: 三类不同RAG模型流程示意图
2. Retrieval Part
2.1 检索资源
从检索内容的数据上来看包含以下几种：

无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等
半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。
结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。
LLMs生成内容

从检索的粒度来看，包含以下几种：

对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章
对于知识图谱，检索粒度包含：实体，三元组，子图

2.2 索引的优化
在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。"><meta property="og:type" content="article"><meta property="og:url" content="https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RAG路线"><meta name=twitter:description content="Retrieval-Augmented Generation for Large Language Models: A Survey
1. Overview of RAG
典型的RAG模型如图1所示


图1: 经典RAG模型
1.1 Naive RAG
Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。

索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。
检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。
生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。

1.2 Advanced RAG
Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。

pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。

优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。
优化初始query：常用的策略有query transformation，query expansion等。


post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。

chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。
chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。



1.3 Modular RAG
模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。

引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等
引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。



图2: 三类不同RAG模型流程示意图
2. Retrieval Part
2.1 检索资源
从检索内容的数据上来看包含以下几种：

无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等
半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。
结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。
LLMs生成内容

从检索的粒度来看，包含以下几种：

对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章
对于知识图谱，检索粒度包含：实体，三元组，子图

2.2 索引的优化
在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tqzhong.github.io/my-blog/posts/"},{"@type":"ListItem","position":2,"name":"RAG路线","item":"https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RAG路线","name":"RAG路线","description":"Retrieval-Augmented Generation for Large Language Models: A Survey 1. Overview of RAG 典型的RAG模型如图1所示 图1: 经典RAG模型 1.1 Naive RAG Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。\n索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。 检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。 生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。 1.2 Advanced RAG Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。\npre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。 优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。 优化初始query：常用的策略有query transformation，query expansion等。 post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。 chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。 chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。 1.3 Modular RAG 模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。\n引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等 引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。 图2: 三类不同RAG模型流程示意图 2. Retrieval Part 2.1 检索资源 从检索内容的数据上来看包含以下几种：\n无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等 半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。 结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。 LLMs生成内容 从检索的粒度来看，包含以下几种：\n对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章 对于知识图谱，检索粒度包含：实体，三元组，子图 2.2 索引的优化 在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。\n","keywords":["AI","RAG","NLP"],"articleBody":"Retrieval-Augmented Generation for Large Language Models: A Survey 1. Overview of RAG 典型的RAG模型如图1所示 图1: 经典RAG模型 1.1 Naive RAG Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。\n索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。 检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。 生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。 1.2 Advanced RAG Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。\npre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。 优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。 优化初始query：常用的策略有query transformation，query expansion等。 post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。 chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。 chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。 1.3 Modular RAG 模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。\n引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等 引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。 图2: 三类不同RAG模型流程示意图 2. Retrieval Part 2.1 检索资源 从检索内容的数据上来看包含以下几种：\n无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等 半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。 结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。 LLMs生成内容 从检索的粒度来看，包含以下几种：\n对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章 对于知识图谱，检索粒度包含：实体，三元组，子图 2.2 索引的优化 在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。\nChunking Strategy：最常用的方法是将文档切分成固定token数的chunk（100，256，512）。越大的chunk能够捕获更多的内容，但也会带来更多噪音，处理更长时间，成本更高；越小的chunk相反。划分chunk存在破坏完整句子的问题，解决该问题的工作有Small2Big等。 Metadata Attachments：chunk可以由元数据（如：page number，file name，author，category timestamp等）扩充，从而检索过程可以使用元数据进行过滤，缩小检索范围。 Structural Index： 分层索引结构 知识图谱索引 2.3 Query的优化 Query扩充： Multi-Query：通过LLM将query扩充成多个，然后并行处理这些queries Sub-Query：对于复杂问题，可以将问题拆解成系列子问题 Chain-of-Verification（CoVe） Query Transformation： Query Rewrite：有些原始queries对于LLM检索来说并不是最优。因此prompt LLM来重写queries，工作如Rewrite-retrieve-read等 Query Routing 2.4 词向量模型（retriever） 从向量编码器角度分包含sparse encoder和dense encoder\nsparse encoder： TF-IDF BM25 dense retriever： BERT-based PLM 3. Generation Part 在完成检索部分后，把所有检索到的信息直接输入LLM来获取答案往往并不是最合理的方案。在生成阶段，一般会从两个方面引入一些调整：调整检索的内容、调整LLM。\n3.1 Context Curation 冗余信息会影响LLM最终的生成结果，通常，LLM会把注意力倾向长文本的开端和结尾，而容易忘记中间的部分。因此在RAG系统中，我们通常需要进一步处理检索到的信息。\nReranking：重排chunks的顺序 rule-based methods：Diversity，Relevance，MRR model-based methods：BERT series（SpanBERT），Cohere rerank，bge-reranker-large Context Selection/Compression：对检索内容的筛选和压缩 Reducing the number of documents 3.2 LLM Fine-tuning 对生成式LLM进行微调，主要适用特定场景下的生成，一般的PLM可能对这些场景了解程度不够，因此需要微调来辅助LLM生成。\n4. Augmentation Process in RAG 常规的RAG流程通过只包含一次检索步骤，然后接着一步生成步骤，对于复杂任务或多步推理场景这种方式局限性较大，因此有优化的检索过程来解决这些问题。\n4.1 迭代检索（Iterative Retrieval） 迭代检索过程中，知识库会基于初始query以及当前生成的文本被重复搜索，为LLM生成提供更全面的信息。相关工作：ITER-RETGEN等。\n4.2 递归检索（Recursive Retrieval） 递归检索通常用于信息检索来提升检索结果的深度和相关性。该过程会基于过往检索的结果迭代优化检索queries。相关工作：IRCoT，ToC等。\n4.3 适应性检索（Adaptive Retrieval） 适应性检索通过让LLMs能够主动决策最优检索的时刻以及检索的内容来优化RAG系统，提升检索信息的相关度以及效率。相关工作：Flare，Self-RAG，AutoGPT，Toolformer，Graph-Toolformer，WebGPT等。 图3: RAG中三类不同增强过程示意图 基于query的RAG方法（query-based） 1. REALM: Retrieval-Augmented Language Model Pre-Training Guu et al. (2020) 提出REALM，一种经典的query-based的RAG方法，文章使用BERT模型作为检索器：\n$$ p(z\\vert x)=\\frac{\\exp f(x,z)}{\\sum_{z^\\prime}\\exp f(x,z^\\prime)},\\\\ f(x,z)=\\text{Embed}_{\\text{input}}(x)^\\top\\text{Embed}_{\\text{doc}}(z) $$ 将检索的文本$z$与query $x$拼接用于answer $y$的生成。\n2. REPLUG: Retrieval-Augmented Black-Box Language Models Shi et al. (2024) 提出一种针对黑盒模型的query-based RAG方法\n无训练Method 基于输入query $x$，选定现有检索器，文档库$\\mathcal D={d_1,\\cdots,d_m}$，检索器为编码器结构，被用来同时对query和文档进行编码。$\\text{E}(d)$为编码器最后一层隐藏表征在所有token上的表征均值。计算query表征与所有文档表征的余弦相似度：\n$$ s(d,x)=cos(\\text E(d), \\text E(x)) $$ 选择其中相似度分数最高的$k$个文档构成集合$\\mathcal D^\\prime\\sub\\mathcal D$。这里为了高效检索，提前计算每个文档的向量表征并构建FAISS索引。\n根据前面计算的相似度分数计算每个相关文档的权重：\n$$ \\lambda(d,x)=\\frac{e^{s(d,x)}}{\\sum_{d\\in\\mathcal D^\\prime}e^{s(d,x)}} $$ 为了同时利用所有相关文档，切不超出模型最大输入长度，作者使用加权解码，用上述$\\lambda(d,x)$作为权重：\n$$ p(y\\vert x,\\mathcal D^\\prime)=\\sum_{d\\in\\mathcal D^\\prime}p(y\\vert d\\ \\circ\\ x)\\cdot\\lambda(d,x) $$ 其中$d\\ \\circ\\ x$表示文档$d$和query $x$的拼接。\n带训练Method 作者同时提出一种训练方法主要用于对齐检索器与生成器，训练过程中只更新检索器参数（针对黑盒模型）。首先用初始检索器检索$k$个最相关文档，与之前类似的，计算每个文档的权重分数：\n$$ P_R(d\\vert x)=\\frac{e^{s(d,x)/\\gamma}}{\\sum_{d\\in\\mathcal D^\\prime}e^{s(d,x)/\\gamma}} $$ 其中$\\gamma$为超参数控制softmax的温度，计算得到的$P_R(d\\vert x)$分布代表了检索器的检索分布。紧接着，给定ground truth $y$，对于每个相关文档，计算生成器在ground truth部分的LM perplexity $P_{LM}(y\\vert d,x)$，并得到生成器的分布：\n$$ Q(d\\vert x,y)=\\frac{e^{P_{LM}(y\\vert d,x)/\\beta}}{\\sum_{d\\in\\mathcal D^\\prime}e^{P_{LM}(y\\vert d,x)/\\beta}} $$ 其中$\\beta$也是调节softmax温度的超参数。最终根据上面检索器的分布$P_R(d\\vert x)$和生成器的分布$Q(d\\vert x,y)$计算两者的KL-divergence并作为损失函数优化检索器参数：\n$$ \\mathcal{L} = \\frac{1}{|\\mathcal{B}|} \\sum_{x \\in \\mathcal{B}} KL \\left( P_R(d \\mid x)\\ \\Vert\\ Q_{LM}(d \\mid x, y) \\right) $$ 其中$\\mathcal B$是query集合，每个query $x$均有一个ground truth $y$。注意到由于检索器参数更新，使得预先存好的所有文档向量表征会有所变化，为了高效训练，作者采用的方案是每训练$T$个steps后重新计算所有文档的向量表征。\n3. In-Context RALM: In-Context Retrieval-Augmented Language Models Ram et al. (2023) 提出一种基于in-context的RAG方法，该方法主要使用了Retrieval Stride和Retrieval Query Length两个trick。\nIn-Context RALM 不同于普通的query-based RAG方法只基于query检索一次文档库，In-Context RALM会在生成过程中不断基于当前的生成结果去多次检索文档库，定义目前生成的文本（包括query）为$x_{\\lt i}$，使用$x_{\\lt i}$检索得到的文档内容为$\\mathcal R_\\mathcal C(x_{\\lt i})$，那么In-Context RALM的生成过程可以通过如下公式定义：\n$$ p(x_i,\\dots,x_n)=\\Pi_{i=1}^n p_\\theta(x_i\\vert \\mathcal R_\\mathcal C(x_{\\lt i});x_{\\lt i}) $$ Retrieval Stride 由于频繁检索文档库会带来比较高的资源消耗，且降低生成速度，因此作者提出Retrieval Stride的概念，即每生成$s(s\u003e1)$个token后进行一个检索，这样RALM的生成过程为：\n$$ p(x_1, \\ldots, x_n) = \\prod_{j=0}^{n_s-1} \\prod_{i=1}^s p_\\theta \\left( x_{s \\cdot j + i} \\mid \\left[ \\mathcal{R}_\\mathcal C(x_{\\leq s \\cdot j}); x_{\\lt s \\cdot j + i} \\right] \\right) $$ 其中$n_s=n/s$为检索的次数（Retrieval Strides）。实验结果表明使用较小的$s$（尽可能多的增加检索次数）会比使用较大的$s$效果好，但是会增加时间成本。\nRetrieval Query Length 作者指出尽管检索query原则上取决于所有的prefix tokens $x_{\\le s\\cdot j}$，但是与生成token最相关的信息往往都聚集在prefix tokens的末尾，如果检索query太长那么这些信息会被稀释。对此作者提出Retrieval Query Length的概念，即控制query长度不超过$\\ell$，当query长度超过$\\ell$时截取整个query的最后$\\ell$个token，即$q_j^{s,\\ell}:=x_{s\\cdot j-\\ell+1},\\cdots,x_{s\\cdot j}$，应用上述trick后的生成过程定义如下：\n$$ p(x_1, \\dots, x_n) = \\prod_{j=0}^{n_s-1} \\prod_{i=1}^s p_\\theta \\left( x_{s \\cdot j + i} \\middle| \\left[ \\mathcal{R}_\\mathcal{C} \\left( q_j^{s,\\ell} \\right); x_{\u003c s \\cdot j + i} \\right] \\right) $$ 4. SELF-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection Asai et al. (2024) 提出了一种基于反馈的RAG框架，主要通过引入Critic模型和Generator模型，Critic模型会在Generator模型生成前判断是否需要进行检索，如果不需要则直接让Generator生成下一个sequence（文章以一个完整的sequence为单位作为检索间隔），否则使用检索器检索相关文档，之后会让Critic判别每个文档的相关性与是否支持回答该问题等信息。作者在模型词表引入一些reflection tokens作为Critic模型的判别输出结果，通过基于prompt GPT4的方式获取有效监督数据并训练Critic模型以及Generator模型（蒸馏GPT4模型知识）。SELF-RAG整体流程框架如图4所示。\n图4: SELF-RAG整体框架 四类reflection tokens 图5: SELF-RAG中使用的四类reflection tokens SELF-RAG Inference算法 图6: SELF-RAG Inference算法流程 SELF-RAG Training算法（Critic $\\mathcal C$ 和Generator $\\mathcal M$） 图7: SELF-RAG Training算法流程 其中Eq1: $$ \\max_\\mathcal C\\mathbb E_{((x,y),r)\\sim\\mathcal D_{critic}}\\log p_\\mathcal C(r\\vert x,y),\\ r\\ \\text{for reflection tokens} $$\nEq2:\n$$ \\max_\\mathcal M\\mathbb E_{(x,y,r)\\sim\\mathcal D_{gen}}\\log p_\\mathcal M(y,r\\vert x) $$\n基于表征的RAG方法（Representation-based） 1. FID: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering Izacard et al. (2021) 提出一种基于representation的RAG方法，作者使用encoder-decoder模型（BART），对于检索器，使用BM25和DPR两种方法检索相关文档，对每个文档都分别使用encoder编码成隐空间表征，并将所有的表征拼接在一起输入decoder解码出answer，作者命名这类结构为Fusion-in-Decoder，结构示意图如图8所示。\n作者在处理数据时，在问题（question），文档标题（title），文档内容（context）之前都添加了特殊tokens：\"$\\text{question:}$\"，$\\text{title:}$，$\\text{context:}$。\n图8: Fusion-in-Decoder结构图 References [1] Gao et al. “Retrieval-Augmented Generation for Large Language Models: A Survey” arXiv preprint arXiv:2312.10997 (2023)\n[2] Guu et al. “Retrieval Augmented Language Model Pre-Training” ICML 2020.\n[3] Shi et al. “REPLUG: Retrieval-Augmented Black-Box Language Models” NAACL 2024.\n[4] Ram et al. “In-Context Retrieval-Augmented Language Models ” TACL 2023.\n[5] Asai et al. “Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection” ICLR 2024.\n[6] Izacard et al. “Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering” EACL 2021.\n","wordCount":"544","inLanguage":"en","datePublished":"2025-01-08T00:00:00Z","dateModified":"2025-01-08T00:00:00Z","author":{"@type":"Person","name":"Rs"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/"},"publisher":{"@type":"Organization","name":"Rs' Log","logo":{"@type":"ImageObject","url":"https://tqzhong.github.io/my-blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},svg:{fontCache:"global"}}</script><nav class=nav><div class=logo-logo-switches><div class=logo><a href=https://tqzhong.github.io/my-blog/ accesskey=h title="Rs' Log (Alt + H)">Rs' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div></div><ul id=menu><li><a href=https://tqzhong.github.io/my-blog/ title=Posts><span>Posts</span></a></li><li><a href=https://tqzhong.github.io/my-blog/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://tqzhong.github.io/my-blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tqzhong.github.io/my-blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://tqzhong.github.io/my-blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://tqzhong.github.io/my-blog/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>RAG路线</h1><div class=post-meta><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-01-08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;3 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span id=view-counter></span>
<script src=https://cdn.jsdelivr.net/npm/leancloud-storage@4.12.0/dist/av-min.js></script><script>AV.initialize("AokLJzaIvwVNJW0b2F0YTLLy-MdYXbMMI","fwZpRfBG259O3LscJFPW3ViH"),loadViewCount(location.pathname,"view-counter");var hasViewCounted=!1;function loadViewCount(e,t){if(hasViewCounted)return;hasViewCounted=!0;var s,n=document.getElementById(t),o=localStorage.getItem("view-count-"+e);o?n.innerText=o:n.innerText="0",s=new AV.Query("Counter"),s.equalTo("url",e),s.find().then(t=>{if(t.length>0){var s,i,a,o=t[0];o.increment("views",1),o.save(),i=o.get("views"),n.innerText=i,localStorage.setItem("view-count-"+e,i)}else a=AV.Object.extend("Counter"),s=new a,s.set("url",e),s.set("views",1),s.save().then(()=>{n.innerText="1",localStorage.setItem("view-count-"+e,1)})}).catch(function(e){console.error("error:",e)})}</script></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#retrieval-augmented-generation-for-large-language-models-a-survey aria-label="Retrieval-Augmented Generation for Large Language Models: A Survey">Retrieval-Augmented Generation for Large Language Models: A Survey</a><ul><li><a href=#1-overview-of-rag aria-label="1. Overview of RAG">1. Overview of RAG</a><ul><li><a href=#11-naive-rag aria-label="1.1 Naive RAG">1.1 Naive RAG</a></li><li><a href=#12-advanced-rag aria-label="1.2 Advanced RAG">1.2 Advanced RAG</a></li><li><a href=#13-modular-rag aria-label="1.3 Modular RAG">1.3 Modular RAG</a></li></ul></li><li><a href=#2-retrieval-part aria-label="2. Retrieval Part">2. Retrieval Part</a><ul><li><a href=#21-%e6%a3%80%e7%b4%a2%e8%b5%84%e6%ba%90 aria-label="2.1 检索资源">2.1 检索资源</a></li><li><a href=#22-%e7%b4%a2%e5%bc%95%e7%9a%84%e4%bc%98%e5%8c%96 aria-label="2.2 索引的优化">2.2 索引的优化</a></li><li><a href=#23-query%e7%9a%84%e4%bc%98%e5%8c%96 aria-label="2.3 Query的优化">2.3 Query的优化</a></li><li><a href=#24-%e8%af%8d%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8bretriever aria-label="2.4 词向量模型（retriever）">2.4 词向量模型（retriever）</a></li></ul></li><li><a href=#3-generation-part aria-label="3. Generation Part">3. Generation Part</a><ul><li><a href=#31-context-curation aria-label="3.1 Context Curation">3.1 Context Curation</a></li><li><a href=#32-llm-fine-tuning aria-label="3.2 LLM Fine-tuning">3.2 LLM Fine-tuning</a></li></ul></li><li><a href=#4-augmentation-process-in-rag aria-label="4. Augmentation Process in RAG">4. Augmentation Process in RAG</a><ul><li><a href=#41-%e8%bf%ad%e4%bb%a3%e6%a3%80%e7%b4%a2iterative-retrieval aria-label="4.1 迭代检索（Iterative Retrieval）">4.1 迭代检索（Iterative Retrieval）</a></li><li><a href=#42-%e9%80%92%e5%bd%92%e6%a3%80%e7%b4%a2recursive-retrieval aria-label="4.2 递归检索（Recursive Retrieval）">4.2 递归检索（Recursive Retrieval）</a></li><li><a href=#43-%e9%80%82%e5%ba%94%e6%80%a7%e6%a3%80%e7%b4%a2adaptive-retrieval aria-label="4.3 适应性检索（Adaptive Retrieval）">4.3 适应性检索（Adaptive Retrieval）</a></li></ul></li></ul></li><li><a href=#%e5%9f%ba%e4%ba%8equery%e7%9a%84rag%e6%96%b9%e6%b3%95query-based aria-label=基于query的RAG方法（query-based）>基于query的RAG方法（query-based）</a><ul><li><a href=#1-realm-retrieval-augmented-language-model-pre-training aria-label="1. REALM: Retrieval-Augmented Language Model Pre-Training">1. REALM: Retrieval-Augmented Language Model Pre-Training</a></li><li><a href=#2-replug-retrieval-augmented-black-box-language-models aria-label="2. REPLUG: Retrieval-Augmented Black-Box Language Models">2. REPLUG: Retrieval-Augmented Black-Box Language Models</a></li><li><a href=#3-in-context-ralm-in-context-retrieval-augmented-language-models aria-label="3. In-Context RALM: In-Context Retrieval-Augmented Language Models">3. In-Context RALM: In-Context Retrieval-Augmented Language Models</a></li><li><a href=#4-self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection aria-label="4. SELF-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection">4. SELF-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection</a></li></ul></li><li><a href=#%e5%9f%ba%e4%ba%8e%e8%a1%a8%e5%be%81%e7%9a%84rag%e6%96%b9%e6%b3%95representation-based aria-label=基于表征的RAG方法（Representation-based）>基于表征的RAG方法（Representation-based）</a><ul><li><a href=#1-fid-leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering aria-label="1. FID: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering">1. FID: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h3 id=retrieval-augmented-generation-for-large-language-models-a-survey>Retrieval-Augmented Generation for Large Language Models: A Survey<a hidden class=anchor aria-hidden=true href=#retrieval-augmented-generation-for-large-language-models-a-survey>#</a></h3><h4 id=1-overview-of-rag>1. Overview of RAG<a hidden class=anchor aria-hidden=true href=#1-overview-of-rag>#</a></h4><p>典型的RAG模型如图1所示
<img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image1.png alt="typical rag model"></p><div align=center style=color:#999>图1: 经典RAG模型</div><h5 id=11-naive-rag>1.1 Naive RAG<a hidden class=anchor aria-hidden=true href=#11-naive-rag>#</a></h5><p>Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。</p><ul><li>索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。</li><li>检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。</li><li>生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。</li></ul><h5 id=12-advanced-rag>1.2 Advanced RAG<a hidden class=anchor aria-hidden=true href=#12-advanced-rag>#</a></h5><p>Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。</p><ul><li>pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。<ul><li>优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。</li><li>优化初始query：常用的策略有query transformation，query expansion等。</li></ul></li><li>post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。<ul><li>chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。</li><li>chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。</li></ul></li></ul><h5 id=13-modular-rag>1.3 Modular RAG<a hidden class=anchor aria-hidden=true href=#13-modular-rag>#</a></h5><p>模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。</p><ul><li>引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等</li><li>引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。</li></ul><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image2.png alt="three rag types"></p><div align=center style=color:#999>图2: 三类不同RAG模型流程示意图</div><h4 id=2-retrieval-part>2. Retrieval Part<a hidden class=anchor aria-hidden=true href=#2-retrieval-part>#</a></h4><h5 id=21-检索资源>2.1 检索资源<a hidden class=anchor aria-hidden=true href=#21-检索资源>#</a></h5><p>从检索内容的数据上来看包含以下几种：</p><ul><li>无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等</li><li>半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。</li><li>结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。</li><li>LLMs生成内容</li></ul><p>从检索的粒度来看，包含以下几种：</p><ul><li>对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章</li><li>对于知识图谱，检索粒度包含：实体，三元组，子图</li></ul><h5 id=22-索引的优化>2.2 索引的优化<a hidden class=anchor aria-hidden=true href=#22-索引的优化>#</a></h5><p>在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。</p><ul><li>Chunking Strategy：最常用的方法是将文档切分成固定token数的chunk（100，256，512）。越大的chunk能够捕获更多的内容，但也会带来更多噪音，处理更长时间，成本更高；越小的chunk相反。划分chunk存在破坏完整句子的问题，解决该问题的工作有Small2Big等。</li><li>Metadata Attachments：chunk可以由元数据（如：page number，file name，author，category timestamp等）扩充，从而检索过程可以使用元数据进行过滤，缩小检索范围。</li><li>Structural Index：<ul><li>分层索引结构</li><li>知识图谱索引</li></ul></li></ul><h5 id=23-query的优化>2.3 Query的优化<a hidden class=anchor aria-hidden=true href=#23-query的优化>#</a></h5><ul><li>Query扩充：<ul><li>Multi-Query：通过LLM将query扩充成多个，然后并行处理这些queries</li><li>Sub-Query：对于复杂问题，可以将问题拆解成系列子问题</li><li>Chain-of-Verification（CoVe）</li></ul></li><li>Query Transformation：<ul><li>Query Rewrite：有些原始queries对于LLM检索来说并不是最优。因此prompt LLM来重写queries，工作如Rewrite-retrieve-read等</li></ul></li><li>Query Routing</li></ul><h5 id=24-词向量模型retriever>2.4 词向量模型（retriever）<a hidden class=anchor aria-hidden=true href=#24-词向量模型retriever>#</a></h5><p>从向量编码器角度分包含sparse encoder和dense encoder</p><ul><li>sparse encoder：<ul><li>TF-IDF</li><li>BM25</li></ul></li><li>dense retriever：<ul><li>BERT-based PLM</li></ul></li></ul><h4 id=3-generation-part>3. Generation Part<a hidden class=anchor aria-hidden=true href=#3-generation-part>#</a></h4><p>在完成检索部分后，把所有检索到的信息直接输入LLM来获取答案往往并不是最合理的方案。在生成阶段，一般会从两个方面引入一些调整：调整检索的内容、调整LLM。</p><h5 id=31-context-curation>3.1 Context Curation<a hidden class=anchor aria-hidden=true href=#31-context-curation>#</a></h5><p>冗余信息会影响LLM最终的生成结果，通常，LLM会把注意力倾向长文本的开端和结尾，而容易忘记中间的部分。因此在RAG系统中，我们通常需要进一步处理检索到的信息。</p><ul><li>Reranking：重排chunks的顺序<ul><li>rule-based methods：Diversity，Relevance，MRR</li><li>model-based methods：BERT series（SpanBERT），Cohere rerank，bge-reranker-large</li></ul></li><li>Context Selection/Compression：对检索内容的筛选和压缩</li><li>Reducing the number of documents</li></ul><h5 id=32-llm-fine-tuning>3.2 LLM Fine-tuning<a hidden class=anchor aria-hidden=true href=#32-llm-fine-tuning>#</a></h5><p>对生成式LLM进行微调，主要适用特定场景下的生成，一般的PLM可能对这些场景了解程度不够，因此需要微调来辅助LLM生成。</p><h4 id=4-augmentation-process-in-rag>4. Augmentation Process in RAG<a hidden class=anchor aria-hidden=true href=#4-augmentation-process-in-rag>#</a></h4><p>常规的RAG流程通过只包含一次检索步骤，然后接着一步生成步骤，对于复杂任务或多步推理场景这种方式局限性较大，因此有优化的检索过程来解决这些问题。</p><h5 id=41-迭代检索iterative-retrieval>4.1 迭代检索（Iterative Retrieval）<a hidden class=anchor aria-hidden=true href=#41-迭代检索iterative-retrieval>#</a></h5><p>迭代检索过程中，知识库会基于初始query以及当前生成的文本被重复搜索，为LLM生成提供更全面的信息。相关工作：ITER-RETGEN等。</p><h5 id=42-递归检索recursive-retrieval>4.2 递归检索（Recursive Retrieval）<a hidden class=anchor aria-hidden=true href=#42-递归检索recursive-retrieval>#</a></h5><p>递归检索通常用于信息检索来提升检索结果的深度和相关性。该过程会基于过往检索的结果迭代优化检索queries。相关工作：IRCoT，ToC等。</p><h5 id=43-适应性检索adaptive-retrieval>4.3 适应性检索（Adaptive Retrieval）<a hidden class=anchor aria-hidden=true href=#43-适应性检索adaptive-retrieval>#</a></h5><p>适应性检索通过让LLMs能够主动决策最优检索的时刻以及检索的内容来优化RAG系统，提升检索信息的相关度以及效率。相关工作：Flare，Self-RAG，AutoGPT，Toolformer，Graph-Toolformer，WebGPT等。
<img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image3.png alt="three rag types"></p><div align=center style=color:#999>图3: RAG中三类不同增强过程示意图</div><h3 id=基于query的rag方法query-based>基于query的RAG方法（query-based）<a hidden class=anchor aria-hidden=true href=#基于query的rag方法query-based>#</a></h3><h4 id=1-realm-retrieval-augmented-language-model-pre-training>1. REALM: Retrieval-Augmented Language Model Pre-Training<a hidden class=anchor aria-hidden=true href=#1-realm-retrieval-augmented-language-model-pre-training>#</a></h4><p><a href="https://proceedings.mlr.press/v119/guu20a.html?ref=https://githubhelp.com" class=entityLink>Guu et al. (2020)</a> 提出REALM，一种经典的query-based的RAG方法，文章使用BERT模型作为检索器：</p><div class=scroll-container>$$
p(z\vert x)=\frac{\exp f(x,z)}{\sum_{z^\prime}\exp f(x,z^\prime)},\\
f(x,z)=\text{Embed}_{\text{input}}(x)^\top\text{Embed}_{\text{doc}}(z)
$$</div><p>将检索的文本$z$与query $x$拼接用于answer $y$的生成。</p><h4 id=2-replug-retrieval-augmented-black-box-language-models>2. REPLUG: Retrieval-Augmented Black-Box Language Models<a hidden class=anchor aria-hidden=true href=#2-replug-retrieval-augmented-black-box-language-models>#</a></h4><p><a href=https://aclanthology.org/2024.naacl-long.463/ class=entityLink>Shi et al. (2024)</a> 提出一种针对黑盒模型的query-based RAG方法</p><ul><li>无训练Method</li></ul><p>基于输入query $x$，选定现有检索器，文档库$\mathcal D={d_1,\cdots,d_m}$，检索器为编码器结构，被用来同时对query和文档进行编码。$\text{E}(d)$为编码器最后一层隐藏表征在所有token上的表征均值。计算query表征与所有文档表征的余弦相似度：</p><div class=scroll-container>$$
s(d,x)=cos(\text E(d), \text E(x))
$$</div><p>选择其中相似度分数最高的$k$个文档构成集合$\mathcal D^\prime\sub\mathcal D$。这里为了高效检索，提前计算每个文档的向量表征并构建FAISS索引。</p><p>根据前面计算的相似度分数计算每个相关文档的权重：</p><div class=scroll-container>$$
\lambda(d,x)=\frac{e^{s(d,x)}}{\sum_{d\in\mathcal D^\prime}e^{s(d,x)}}
$$</div><p>为了同时利用所有相关文档，切不超出模型最大输入长度，作者使用加权解码，用上述$\lambda(d,x)$作为权重：</p><div class=scroll-container>$$
p(y\vert x,\mathcal D^\prime)=\sum_{d\in\mathcal D^\prime}p(y\vert d\ \circ\ x)\cdot\lambda(d,x)
$$</div><p>其中$d\ \circ\ x$表示文档$d$和query $x$的拼接。</p><ul><li>带训练Method</li></ul><p>作者同时提出一种训练方法主要用于对齐检索器与生成器，训练过程中只更新检索器参数（针对黑盒模型）。首先用初始检索器检索$k$个最相关文档，与之前类似的，计算每个文档的权重分数：</p><div class=scroll-container>$$
P_R(d\vert x)=\frac{e^{s(d,x)/\gamma}}{\sum_{d\in\mathcal D^\prime}e^{s(d,x)/\gamma}}
$$</div><p>其中$\gamma$为超参数控制softmax的温度，计算得到的$P_R(d\vert x)$分布代表了检索器的检索分布。紧接着，给定ground truth $y$，对于每个相关文档，计算生成器在ground truth部分的LM perplexity $P_{LM}(y\vert d,x)$，并得到生成器的分布：</p><div class=scroll-container>$$
Q(d\vert x,y)=\frac{e^{P_{LM}(y\vert d,x)/\beta}}{\sum_{d\in\mathcal D^\prime}e^{P_{LM}(y\vert d,x)/\beta}}
$$</div><p>其中$\beta$也是调节softmax温度的超参数。最终根据上面检索器的分布$P_R(d\vert x)$和生成器的分布$Q(d\vert x,y)$计算两者的KL-divergence并作为损失函数优化检索器参数：</p><div class=scroll-container>$$
\mathcal{L} = \frac{1}{|\mathcal{B}|} \sum_{x \in \mathcal{B}} KL \left( P_R(d \mid x)\ \Vert\ Q_{LM}(d \mid x, y) \right)
$$</div><p>其中$\mathcal B$是query集合，每个query $x$均有一个ground truth $y$。注意到由于检索器参数更新，使得预先存好的所有文档向量表征会有所变化，为了高效训练，作者采用的方案是每训练$T$个steps后重新计算所有文档的向量表征。</p><h4 id=3-in-context-ralm-in-context-retrieval-augmented-language-models>3. In-Context RALM: In-Context Retrieval-Augmented Language Models<a hidden class=anchor aria-hidden=true href=#3-in-context-ralm-in-context-retrieval-augmented-language-models>#</a></h4><p><a href=https://aclanthology.org/2023.tacl-1.75/ class=entityLink>Ram et al. (2023)</a> 提出一种基于in-context的RAG方法，该方法主要使用了Retrieval Stride和Retrieval Query Length两个trick。</p><ul><li>In-Context RALM</li></ul><p>不同于普通的query-based RAG方法只基于query检索一次文档库，In-Context RALM会在生成过程中不断基于当前的生成结果去多次检索文档库，定义目前生成的文本（包括query）为$x_{\lt i}$，使用$x_{\lt i}$检索得到的文档内容为$\mathcal R_\mathcal C(x_{\lt i})$，那么In-Context RALM的生成过程可以通过如下公式定义：</p><div class=scroll-container>$$
p(x_i,\dots,x_n)=\Pi_{i=1}^n p_\theta(x_i\vert \mathcal R_\mathcal C(x_{\lt i});x_{\lt i})
$$</div><ul><li>Retrieval Stride</li></ul><p>由于频繁检索文档库会带来比较高的资源消耗，且降低生成速度，因此作者提出Retrieval Stride的概念，即每生成$s(s>1)$个token后进行一个检索，这样RALM的生成过程为：</p><div class=scroll-container>$$
p(x_1, \ldots, x_n) = \prod_{j=0}^{n_s-1} \prod_{i=1}^s p_\theta \left( x_{s \cdot j + i} \mid \left[ \mathcal{R}_\mathcal C(x_{\leq s \cdot j}); x_{\lt s \cdot j + i} \right] \right)
$$</div><p>其中$n_s=n/s$为检索的次数（Retrieval Strides）。实验结果表明使用较小的$s$（尽可能多的增加检索次数）会比使用较大的$s$效果好，但是会增加时间成本。</p><ul><li>Retrieval Query Length</li></ul><p>作者指出尽管检索query原则上取决于所有的prefix tokens $x_{\le s\cdot j}$，但是与生成token最相关的信息往往都聚集在prefix tokens的末尾，如果检索query太长那么这些信息会被稀释。对此作者提出Retrieval Query Length的概念，即控制query长度不超过$\ell$，当query长度超过$\ell$时截取整个query的最后$\ell$个token，即$q_j^{s,\ell}:=x_{s\cdot j-\ell+1},\cdots,x_{s\cdot j}$，应用上述trick后的生成过程定义如下：</p><div class=scroll-container>$$
p(x_1, \dots, x_n) =
\prod_{j=0}^{n_s-1} \prod_{i=1}^s
p_\theta \left( x_{s \cdot j + i} \middle| \left[ \mathcal{R}_\mathcal{C} \left( q_j^{s,\ell} \right); x_{< s \cdot j + i} \right] \right)
$$</div><h4 id=4-self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection>4. SELF-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection<a hidden class=anchor aria-hidden=true href=#4-self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection>#</a></h4><p><a href=https://iclr.cc/virtual/2024/poster/18095 class=entityLink>Asai et al. (2024)</a> 提出了一种基于反馈的RAG框架，主要通过引入Critic模型和Generator模型，Critic模型会在Generator模型生成前判断是否需要进行检索，如果不需要则直接让Generator生成下一个sequence（文章以一个完整的sequence为单位作为检索间隔），否则使用检索器检索相关文档，之后会让Critic判别每个文档的相关性与是否支持回答该问题等信息。作者在模型词表引入一些reflection tokens作为Critic模型的判别输出结果，通过基于prompt GPT4的方式获取有效监督数据并训练Critic模型以及Generator模型（蒸馏GPT4模型知识）。SELF-RAG整体流程框架如图4所示。</p><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image7.png alt=SELF-RAG-FRAMEWORK></p><div align=center style=color:#999>图4: SELF-RAG整体框架</div><ul><li>四类reflection tokens</li></ul><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image4.png alt=self-rag-reflection-tokens></p><div align=center style=color:#999>图5: SELF-RAG中使用的四类reflection tokens</div><ul><li>SELF-RAG Inference算法</li></ul><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image5.png alt=self-rag-inference></p><div align=center style=color:#999>图6: SELF-RAG Inference算法流程</div><ul><li>SELF-RAG Training算法（Critic $\mathcal C$ 和Generator $\mathcal M$）</li></ul><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image6.png alt=self-rag-training></p><div align=center style=color:#999>图7: SELF-RAG Training算法流程</div>其中Eq1:<p>$$
\max_\mathcal C\mathbb E_{((x,y),r)\sim\mathcal D_{critic}}\log p_\mathcal C(r\vert x,y),\ r\ \text{for reflection tokens}
$$</p><p>Eq2:</p><p>$$
\max_\mathcal M\mathbb E_{(x,y,r)\sim\mathcal D_{gen}}\log p_\mathcal M(y,r\vert x)
$$</p><h3 id=基于表征的rag方法representation-based>基于表征的RAG方法（Representation-based）<a hidden class=anchor aria-hidden=true href=#基于表征的rag方法representation-based>#</a></h3><h4 id=1-fid-leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering>1. FID: Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering<a hidden class=anchor aria-hidden=true href=#1-fid-leveraging-passage-retrieval-with-generative-models-for-open-domain-question-answering>#</a></h4><p><a href=https://aclanthology.org/2021.eacl-main.74/ class=entityLink>Izacard et al. (2021)</a> 提出一种基于representation的RAG方法，作者使用encoder-decoder模型（BART），对于检索器，使用BM25和DPR两种方法检索相关文档，对每个文档都分别使用encoder编码成隐空间表征，并将所有的表征拼接在一起输入decoder解码出answer，作者命名这类结构为Fusion-in-Decoder，结构示意图如图8所示。</p><p>作者在处理数据时，在问题（question），文档标题（title），文档内容（context）之前都添加了特殊tokens："$\text{question:}$"，$\text{title:}$，$\text{context:}$。</p><p><img loading=lazy src=/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image8.png alt=fusion-in-decoder></p><div align=center style=color:#999>图8: Fusion-in-Decoder结构图</div><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Gao et al. <a href=https://arxiv.org/abs/2312.10997 class=entityLink>“Retrieval-Augmented Generation for Large Language Models: A Survey”</a> arXiv preprint arXiv:2312.10997 (2023)</p><p>[2] Guu et al. <a href="https://proceedings.mlr.press/v119/guu20a.html?ref=https://githubhelp.com" class=entityLink>“Retrieval Augmented Language Model Pre-Training”</a> ICML 2020.</p><p>[3] Shi et al. <a href=https://aclanthology.org/2024.naacl-long.463/ class=entityLink>“REPLUG: Retrieval-Augmented Black-Box Language Models”</a> NAACL 2024.</p><p>[4] Ram et al. <a href=https://aclanthology.org/2023.tacl-1.75/ class=entityLink>“In-Context Retrieval-Augmented Language Models
”</a> TACL 2023.</p><p>[5] Asai et al. <a herf=https://iclr.cc/virtual/2024/poster/18095 class=entityLink>“Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection”</a> ICLR 2024.</p><p>[6] Izacard et al. <a href=https://aclanthology.org/2021.eacl-main.74/ class=entityLink>“Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering”</a> EACL 2021.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tqzhong.github.io/my-blog/tags/ai/>AI</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/rag/>RAG</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/><span class=title>« Prev</span><br><span>DeepSeek-R1技术报告解读</span>
</a><a class=next href=https://tqzhong.github.io/my-blog/posts/2024-11-21-reinforcement-learning/><span class=title>Next »</span><br><span>强化学习笔记</span></a></nav></footer></article><div class=social-icons><a href=https://github.com/tqzhong target=_blank rel="noopener noreferrer"><i class="fab fa-github"></i>
</a><a href=https://x.com/rs1047g target=_blank rel="noopener noreferrer"><i class="fab fa-twitter"></i>
</a><a href="https://scholar.google.com/citations?hl=en&amp;user=UNNLJX4AAAAJ" target=_blank rel="noopener noreferrer"><i class="fab fa-google"></i>
</a><a href=https://tqzhong.github.io target=_blank rel="noopener noreferrer"><i class="fas fa-user"></i></a></div><script src=https://utteranc.es/client.js repo=tqzhong/my-blog issue-term=pathname label=hugo-comment theme=github-dark crossorigin=anonymous async></script></main><footer class=footer><span>&copy; 2025 <a href=https://tqzhong.github.io/my-blog/>Rs' Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}]})})</script></body></html>