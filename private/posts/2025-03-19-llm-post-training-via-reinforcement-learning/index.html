<!doctype html><html lang=en dir=auto><a href=https://yourpersonalwebsite.com target=_blank><head><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="noindex, nofollow"><title>大模型post-training方法——强化学习篇 | Rs' Log</title>
<meta name=keywords content="AI,NLP,LLM,RL,Reasoning"><meta name=description content="PPO

PPO（Proximal Policy Optimization）算法出自Schulman et al.，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：

$$
\mathcal J_{PPO}(\theta)=\mathbb E_{[q\sim P(Q),o\sim \pi_{\theta_{old}}(O\vert q)]}\frac{1}{\vert o\vert}\sum_{t=1}^{\vert o\vert}\min\left[\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{\theta_{old}}(o_t\vert q,o_{< t})}A_t,\text{clip}\left(\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{\theta_{old}}(o_t\vert q,o_{< t})},1-\epsilon,1+\epsilon\right)A_t\right]
$$

其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：

$$
r_t=r_\phi(q,o_{1:\vert o\vert}) - \beta \log\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{ref}(o_t\vert q,o_{< t})}
$$


$$
A_t=\delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2}+\cdots=\sum_{l=0}^\infty (\gamma\lambda)^l\delta_{t+l}
$$


$$
\delta_t=r_t+\gamma V(s_{t+1}) - V(s_t)
$$

针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\vert o\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\vert o\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\phi(q,o_{1:\vert o\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\log\frac{\pi_\theta(\cdot)}{\pi_{ref}(\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12


def get_advantages_and_returns(self, values, rewards):
    lastgaelam = 0
    advantages_reversed = []
    max_seq_len = rewards.shape[-1]
    for t in reversed(range(max_seq_len)):
        nextvalues = values[:, t + 1] if t < max_seq_len - 1 else 0.0
        delta = rewards[:, t] + self.gamma * nextvalues - values[:, t]
        lastgaelam = delta + self.gamma * self.lam * lastgaelam
        advantages_reversed.append(lastgaelam)
    advantages = torch.stack(advantages_reversed[::-1], dim=1)
    returns = advantages + values
    return advantages, returns


经过一次for循环得到的分别是（令max_seq_len为$\vert o\vert$）："><meta name=author content="Rs"><link rel=canonical href=https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css><link crossorigin=anonymous href=/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/my-blog/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon type=image/x-icon sizes=48x48 href=images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=images/favicon-32x32.png><link rel=apple-touch-icon href=images/apple-touch-icon.png><link rel=icon sizes=512x512 href=images/android-chrome-512x512.png type=image/png><link rel=icon sizes=192x192 href=images/android-chrome-192x192.png type=image/png><link rel=mask-icon href=https://tqzhong.github.io/my-blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href="/my-blog/css/custom.css?v=1.7"></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},svg:{fontCache:"global"}}</script><nav class=nav><div class=logo-logo-switches><div class=logo><a href=https://tqzhong.github.io/my-blog/ accesskey=h title="Rs' Log (Alt + H)">Rs' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div></div><ul id=menu><li><a href=https://tqzhong.github.io/my-blog/ title=Posts><span>Posts</span></a></li><li><a href=https://tqzhong.github.io/my-blog/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://tqzhong.github.io/my-blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tqzhong.github.io/my-blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://tqzhong.github.io/my-blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://tqzhong.github.io/my-blog/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>大模型post-training方法——强化学习篇</h1><div class=post-meta><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-03-19 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;4 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span id=view-counter></span>
<script src=https://cdn.jsdelivr.net/npm/leancloud-storage@4.12.0/dist/av-min.js></script><script>AV.initialize("AokLJzaIvwVNJW0b2F0YTLLy-MdYXbMMI","fwZpRfBG259O3LscJFPW3ViH"),loadViewCount(location.pathname,"view-counter");var hasViewCounted=!1;function loadViewCount(e,t){if(hasViewCounted)return;hasViewCounted=!0;var s,n=document.getElementById(t),o=localStorage.getItem("view-count-"+e);o?n.innerText=o:n.innerText="0",s=new AV.Query("Counter"),s.equalTo("url",e),s.find().then(t=>{if(t.length>0){var s,i,a,o=t[0];o.increment("views",1),o.save(),i=o.get("views"),n.innerText=i,localStorage.setItem("view-count-"+e,i)}else a=AV.Object.extend("Counter"),s=new a,s.set("url",e),s.set("views",1),s.save().then(()=>{n.innerText="1",localStorage.setItem("view-count-"+e,1)})}).catch(function(e){console.error("error:",e)})}</script></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#ppo aria-label=PPO>PPO</a></li><li><a href=#grpo aria-label=GRPO>GRPO</a></li><li><a href=#dapo aria-label=DAPO>DAPO</a><ul><li><a href=#insights aria-label=Insights>Insights</a></li></ul></li><li><a href=#dr-grpo aria-label="Dr. GRPO">Dr. GRPO</a><ul><li><a href=#insights-1 aria-label=Insights>Insights</a></li></ul></li><li><a href=#skywork-open-reasoner-series aria-label="Skywork Open Reasoner Series">Skywork Open Reasoner Series</a><ul><li><a href=#data-preparation aria-label="Data Preparation">Data Preparation</a></li><li><a href=#multi-stage-grpo-with-adaptive-entropy-control aria-label="Multi-stage GRPO with Adaptive Entropy Control">Multi-stage GRPO with Adaptive Entropy Control</a></li><li><a href=#multi-stage-training aria-label="Multi-Stage Training">Multi-Stage Training</a></li><li><a href=#on-the-issue-of-truncated-samples aria-label="On the Issue of Truncated Samples">On the Issue of Truncated Samples</a></li><li><a href=#adaptive-entropy-control aria-label="Adaptive Entropy Control">Adaptive Entropy Control</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h3 id=ppo>PPO<a hidden class=anchor aria-hidden=true href=#ppo>#</a></h3><p>PPO（Proximal Policy Optimization）算法出自<a href=https://arxiv.org/abs/1707.06347 class=entityLink>Schulman et al.</a>，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：</p><div class=scroll-container>$$
\mathcal J_{PPO}(\theta)=\mathbb E_{[q\sim P(Q),o\sim \pi_{\theta_{old}}(O\vert q)]}\frac{1}{\vert o\vert}\sum_{t=1}^{\vert o\vert}\min\left[\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{\theta_{old}}(o_t\vert q,o_{< t})}A_t,\text{clip}\left(\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{\theta_{old}}(o_t\vert q,o_{< t})},1-\epsilon,1+\epsilon\right)A_t\right]
$$</div><p>其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：</p><div class=scroll-container>$$
r_t=r_\phi(q,o_{1:\vert o\vert}) - \beta \log\frac{\pi_\theta(o_t\vert q,o_{< t})}{\pi_{ref}(o_t\vert q,o_{< t})}
$$</div><div class=scroll-container>$$
A_t=\delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2}+\cdots=\sum_{l=0}^\infty (\gamma\lambda)^l\delta_{t+l}
$$</div><div class=scroll-container>$$
\delta_t=r_t+\gamma V(s_{t+1}) - V(s_t)
$$</div><p>针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\vert o\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\vert o\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\phi(q,o_{1:\vert o\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\log\frac{\pi_\theta(\cdot)}{\pi_{ref}(\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_advantages_and_returns</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>values</span><span class=p>,</span> <span class=n>rewards</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>lastgaelam</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>advantages_reversed</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>max_seq_len</span> <span class=o>=</span> <span class=n>rewards</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>reversed</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>max_seq_len</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>nextvalues</span> <span class=o>=</span> <span class=n>values</span><span class=p>[:,</span> <span class=n>t</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span> <span class=k>if</span> <span class=n>t</span> <span class=o>&lt;</span> <span class=n>max_seq_len</span> <span class=o>-</span> <span class=mi>1</span> <span class=k>else</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>        <span class=n>delta</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=n>nextvalues</span> <span class=o>-</span> <span class=n>values</span><span class=p>[:,</span> <span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>lastgaelam</span> <span class=o>=</span> <span class=n>delta</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>gamma</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>lam</span> <span class=o>*</span> <span class=n>lastgaelam</span>
</span></span><span class=line><span class=cl>        <span class=n>advantages_reversed</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>lastgaelam</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>advantages</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>advantages_reversed</span><span class=p>[::</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>returns</span> <span class=o>=</span> <span class=n>advantages</span> <span class=o>+</span> <span class=n>values</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>advantages</span><span class=p>,</span> <span class=n>returns</span>
</span></span></code></pre></td></tr></table></div></div><p>经过一次for循环得到的分别是（令max_seq_len为$\vert o\vert$）：</p><div class=scroll-container>$$
A_{t=\vert o\vert - 1}=\delta_{\vert o\vert - 1}\\\\
A_{t=\vert o\vert - 2}=(\gamma\lambda)\delta_{\vert o\vert - 1} + \delta_{\vert o\vert -2}\\\\
A_{t=\vert o\vert -3}=(\gamma\lambda)^2\delta_{\vert o\vert - 1} + (\gamma\lambda)\delta_{\vert o\vert -2} + \delta_{\vert o\vert - 3}\\\\
\cdots\\\\
A_{t=0}=(\gamma\lambda)^{\vert o\vert -1}\delta_{\vert o\vert - 1} + (\gamma\lambda)^{\vert o\vert -2}\delta_{\vert o\vert -2} + \cdots + (\gamma\lambda)\delta_{1} + \delta_0
$$</div><p>经过翻转后，得到优势向量$A_t=[A_{t=0}, A_{t=1},\cdots, A_{t=\vert o\vert - 1}]$，向量维度为(bs, max_seq_len)</p><h3 id=grpo>GRPO<a hidden class=anchor aria-hidden=true href=#grpo>#</a></h3><p>GRPO（Group Relative Policy Optimization）算法出自<a href=https://arxiv.org/abs/2402.03300 class=entityLink>Shao et al.</a>，其优化目标如下：</p><div class=scroll-container>$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i,
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i
\right]
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,< t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, < t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$</div><div class=scroll-container>$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) =
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, < t})}{\pi_{\theta}(o_{i, t} | q, o_{i, < t})}
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, < t})}{\pi_{\theta}(o_{i, t} | q, o_{i, < t})} - 1,
$$</div><div class=scroll-container>$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$</div><h3 id=dapo>DAPO<a hidden class=anchor aria-hidden=true href=#dapo>#</a></h3><p><a href=https://arxiv.org/abs/2503.14476 class=entityLink>Yu et al.</a>提出了DAPO（Decouple Clip and Dynamic sAmpling Policy Optimization）算法，该算法基于<a href=https://arxiv.org/abs/2402.03300 class=entityLink>GRPO</a>算法提出了四点改进，其优化目标如下：</p><div class=scroll-container>$$
\begin{align*}
\mathcal J_{DAPO}(\theta)&=\mathbb E_{(q,a)\sim\mathcal D,\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}(\cdot\vert q)}\left\{\frac{1}{\sum_{i=1}^G\vert o_i\vert}\sum_{i=1}^G\sum_{t=1}^{\vert o_i\vert}\min\left[r_{i,t}(\theta)\hat A_{i,t},\text{clip}\left(r_{i,t}(\theta),1-\epsilon_{\text{low}},1+\epsilon_{\text{high}}\right)\hat A_{i,t}\right]\right\}\\
% & \text{s.t.}\quad 0 < \vert \{o_i\vert \text{is\_equivalent}\}
& \text{s.t.}\quad 0 < \left\vert \{o_i\ \vert\ \text{is\_equivalent}(a, o_i)\}\right\vert < G,
\end{align*}
$$</div><div class=scroll-container>$$
r_{i,t}(\theta)=\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, < t})},\quad \hat A_{i,t}=\frac{R_i-\text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}
$$</div><p>首先作者移除了GRPO算法中的KL散度惩罚，作者认为对于训练long-CoT推理模型，actor model的输出分布与ref model的输出分布自然存在较大差异，没有必要设置KL散度限制。其次对于DAPO，作者采用基于规则的奖励模型，对于可验证任务（automated throrem proving、computer programming、mathematics competition），作者使用如下奖励函数，其中$\hat y$是预测答案，$y$是标准答案。</p><div class=scroll-container>$$
R(\hat y, y)=
\begin{cases}
1,& \text{is\_equivalent}(\hat y, y)\\
-1,& \text{otherwise}
\end{cases}
$$</div><h4 id=insights>Insights<a hidden class=anchor aria-hidden=true href=#insights>#</a></h4><p>接着，作者针对GRPO的理论缺陷提出了四点比较有意思的insights，每个insight对原本算法的改动都很小，但存在一定效果的提升。</p><ul><li><strong>Clip-Higher</strong></li></ul><p>TLDR：将原先的clip函数的上下界单独设置，而不是统一设置。</p><p>Motivation：对于生成的sentences，其中大部分token的概率值都较低，因此使用一个较低的$\epsilon$（一般算法设置$\epsilon=0.2$）对于这些低概率token的提升非常有限，比如$\pi_{\theta_{old}}(o_i\vert q)=0.01$，当$\epsilon=0.2$时，$\pi_{\theta}(o_i\vert q)$最大值只能为0.012。简单来说就是大部分token的概率值均偏低（&lt; 0.2），而低概率值的token更容易被clip（原因如上所述），作者认为这限制了模型对低概率token的提升，从而限制了整个模型的生成多样性。作者论文中实验设置了$\epsilon_{\text{low}}=0.2$，$\epsilon_{\text{high}}=0.28$。</p><ul><li><strong>Dynamic Sampling</strong></li></ul><p>TLDR：让每批采样的answer不能全部正确也不能全部错误。</p><p>Motivation：当每批采样的answer全部正确或全部错误时，计算得到的优势$\hat A_{i,t}=0$，这导致梯度值为零，那导致模型在这一步上等价于没有学习，降低了采样效率。因此作者在每个step会多次采样（理解为对同一批prompt生成answer），直到answer的平均准确率介于0和1之间。</p><ul><li><strong>Token-Level Policy Gradient Loss</strong></li></ul><p>TLDR：对一批生成样本中的每个token采用相同的损失贡献比例，而不是每个样本各自先按长度归一化损失再平均每个样本的损失。</p><p>Motivation：作者认为GRPO中的损失归一方式对long-CoT RL场景不友好，在GRPO中，每批样本中长样本的每个token贡献的损失比重会低于短样本的每个token贡献的损失比重，这会导致两个问题：1）对于高质量的长样本，这会阻碍模型学习这类样本的推理模式，2）对于低质量的长样本（出现重复，垃圾话），样本层级的损失计算也无法有效对这些样本进行惩罚。</p><ul><li><strong>Overlong Reward Shaping</strong></li></ul><p>TLDR：设置一个最大生成长度，对超出长度的样本进行惩罚。</p><p>Motivation：传统RL训练，对于过长样本会直接截断，但这种直接截断会带来噪音影响训练过程，因为一个合理但过长的样本被截断显然会影响模型训练。作者首先尝试将每批数据中被截断的损失mask掉，发现这会提升训练稳定性且提升模型性能。进一步地，作者设计了SoftOverlongPunishment，计算方式如下。这个惩罚性奖励被添加到原始基于规则的正确性奖励中一起计算总奖励。</p><div class=scroll-container>$$
R_{\text{length}}(y)=
\begin{cases}
0,& \vert y\vert \le L_{\text{max}} - L_{\text{cache}} \\\\
\frac{(L_{\text{max}}-L_{\text{cache}})-\vert y\vert}{L_{\text{cache}}},& L_{\text{max}}-L_{\text{cache}} < \vert y\vert\le L_{\text{max}}\\\\
-1,& L_{\text{max}} < \vert y\vert
\end{cases}
$$</div><h3 id=dr-grpo>Dr. GRPO<a hidden class=anchor aria-hidden=true href=#dr-grpo>#</a></h3><p><a href=https://github.com/sail-sg/understand-r1-zero class=entityLink>Liu et al.</a>提出Dr. GRPO，该工作指出GRPO算法存在的一些bias，并且这些bias可能导致了GRPO算法随着训练步数增加，生成answer长度不断增加的现象（包括出现aha moment）。该工作提出了两个bias：</p><ul><li><p>Response-level length bias：源于对损失除了$\vert o_i\vert$，这样，对于正优势（$\hat A_{i,t}> 0$，correct response），该偏差使得较短的response的梯度更大（$\vert o_i\vert$更小），从而导致策略倾向更简洁的正确回答。相反，对于负优势（$\hat A_{i,t}&lt; 0$，incorrect response），该偏差使得较长的response的梯度更大（$\vert o_i\vert$更大），从而导致策略倾向更复杂的错误回答。</p></li><li><p>Question-level difficulty bias：源于计算优势时对奖励偏差除了$\text{std}(\lbrace r_1,\cdots,r_G\rbrace)$。因此对于某个特定question，如果其answers容易得到较低的variance，那么这批answer的梯度会更大。通常来说优势归一化在常规RL算法中是在一整个batch上进行，而GRPO在每个question上进行归一化，导致对于不同question的answer，其梯度值可能会有较大差异。</p></li></ul><p>对此，作者移除了$\frac{1}{\vert o_i\vert}$和$\text{std}(\lbrace r_1,\cdots,r_G\rbrace)$，并将$\vert o_i\vert$替换为一个固定值。</p><div class=scroll-container>$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&=\frac{1}{G} \sum_{i=1}^G\textcolor{red}{\frac{1}{\vert o_i\vert}}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,< t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,< t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, < t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$</div><div class=scroll-container>$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\textcolor{red}{\text{std}(\{r_1, r_2, \cdots, r_G\})}}.
$$</div><h4 id=insights-1>Insights<a hidden class=anchor aria-hidden=true href=#insights-1>#</a></h4><p>作者对比了GRPO与Dr. GRPO，发现随着训练进行，Dr. GRPO的平均生成长度不会一直增加而是收敛。两者回答正确的answer长度均收敛，但不正确的answer长度中，GRPO不断增加而Dr. GRPO收敛甚至有所下降。两者最终性能表现相当。这证明了Dr. GRPO有更高的token efficiency。
<img loading=lazy src=/my-blog/images/2025-03-19-llm-post-training-via-reinforcement-learning/2025-03-19-image1.jpg alt=insights-dr.grpo></p><div align=center style=color:#999>图1：GRPO vs. Dr. GRPO</div><h3 id=skywork-open-reasoner-series>Skywork Open Reasoner Series<a hidden class=anchor aria-hidden=true href=#skywork-open-reasoner-series>#</a></h3><p>Blog链接：<a href=https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680 class=entityLink>He et al.</a></p><p>7B数学模型在AIME24上取得69.8%准确率（avg@8）</p><p><img loading=lazy src=/my-blog/images/2025-03-19-llm-post-training-via-reinforcement-learning/2025-03-19-image3.jpg alt=insights-dr.grpo></p><div align=center style=color:#999>图2：Skywork-OR1-Math-7B Performance on AIME24（avg@8）</div><h4 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h4><h4 id=multi-stage-grpo-with-adaptive-entropy-control>Multi-stage GRPO with Adaptive Entropy Control<a hidden class=anchor aria-hidden=true href=#multi-stage-grpo-with-adaptive-entropy-control>#</a></h4><p>优化目标函数如下：</p><div class=scroll-container>$$
\mathcal J(\theta)=\frac{1}{T_k}\sum_{i\in\mathcal T_k}\sum_{j=1}^M\left\{\sum_{t=0}^{\vert y_{ij}-1\vert}\min\{\rho_t^{ij}(\theta)\hat A_{ij},\text{clip }(\rho_t^{ij}(\theta),1-\epsilon,1+\epsilon)\hat A_{ij}\}-\alpha_k\mathbb H_t^{ij}(\theta)\right\}
$$</div><div class=scroll-container>$$
\hat A_{ij}=\frac{r_{ij}-\text{mean }(\textbf{r}_i)}{\text{std }(\textbf{r}_i)}
$$</div><div class=scroll-container>$$
\rho_{t}^{ij}(\theta)=\frac{\pi_\theta(a_t^{(ij)}\vert s_t^{(ij)})}{\pi_{\theta_{k}}(a_t^{(ij)}\vert s_t^{(ij)})}
$$</div><div class=scroll-container>$$
s_t^{(ij)}=(x_i,a_0^{(ij)},\cdots,a_{t-1}^{(ij)})
$$</div><div class=scroll-container>$$
T_k=\sum_{i\in\mathcal T_k}\sum_{j=1}^M\vert y_{ij}\vert
$$</div><div class=scroll-container>$$
\mathbb H_{t}^{ij}(\theta)=\mathcal H(\pi_\theta(\cdot\vert s_t^{(ij)}))=-\sum_{a_t^{(ij)}\in\mathcal V}\pi_\theta(a_t^{(ij)}\vert s_t^{(ij)})\log\pi_\theta(a_t^{(ij)}\vert s_t^{(ij)})
$$</div><p>相较于GRPO的主要改动：</p><ul><li>去除KL散度惩罚，这与DAPO，Dr. GRPO中的做法相一致。</li><li>增加生成熵的约束，防止模型熵爆炸。</li><li>归一化采用batch内所有数据归一化，即最后损失项乘了 $\frac{1}{\sum_{i\in\mathcal T_k}\sum_{j=1}^M}$，其中$\mathcal T_k$为当前batch中所有prompt的集合，$M$为每个prompt的生成answer数量。</li></ul><p>此外在训练数据处理上，作者针对long cot模型的训练加入了以下优化：</p><ul><li>Offline & online filtering：训练前，使用base model对每条prompt生成一批answer（M条），滤除answer完全正确或者完全错误的prompt；训练时，在每个epoch开始前，用上一轮epoch结束后的actor model对训练prompt生成一批answer（M条），滤除answer完全正确的prompt。</li><li>Rejection Sampling：每个训练step时，当前batch的所有prompt $x_i$，模型生成得到的结果并计算每条样本的优势$\hat A_{ij}$，要求每个prompt对应的所有优势中，至少有一个优势不为0，否则将滤除（理解是这样），公式表达如下：<div class=scroll-container>$$
\mathcal T_k:=\left\{i\in[N]:\exists\ j\in[M]\quad\hat A_{ij}\neq 0 \right\}
$$</div></li></ul><h4 id=multi-stage-training>Multi-Stage Training<a hidden class=anchor aria-hidden=true href=#multi-stage-training>#</a></h4><p>作者训练分为3个阶段，逐渐增大最大生成长度，主要用来减少训练时间，同时保证最终模型的性能。当前一个阶段性能收敛时进入下一个阶段（但感觉得通过实验来选取经验值，如第一阶段训练多少个step这种）。</p><table><thead><tr><th style=text-align:left>Stage1</th><th style=text-align:left>Stage2</th><th style=text-align:left>Stage3</th></tr></thead><tbody><tr><td style=text-align:left>8K</td><td style=text-align:left>16K</td><td style=text-align:left>32K</td></tr></tbody></table><h4 id=on-the-issue-of-truncated-samples>On the Issue of Truncated Samples<a hidden class=anchor aria-hidden=true href=#on-the-issue-of-truncated-samples>#</a></h4><p>作者针对最大生成长度限制内，出现生成过长导致截断的问题，提出了Advantage Mask For Truncated方法，并给出了两种方案：</p><ul><li>Adv-Mask-Before:</li></ul><div class=scroll-container>$$
\hat A_{ij}=
\begin{cases}
\frac{r(x_i,y_{ij})-\text{mean}(\hat {\mathbb R}_i)}{\text{std}(\hat{\mathbb R}_i)}& \vert y_{ij}\vert \le T_{\text{max}}\\\\
0,& \vert y_{ij}\vert > T_{\text{max}}
\end{cases}
$$</div>其中$\hat{\mathbb R}_i$为没被阶段的answer的奖励集合。<ul><li>Adv-Mask-After:</li></ul><div class=scroll-container>$$
\hat A_{ij}=
\begin{cases}
\frac{r(x_i,y_{ij})-\text{mean}({\mathbb R}_i)}{\text{std}({\mathbb R}_i)}& \vert y_{ij}\vert \le T_{\text{max}}\\\\
0,& \vert y_{ij}\vert > T_{\text{max}}
\end{cases}
$$</div>其中$\mathbb R_i$为所有answer的奖励集合。<p>实验结果发现两个mask效果都不如不加mask，因此作者没有使用这两种mask。</p><h4 id=adaptive-entropy-control>Adaptive Entropy Control<a hidden class=anchor aria-hidden=true href=#adaptive-entropy-control>#</a></h4><p>作者发现生成熵损失对超参数$\alpha_k$和训练数据分布都非常敏感，因此提出adaptive entropy control的方法。具体来说，作者设置一个熵阈值<code>tgt-ent</code>（即想要模型保持的生成熵水平）以及一个变化量$\vartriangle$。设置$\alpha_k$初始值为0，每个step前，用当前actor模型计算当前batch的平均生成熵<code>e</code>，如果<code>e</code>小于<code>tgt-ent</code>，那么增加$\alpha_k: \alpha_k=\alpha_k+\vartriangle$。考虑到增加熵损失会带来训练不稳定，因此当<code>e</code>大于<code>tgt-ent</code>时，该step不会启用熵损失，即理解为$\alpha_k=0$。实验中，作者设置<code>tgt-ent=0.2</code>，$\vartriangle$=0.005。</p><p><img loading=lazy src=/my-blog/images/2025-03-19-llm-post-training-via-reinforcement-learning/2025-03-19-image2.jpg alt=insights-dr.grpo></p><div align=center style=color:#999>图3：Adaptive entropy control, tgt-ent=0.2, $\vartriangle$=0.005</div><p>这部分的大概作用就是，不设置生成熵损失或者超参数$\alpha_k$比较小的时候，在较少的训练step后，模型的生成熵便会降到很低接近0的值，这对模型的探索能力起到负面影响，同时作者发现，当$\alpha_k$设置较大时（> 1e-3），在较少训练step后，模型的生成熵就爆炸了，因此可以理解，提高$\alpha_k$会在一定程度上扰乱模型的生成熵防止其快速收敛。训练的最终目的是让模型的生成熵稳定在一个较低水平但不是接近0的值，因此这种Adaptive Entropy Control方法可以很好地解决这个问题。当前估计的熵值<code>e</code>较小则提高$\alpha_k$，当前估计的熵值<code>e</code>较大则不启用熵损失，让模型自然训练降低生成熵即可。</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Yu et al. <a href=https://arxiv.org/abs/2503.14476 class=entityLink>“DAPO: An Open-Source LLM Reinforcement Learning System at Scale”</a> arXiv preprint arXiv:2503.14476 (2025).</p><p>[2] Shao et al. <a href=https://arxiv.org/abs/2402.03300 class=entityLink>“DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models”</a> arXiv preprint arXiv:2402.03300 (2024).</p><p>[3] Schulman et al. <a href=https://arxiv.org/abs/1707.06347 class=entityLink>“Proximal Policy Optimization Algorithms”</a> arXiv preprint arXiv:1707.06347 (2017).</p><p>[4] Liu et al. <a href=https://github.com/sail-sg/understand-r1-zero class=entityLink>“Understanding R1-Zero-Like Training: A Critical Perspective”</a> Github 2025.</p><p>[5] He et al. <a href=https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680 class=entityLink>“Skywork Open Reasoner Series”</a> Notion Blog 2025.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tqzhong.github.io/my-blog/tags/ai/>AI</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/nlp/>NLP</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/llm/>LLM</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/rl/>RL</a></li><li><a href=https://tqzhong.github.io/my-blog/tags/reasoning/>Reasoning</a></li></ul><nav class=paginav><a class=next href=https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/><span class=title>Next »</span><br><span>GRPO From Scratch</span></a></nav></footer></article><div class=social-icons><a href=https://github.com/tqzhong target=_blank rel="noopener noreferrer"><i class="fab fa-github"></i>
</a><a href=https://x.com/rs1047g target=_blank rel="noopener noreferrer"><i class="fab fa-twitter"></i>
</a><a href="https://scholar.google.com/citations?hl=en&amp;user=UNNLJX4AAAAJ" target=_blank rel="noopener noreferrer"><i class="fab fa-google"></i>
</a><a href=https://tqzhong.github.io target=_blank rel="noopener noreferrer"><i class="fas fa-user"></i></a></div><script src=https://utteranc.es/client.js repo=tqzhong/my-blog issue-term=pathname label=hugo-comment theme=github-dark crossorigin=anonymous async></script></main><footer class=footer><span>&copy; 2025 <a href=https://tqzhong.github.io/my-blog/>Rs' Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}]})})</script></body></html>