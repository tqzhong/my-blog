<!doctype html><html lang=en dir=auto><a href=https://yourpersonalwebsite.com target=_blank><head><script src=https://cdn.jsdelivr.net/npm/fuse.js/dist/fuse.min.js></script><link rel=stylesheet href=https://unpkg.com/@waline/client@v3/dist/waline.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/plugins/line-numbers.min.js></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="noindex, nofollow"><title>NLP | Rs' Log</title>
<meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://tqzhong.github.io/my-blog/tags/nlp/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css><link crossorigin=anonymous href=/my-blog/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon type=image/x-icon sizes=48x48 href=images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=images/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=images/favicon-32x32.png><link rel=apple-touch-icon href=images/apple-touch-icon.png><link rel=icon sizes=512x512 href=images/android-chrome-512x512.png type=image/png><link rel=icon sizes=192x192 href=images/android-chrome-192x192.png type=image/png><link rel=mask-icon href=https://tqzhong.github.io/my-blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://tqzhong.github.io/my-blog/tags/nlp/index.xml><link rel=alternate hreflang=en href=https://tqzhong.github.io/my-blog/tags/nlp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href="/my-blog/css/custom.css?v=1.7"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},svg:{fontCache:"global"}}</script><nav class=nav><div class=logo-logo-switches><div class=logo><a href=https://tqzhong.github.io/my-blog/ accesskey=h title="Rs' Log (Alt + H)">Rs' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div></div><ul id=menu><li><a href=https://tqzhong.github.io/my-blog/ title=Posts><span>Posts</span></a></li><li><a href=https://tqzhong.github.io/my-blog/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://tqzhong.github.io/my-blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://tqzhong.github.io/my-blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://tqzhong.github.io/my-blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://tqzhong.github.io/my-blog/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>NLP</h1></h1></header><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/>大模型post-training方法——强化学习篇</a></h2></header><div class=entry-content><p>PPO PPO（Proximal Policy Optimization）算法出自Schulman et al.，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：
$$ \mathcal J_{PPO}(\theta)=\mathbb E_{[q\sim P(Q),o\sim \pi_{\theta_{old}}(O\vert q)]}\frac{1}{\vert o\vert}\sum_{t=1}^{\vert o\vert}\min\left[\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})}A_t,\text{clip}\left(\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})},1-\epsilon,1+\epsilon\right)A_t\right] $$ 其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：
$$ r_t=r_\phi(q,o_{1:\vert o\vert}) - \beta \log\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{ref}(o_t\vert q,o_{&lt; t})} $$ $$ A_t=\delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2}+\cdots=\sum_{l=0}^\infty (\gamma\lambda)^l\delta_{t+l} $$ $$ \delta_t=r_t+\gamma V(s_{t+1}) - V(s_t) $$ 针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\vert o\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\vert o\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\phi(q,o_{1:\vert o\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\log\frac{\pi_\theta(\cdot)}{\pi_{ref}(\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：
1 2 3 4 5 6 7 8 9 10 11 12 def get_advantages_and_returns(self, values, rewards): lastgaelam = 0 advantages_reversed = [] max_seq_len = rewards.shape[-1] for t in reversed(range(max_seq_len)): nextvalues = values[:, t + 1] if t &lt; max_seq_len - 1 else 0.0 delta = rewards[:, t] + self.gamma * nextvalues - values[:, t] lastgaelam = delta + self.gamma * self.lam * lastgaelam advantages_reversed.append(lastgaelam) advantages = torch.stack(advantages_reversed[::-1], dim=1) returns = advantages + values return advantages, returns 经过一次for循环得到的分别是（令max_seq_len为$\vert o\vert$）：
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-03-19 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;4 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to 大模型post-training方法——强化学习篇" href=https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/>GRPO From Scratch</a></h2></header><div class=entry-content><p>简介 本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：GRPO_From-Scratch以及GRPO_Qwen-0_5_Instruct。注：原作者使用8张80G A100完成实验。
GRPO GRPO算法原理见alg-grpo，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。
$$ \begin{align*} \mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\ &=\frac{1}{G} \sum_{i=1}^G \left\{ \min \left[ \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right] - \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}] \right\}\\ &=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\} \end{align*} $$ $$ D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1, $$ $$ \hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}. $$ GRPO算法出自文章DeepSeekMath (2024)，其中KL散度的计算采用了Approximating KL Divergence中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-03-05 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;22 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2025-03-05-grpo/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to GRPO From Scratch" href=https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/>DeepSeek-V3技术报告解读</a></h2></header><div class=entry-content><p>1. 摘要 DeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：DeepSeek-V3
2. DeepSeek-V3模型结构 2.1 Basic Architecture 图1: DeepSeek-V3基础结构图 DeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。
2.1.1 Multi-Head Latent Attention 定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\bold{h}_t\in\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：
$$ \textcolor{blue}{\bold{c}_t^{KV}}=W^{DKV}\bold{h}_t,\\ $$ $$ \left[\mathbf{k}_{t,1}; \mathbf{k}^C_{t,2}; \dots; \mathbf{k}^C_{t,n_h} \right] = \mathbf{k}^C_t = W^{UK} \mathbf{c}^{KV}_t, $$ $$ \textcolor{blue}{\mathbf{k}^R_t} = \mathrm{RoPE}(W^{KR} \mathbf{h}_t), $$ $$ \mathbf{k}_{t,i} = \left[\mathbf{k}^C_{t,i}; \mathbf{k}^R_t \right], $$ $$ \left[\mathbf{v}^C_{t,1}; \mathbf{v}^C_{t,2}; \dots; \mathbf{v}^C_{t,n_h} \right] = \mathbf{v}^C_t = W^{UV} \mathbf{c}^{KV}_t. $$ 其中$\bold{c}_t^{KV}\in\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\in\mathbb R^{d_c\times d}$为下投影矩阵，$W^{UK},W^{UV}\in\mathbb R^{d_hn_h\times d_c}$为key和value的上投影矩阵。$W^{KR}\in\mathbb R^{d_h^R\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\textcolor{blue}{\bold{c}_t^{KV}}$和$\textcolor{blue}{\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-01-29 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;4 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2025-01-29-deepseek-v3/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to DeepSeek-V3技术报告解读" href=https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/>DeepSeek-R1技术报告解读</a></h2></header><div class=entry-content><p>1. 摘要 本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。
2. 主要贡献 新的后训练范式：在Base Model上直接使用Large-Scale RL
不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了自我验证，反思，生成长的CoT的能力。 团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。 蒸馏：小模型也可以很强大
开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。 3. 方法 3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model DeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。
3.1.1 Reinforcement Learning Algorithm 团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\pi_{\theta_{old}}$采样一组输出${o_1,o_2,\cdots,o_G}$，然后使用如下优化目标优化策略模型$\pi_\theta$：
$$ \begin{align*} \mathcal{J}_{\text{GRPO}}(\theta) &= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\ &=\frac{1}{G} \sum_{i=1}^G \left( \min \left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right) - \beta D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) \right), \end{align*} $$ $$ D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - \log \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1, $$ $$ A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}. $$ 其中$\epsilon$和$\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\cdots,r_G}$计算得到。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-01-27 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;2 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2025-01-27-deepseek-r1/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to DeepSeek-R1技术报告解读" href=https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/>RAG路线</a></h2></header><div class=entry-content><p>Retrieval-Augmented Generation for Large Language Models: A Survey 1. Overview of RAG 典型的RAG模型如图1所示 图1: 经典RAG模型 1.1 Naive RAG Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。
索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。 检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。 生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。 1.2 Advanced RAG Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。
pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。 优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。 优化初始query：常用的策略有query transformation，query expansion等。 post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。 chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。 chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。 1.3 Modular RAG 模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。
引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等 引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。 图2: 三类不同RAG模型流程示意图 2. Retrieval Part 2.1 检索资源 从检索内容的数据上来看包含以下几种：
无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等 半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。 结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。 LLMs生成内容 从检索的粒度来看，包含以下几种：
对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章 对于知识图谱，检索粒度包含：实体，三元组，子图 2.2 索引的优化 在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2025-01-08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;3 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2025-01-08-retrieval-augmented-generation/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to RAG路线" href=https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/>Deepspeed多机多卡训练&代码细节</a></h2></header><div class=entry-content><p>本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。
Supervised finetuning 首先在主节点克隆deepspeed-chat仓库。
使用的主要环境：
1 2 3 4 5 6 7 8 9 pip install torch==1.13.0 pip install datasets pip install sentencepiece pip install protobuf==3.20.3 pip install accelerate pip install deepspeed==0.10.0 pip install transformers==4.44.2 pip install tensorboard pip install numpy==1.26.4 deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：
1 2 sudo apt update sudo apt install nvidia-cuda-toolkit 之后先跑通单节点，我用的是step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。
跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了synthetic-instruct-gptj-pairwise数据集，两个文件保存在主节点：
1 2 3 4 datasets └── synthetic-instruct-gptj-pairwise ├── dataset_infos.json └── train-00000-of-00001-1e5d57b93c448e7a.parquet 在dschat/utils/data/raw_datasets.py的数据集类PromptRawDataset上也做了对应修改:
1 2 3 4 5 6 7 8 9 class PromptRawDataset(object): def __init__(self, output_path, seed, local_rank, dataset_name): self.output_path = output_path self.seed = seed self.local_rank = local_rank '''原始数据的读取，这里根据自己数据集作相应修改''' self.raw_datasets = load_dataset('parquet', data_files=dataset_name) ... 到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是FusedAdam，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成AdamW就跑通了。查了一下FusedAdam在需要大量计算资源的场景下有一定优势。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2024-10-30 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;9 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/2024-10-30-deepspeed/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to Deepspeed多机多卡训练&代码细节" href=https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/></a></article><article class="post-entry bordered-article"><header class=entry-header><h2><a href=https://tqzhong.github.io/my-blog/posts/llm-post-training/>大模型post-training方法</a></h2></header><div class=entry-content><p>1. DPO Rafailov et al. (2023)基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：
$$ \mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})] $$ 2. Simple-DPO Meng et al. (2024)考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：
$$ p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i}) $$ 因此Simple-DPO考虑将奖励函数表达式改为：
$$ r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i}) $$ 此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma>0)$，表达式如下：
$$ p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma) $$ 从而，Simple-DPO的优化函数： $$ \mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)] $$ 3. KTO KTO loss (Ethayarajh et al. (2024))与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。
...</p></div><footer class=entry-footer><i class="fas fa-calendar-alt blog-meta-icon"></i>&nbsp;2024-10-09 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-clock blog-meta-icon"></i>&nbsp;3 min &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-user blog-meta-icon"></i>&nbsp;Rs &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <i class="fas fa-eye blog-meta-icon"></i>&nbsp;<span class=view-counter data-url=https://tqzhong.github.io/my-blog/posts/llm-post-training/></span>
<script>document.addEventListener("DOMContentLoaded",function(){var t,e=document.querySelector(".view-counter[data-url='https://tqzhong.github.io/my-blog/posts/llm-post-training/']");e&&(t=localStorage.getItem("view-count-/my-blog/posts/llm-post-training/"),e.innerText=t||"0")})</script></footer><a class=entry-link aria-label="post link to 大模型post-training方法" href=https://tqzhong.github.io/my-blog/posts/llm-post-training/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://tqzhong.github.io/my-blog/>Rs' Log</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js></script><script src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}]})})</script></body></html>