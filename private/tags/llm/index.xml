<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>LLM on Rs' Log</title><link>https://tqzhong.github.io/my-blog/tags/llm/</link><description>Recent content in LLM on Rs' Log</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 19 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tqzhong.github.io/my-blog/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>大模型post-training方法——强化学习篇</title><link>https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/</guid><description>&lt;h3 id="ppo">PPO&lt;/h3>
&lt;!-- #### Algorithm -->
&lt;p>PPO（Proximal Policy Optimization）算法出自&lt;a href="https://arxiv.org/abs/1707.06347" class="entityLink">Schulman et al.&lt;/a>，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：&lt;/p>
&lt;div class="scroll-container">
$$
\mathcal J_{PPO}(\theta)=\mathbb E_{[q\sim P(Q),o\sim \pi_{\theta_{old}}(O\vert q)]}\frac{1}{\vert o\vert}\sum_{t=1}^{\vert o\vert}\min\left[\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})}A_t,\text{clip}\left(\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})},1-\epsilon,1+\epsilon\right)A_t\right]
$$
&lt;/div>
&lt;p>其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：&lt;/p>
&lt;div class="scroll-container">
$$
r_t=r_\phi(q,o_{1:\vert o\vert}) - \beta \log\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{ref}(o_t\vert q,o_{&lt; t})}
$$
&lt;/div>
&lt;div class="scroll-container">
$$
A_t=\delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2}+\cdots=\sum_{l=0}^\infty (\gamma\lambda)^l\delta_{t+l}
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\delta_t=r_t+\gamma V(s_{t+1}) - V(s_t)
$$
&lt;/div>
&lt;p>针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\vert o\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\vert o\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\phi(q,o_{1:\vert o\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\log\frac{\pi_\theta(\cdot)}{\pi_{ref}(\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_advantages_and_returns&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rewards&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lastgaelam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">advantages_reversed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">max_seq_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rewards&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">reversed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_seq_len&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nextvalues&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">max_seq_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">delta&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rewards&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gamma&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">nextvalues&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">values&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lastgaelam&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">delta&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gamma&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lam&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">lastgaelam&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">advantages_reversed&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lastgaelam&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">advantages&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">advantages_reversed&lt;/span>&lt;span class="p">[::&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">returns&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">advantages&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">values&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">advantages&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">returns&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>经过一次for循环得到的分别是（令max_seq_len为$\vert o\vert$）：&lt;/p></description></item><item><title>GRPO From Scratch</title><link>https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/</guid><description>&lt;h3 id="简介">简介&lt;/h3>
&lt;p>本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：&lt;a href="https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb" class="entityLink">GRPO_From-Scratch&lt;/a>以及&lt;a href="https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb" class="entityLink">GRPO_Qwen-0_5_Instruct&lt;/a>。注：原作者使用8张80G A100完成实验。&lt;/p>
&lt;h3 id="grpo">GRPO&lt;/h3>
&lt;p>GRPO算法原理见&lt;a href="https://matrixai.online/my-blog/posts/2025-01-27-deepseek-r1/#311-reinforcement-learning-algorithm" class="entityLink">alg-grpo&lt;/a>，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。&lt;/p>
&lt;div class="scroll-container">
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &amp;= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&amp;=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i,
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i
\right]
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&amp;=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$
&lt;/div>
&lt;div class="scroll-container">
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) =
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})}
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1,
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
&lt;/div>
&lt;p>GRPO算法出自文章&lt;a href="https://arxiv.org/abs/2402.03300" class="entityLink">DeepSeekMath (2024)&lt;/a>，其中KL散度的计算采用了&lt;a href="http://joschu.net/blog/kl-approx.html" class="entityLink">Approximating KL Divergence&lt;/a>中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。&lt;/p></description></item><item><title>DeepSeek-V3技术报告解读</title><link>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</guid><description>&lt;h3 id="1-摘要">1. 摘要&lt;/h3>
&lt;p>DeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：&lt;a href="https://github.com/deepseek-ai/DeepSeek-V3" class="entityLink">DeepSeek-V3&lt;/a>&lt;/p>
&lt;h3 id="2-deepseek-v3模型结构">2. DeepSeek-V3模型结构&lt;/h3>
&lt;h4 id="21-basic-architecture">2.1 Basic Architecture&lt;/h4>
&lt;p>&lt;img loading="lazy" src="https://tqzhong.github.io/my-blog/my-blog/images/2025-01-29-deepseek-v3/2025-01-29-image2.jpg" alt="deepseek-v3-architecture" />
&lt;/p>
&lt;div align='center' style="color: #999999">图1: DeepSeek-V3基础结构图&lt;/div>
&lt;p>DeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。&lt;/p>
&lt;h5 id="211-multi-head-latent-attention">2.1.1 Multi-Head Latent Attention&lt;/h5>
&lt;p>定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\bold{h}_t\in\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：&lt;/p>
&lt;div class="scroll-container">
$$
\textcolor{blue}{\bold{c}_t^{KV}}=W^{DKV}\bold{h}_t,\\
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\left[\mathbf{k}_{t,1}; \mathbf{k}^C_{t,2}; \dots; \mathbf{k}^C_{t,n_h} \right] = \mathbf{k}^C_t = W^{UK} \mathbf{c}^{KV}_t,
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\textcolor{blue}{\mathbf{k}^R_t} = \mathrm{RoPE}(W^{KR} \mathbf{h}_t),
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\mathbf{k}_{t,i} = \left[\mathbf{k}^C_{t,i}; \mathbf{k}^R_t \right],
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\left[\mathbf{v}^C_{t,1}; \mathbf{v}^C_{t,2}; \dots; \mathbf{v}^C_{t,n_h} \right] = \mathbf{v}^C_t = W^{UV} \mathbf{c}^{KV}_t.
$$
&lt;/div>
&lt;p>其中$\bold{c}_t^{KV}\in\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\in\mathbb R^{d_c\times d}$为下投影矩阵，$W^{UK},W^{UV}\in\mathbb R^{d_hn_h\times d_c}$为key和value的上投影矩阵。$W^{KR}\in\mathbb R^{d_h^R\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\textcolor{blue}{\bold{c}_t^{KV}}$和$\textcolor{blue}{\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。&lt;/p></description></item><item><title>DeepSeek-R1技术报告解读</title><link>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</guid><description>&lt;h3 id="1-摘要">1. 摘要&lt;/h3>
&lt;p>本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。&lt;/p>
&lt;h3 id="2-主要贡献">2. 主要贡献&lt;/h3>
&lt;p>新的后训练范式：在Base Model上直接使用Large-Scale RL&lt;/p>
&lt;ul>
&lt;li>不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了&lt;strong>自我验证，反思，生成长的CoT&lt;/strong>的能力。&lt;/li>
&lt;li>团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。&lt;/li>
&lt;/ul>
&lt;p>蒸馏：小模型也可以很强大&lt;/p>
&lt;ul>
&lt;li>开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。&lt;/li>
&lt;/ul>
&lt;h3 id="3-方法">3. 方法&lt;/h3>
&lt;h4 id="31-deepseek-r1-zero-reinforcement-learning-on-the-base-model">3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model&lt;/h4>
&lt;p>DeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。&lt;/p>
&lt;h5 id="311-reinforcement-learning-algorithm">3.1.1 Reinforcement Learning Algorithm&lt;/h5>
&lt;p>团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\pi_{\theta_{old}}$采样一组输出${o_1,o_2,\cdots,o_G}$，然后使用如下优化目标优化策略模型$\pi_\theta$：&lt;/p>
&lt;div class="scroll-container">
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &amp;= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&amp;=\frac{1}{G} \sum_{i=1}^G \left(
\min \left(
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i,
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i
\right)
- \beta D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}})
\right),
\end{align*}
$$
&lt;/div>
&lt;div class="scroll-container">
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) =
\frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)}
- \log \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1,
$$
&lt;/div>
&lt;div class="scroll-container">
$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
&lt;/div>
&lt;p>其中$\epsilon$和$\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\cdots,r_G}$计算得到。&lt;/p></description></item><item><title>Deepspeed多机多卡训练&amp;代码细节</title><link>https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/</guid><description>&lt;p>本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。&lt;/p>
&lt;h3 id="supervised-finetuning">Supervised finetuning&lt;/h3>
&lt;p>首先在主节点克隆&lt;a href="https://github.com/microsoft/DeepSpeedExamples" class="entityLink">deepspeed-chat&lt;/a>仓库。&lt;/p>
&lt;p>使用的主要环境：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">pip install &lt;span class="nv">torch&lt;/span>&lt;span class="o">==&lt;/span>1.13.0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install datasets
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install sentencepiece
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install &lt;span class="nv">protobuf&lt;/span>&lt;span class="o">==&lt;/span>3.20.3
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install accelerate
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install &lt;span class="nv">deepspeed&lt;/span>&lt;span class="o">==&lt;/span>0.10.0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install &lt;span class="nv">transformers&lt;/span>&lt;span class="o">==&lt;/span>4.44.2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install tensorboard
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">pip install &lt;span class="nv">numpy&lt;/span>&lt;span class="o">==&lt;/span>1.26.4
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">sudo apt update
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sudo apt install nvidia-cuda-toolkit
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>之后先跑通单节点，我用的是&lt;code>step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh&lt;/code>，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。&lt;/p>
&lt;p>跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了&lt;code>synthetic-instruct-gptj-pairwise&lt;/code>数据集，两个文件保存在主节点：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">datasets
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └── synthetic-instruct-gptj-pairwise
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ├── dataset_infos.json
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └── train-00000-of-00001-1e5d57b93c448e7a.parquet
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在&lt;code>dschat/utils/data/raw_datasets.py&lt;/code>的数据集类&lt;code>PromptRawDataset&lt;/code>上也做了对应修改:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">PromptRawDataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">object&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_path&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seed&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">local_rank&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dataset_name&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output_path&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">seed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">local_rank&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;&amp;#39;&amp;#39;原始数据的读取，这里根据自己数据集作相应修改&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">raw_datasets&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">load_dataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;parquet&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">data_files&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dataset_name&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是&lt;code>FusedAdam&lt;/code>，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成&lt;code>AdamW&lt;/code>就跑通了。查了一下&lt;code>FusedAdam&lt;/code>在需要大量计算资源的场景下有一定优势。&lt;/p></description></item><item><title>大模型post-training方法</title><link>https://tqzhong.github.io/my-blog/posts/llm-post-training/</link><pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/llm-post-training/</guid><description>&lt;!--tips:-->
&lt;!--公式块里，如果加了class=scroll-container(滚轮滑块防止单行公式太长)，大小于号注意要与后面的字符隔开一个空格，否则无法正常编译-->
&lt;!--常规公式块里，换行要用\\\\而不是\\否则无法正常编译，但是在{cases}环境里，换行用\\即可（目前发现这个，后续有其他再补充）-->
&lt;h3 id="1-dpo">1. DPO&lt;/h3>
&lt;p>&lt;a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html" class="entityLink">Rafailov et al. (2023)&lt;/a>基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：&lt;/p>
&lt;div class="scroll-container">
$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$
&lt;/div>
&lt;h3 id="2-simple-dpo">2. Simple-DPO&lt;/h3>
&lt;p>&lt;a href="https://arxiv.org/abs/2405.14734" class="entityLink">Meng et al. (2024)&lt;/a>考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：&lt;/p>
&lt;div class="scroll-container">
$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i})
$$
&lt;/div>
&lt;p>因此Simple-DPO考虑将奖励函数表达式改为：&lt;/p>
&lt;div class="scroll-container">
$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i})
$$
&lt;/div>
&lt;p>此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma&amp;gt;0)$，表达式如下：&lt;/p>
&lt;div class="scroll-container">
$$
p(y_w>y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$
&lt;/div>
从而，Simple-DPO的优化函数：
&lt;div class="scroll-container">
$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$
&lt;/div>
&lt;h3 id="3-kto">3. KTO&lt;/h3>
&lt;p>KTO loss (&lt;a href="https://arxiv.org/abs/2402.01306" class="entityLink">Ethayarajh et al. (2024)&lt;/a>)与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。&lt;/p></description></item></channel></rss>