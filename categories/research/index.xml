<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Research on Rs&#39; Log</title>
    <link>https://tqzhong.github.io/my-blog/categories/research/</link>
    <description>Recent content in Research on Rs&#39; Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tqzhong.github.io/my-blog/categories/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型post-training方法——强化学习篇</title>
      <link>https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/</link>
      <pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2025-03-19-llm-post-training-via-reinforcement-learning/</guid>
      <description>&lt;h3 id=&#34;ppo&#34;&gt;PPO&lt;/h3&gt;
&lt;!-- #### Algorithm --&gt;
&lt;p&gt;PPO（Proximal Policy Optimization）算法出自&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34; class=&#34;entityLink&#34;&gt;Schulman et al.&lt;/a&gt;，在微调大模型中，该算法通过最大化以下目标函数来优化模型参数：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\mathcal J_{PPO}(\theta)=\mathbb E_{[q\sim P(Q),o\sim \pi_{\theta_{old}}(O\vert q)]}\frac{1}{\vert o\vert}\sum_{t=1}^{\vert o\vert}\min\left[\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})}A_t,\text{clip}\left(\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{\theta_{old}}(o_t\vert q,o_{&lt; t})},1-\epsilon,1+\epsilon\right)A_t\right]
$$
&lt;/div&gt;
&lt;p&gt;其中优势函数$A_t$通过使用GAE（Generalized Advantage Estimation）算法计算得到：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
r_t=r_\phi(q,o_{1:\vert o\vert}) - \beta \log\frac{\pi_\theta(o_t\vert q,o_{&lt; t})}{\pi_{ref}(o_t\vert q,o_{&lt; t})}
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
A_t=\delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2}+\cdots=\sum_{l=0}^\infty (\gamma\lambda)^l\delta_{t+l}
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\delta_t=r_t+\gamma V(s_{t+1}) - V(s_t)
$$
&lt;/div&gt;
&lt;p&gt;针对大模型微调的场景，$q$为问题（或者prompt），假设其最大长度为max_prompt_len，生成的$o_{1:\vert o\vert}$为答案（或者generation sentence），假设其最大长度为max_seq_len。上式中$r_t$为奖励，$r_\phi$为reward model（PPO优化中参数不更新），该模型输入$q$和$o_{1:\vert o\vert}$得到每个句子的最后一个有效token上的reward score，因此$r_\phi(q,o_{1:\vert o\vert})$的维度可以记作(bs,)（$bs$为ppo批量大小），KL惩罚项使用估计项$\log\frac{\pi_\theta(\cdot)}{\pi_{ref}(\cdot)}$，该项得到的维度为(bs, max_seq_len)，因此最终的奖励向量$r_t$维度为(bs, max_seq_len)。接着看一下DeepSpeed中对优势函数和回报实现的代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_advantages_and_returns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rewards&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;lastgaelam&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;advantages_reversed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;max_seq_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rewards&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;reversed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_seq_len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;nextvalues&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_seq_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rewards&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nextvalues&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;lastgaelam&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;delta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gamma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lam&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lastgaelam&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;advantages_reversed&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lastgaelam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;advantages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;advantages_reversed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[::&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;returns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;advantages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;advantages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;returns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;经过一次for循环得到的分别是（令max_seq_len为$\vert o\vert$）：&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GRPO From Scratch</title>
      <link>https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/</link>
      <pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2025-03-05-grpo/</guid>
      <description>&lt;h3 id=&#34;简介&#34;&gt;简介&lt;/h3&gt;
&lt;p&gt;本篇博客基于Andriy Burkov的grpo开源代码，简单跑通GRPO的完整流程。使用的GPU资源为1张3090（24G）。原作者代码见：&lt;a href=&#34;https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb&#34; class=&#34;entityLink&#34;&gt;GRPO_From-Scratch&lt;/a&gt;以及&lt;a href=&#34;https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb&#34; class=&#34;entityLink&#34;&gt;GRPO_Qwen-0_5_Instruct&lt;/a&gt;。注：原作者使用8张80G A100完成实验。&lt;/p&gt;
&lt;h3 id=&#34;grpo&#34;&gt;GRPO&lt;/h3&gt;
&lt;p&gt;GRPO算法原理见&lt;a href=&#34;https://matrixai.online/my-blog/posts/2025-01-27-deepseek-r1/#311-reinforcement-learning-algorithm&#34; class=&#34;entityLink&#34;&gt;alg-grpo&lt;/a&gt;，原作者在这块的实现基本遵从DeepSeek技术报告中的损失公式，后面代码处详细展开。&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &amp;= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&amp;=\frac{1}{G} \sum_{i=1}^G \left\{
\min \left[ 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i 
\right] 
- \beta \mathbb D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]
\right\}\\
&amp;=\frac{1}{G} \sum_{i=1}^G\frac{1}{\vert o_i\vert}\sum_{t=1}^{\vert o_i\vert}\left\{\min\left[\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q, o_{i,&lt; t})}\hat A_{i,t},\ \text{clip}\left(\frac{\pi_\theta(o_{i,t}\vert q,o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\vert q,o_{i, &lt; t})},1-\epsilon,1+\epsilon\right)\hat A_{i,t}\right] - \beta\mathbb D_{KL}[\pi_\theta\Vert\pi_{\text{ref}}]\right\}
\end{align*}
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} 
- \log \frac{\pi_{\text{ref}}(o_{i, t} | q, o_{i, &lt; t})}{\pi_{\theta}(o_{i, t} | q, o_{i, &lt; t})} - 1,
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\hat A_{i,t}=A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
&lt;/div&gt;
&lt;p&gt;GRPO算法出自文章&lt;a href=&#34;https://arxiv.org/abs/2402.03300&#34; class=&#34;entityLink&#34;&gt;DeepSeekMath (2024)&lt;/a&gt;，其中KL散度的计算采用了&lt;a href=&#34;http://joschu.net/blog/kl-approx.html&#34; class=&#34;entityLink&#34;&gt;Approximating KL Divergence&lt;/a&gt;中的无偏估计方法，即$\mathbb D_{KL}(q\Vert p)=r-1-\log r$，其中$r=\log\frac{p(x)}{q(x)}$，该估计相比$-\log r$具有更小的方差，比$\frac{1}{2}(\log r)^2$具有更小的偏差（无偏）。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DeepSeek-V3技术报告解读</title>
      <link>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</link>
      <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</guid>
      <description>&lt;h3 id=&#34;1-摘要&#34;&gt;1. 摘要&lt;/h3&gt;
&lt;p&gt;DeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-V3&#34; class=&#34;entityLink&#34;&gt;DeepSeek-V3&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-deepseek-v3模型结构&#34;&gt;2. DeepSeek-V3模型结构&lt;/h3&gt;
&lt;h4 id=&#34;21-basic-architecture&#34;&gt;2.1 Basic Architecture&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://tqzhong.github.io/my-blog/my-blog/images/2025-01-29-deepseek-v3/2025-01-29-image2.jpg&#34; alt=&#34;deepseek-v3-architecture&#34;  /&gt;
&lt;/p&gt;
&lt;div align=&#39;center&#39; style=&#34;color: #999999&#34;&gt;图1: DeepSeek-V3基础结构图&lt;/div&gt;
&lt;p&gt;DeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。&lt;/p&gt;
&lt;h5 id=&#34;211-multi-head-latent-attention&#34;&gt;2.1.1 Multi-Head Latent Attention&lt;/h5&gt;
&lt;p&gt;定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\bold{h}_t\in\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\textcolor{blue}{\bold{c}_t^{KV}}=W^{DKV}\bold{h}_t,\\
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\left[\mathbf{k}_{t,1}; \mathbf{k}^C_{t,2}; \dots; \mathbf{k}^C_{t,n_h} \right] = \mathbf{k}^C_t = W^{UK} \mathbf{c}^{KV}_t,
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\textcolor{blue}{\mathbf{k}^R_t} = \mathrm{RoPE}(W^{KR} \mathbf{h}_t),
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\mathbf{k}_{t,i} = \left[\mathbf{k}^C_{t,i}; \mathbf{k}^R_t \right],
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\left[\mathbf{v}^C_{t,1}; \mathbf{v}^C_{t,2}; \dots; \mathbf{v}^C_{t,n_h} \right] = \mathbf{v}^C_t = W^{UV} \mathbf{c}^{KV}_t.
$$
&lt;/div&gt;
&lt;p&gt;其中$\bold{c}_t^{KV}\in\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\in\mathbb R^{d_c\times d}$为下投影矩阵，$W^{UK},W^{UV}\in\mathbb R^{d_hn_h\times d_c}$为key和value的上投影矩阵。$W^{KR}\in\mathbb R^{d_h^R\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\textcolor{blue}{\bold{c}_t^{KV}}$和$\textcolor{blue}{\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DeepSeek-R1技术报告解读</title>
      <link>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</link>
      <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</guid>
      <description>&lt;h3 id=&#34;1-摘要&#34;&gt;1. 摘要&lt;/h3&gt;
&lt;p&gt;本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。&lt;/p&gt;
&lt;h3 id=&#34;2-主要贡献&#34;&gt;2. 主要贡献&lt;/h3&gt;
&lt;p&gt;新的后训练范式：在Base Model上直接使用Large-Scale RL&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了&lt;strong&gt;自我验证，反思，生成长的CoT&lt;/strong&gt;的能力。&lt;/li&gt;
&lt;li&gt;团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;蒸馏：小模型也可以很强大&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-方法&#34;&gt;3. 方法&lt;/h3&gt;
&lt;h4 id=&#34;31-deepseek-r1-zero-reinforcement-learning-on-the-base-model&#34;&gt;3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model&lt;/h4&gt;
&lt;p&gt;DeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。&lt;/p&gt;
&lt;h5 id=&#34;311-reinforcement-learning-algorithm&#34;&gt;3.1.1 Reinforcement Learning Algorithm&lt;/h5&gt;
&lt;p&gt;团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\pi_{\theta_{old}}$采样一组输出${o_1,o_2,\cdots,o_G}$，然后使用如下优化目标优化策略模型$\pi_\theta$：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &amp;= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&amp;=\frac{1}{G} \sum_{i=1}^G \left( 
\min \left( 
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i, 
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i 
\right) 
- \beta D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}})
\right),
\end{align*}
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) = 
\frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} 
- \log \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1,
$$
&lt;/div&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
&lt;/div&gt;
&lt;p&gt;其中$\epsilon$和$\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\cdots,r_G}$计算得到。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>RAG路线</title>
      <link>https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/</link>
      <pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2025-01-08-retrieval-augmented-generation/</guid>
      <description>&lt;h3 id=&#34;retrieval-augmented-generation-for-large-language-models-a-survey&#34;&gt;Retrieval-Augmented Generation for Large Language Models: A Survey&lt;/h3&gt;
&lt;h4 id=&#34;1-overview-of-rag&#34;&gt;1. Overview of RAG&lt;/h4&gt;
&lt;p&gt;典型的RAG模型如图1所示
&lt;img loading=&#34;lazy&#34; src=&#34;https://tqzhong.github.io/my-blog/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image1.png&#34; alt=&#34;typical rag model&#34;  /&gt;
&lt;/p&gt;
&lt;div align=&#39;center&#39; style=&#34;color: #999999&#34;&gt;图1: 经典RAG模型&lt;/div&gt;
&lt;h5 id=&#34;11-naive-rag&#34;&gt;1.1 Naive RAG&lt;/h5&gt;
&lt;p&gt;Naive RAG为传统的RAG方法，主要流程包括：索引，检索，生成。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引（Indexing）：将文档（PDF，HTML，Word，Markdown）切分成chunks，每个chunk为一段文本，使用一个词向量模型将每个chunk编码成向量表征存储在向量数据库中。这一步是为了高校地搜索查找待检索片段。&lt;/li&gt;
&lt;li&gt;检索（Retrieval）：基于用户的一条query，RAG系统使用相同的编码模型将query编码成对应向量表征，用query向量表征与向量数据库中的所有向量计算相似度，选择相似度最高的K个chunks，这些chunks将被用于扩充query的prompt。&lt;/li&gt;
&lt;li&gt;生成（Generation）：用户的query和被选择的chunks被整合成连贯的prompt输入给LLM，LLM基于扩充后的prompt生成结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;12-advanced-rag&#34;&gt;1.2 Advanced RAG&lt;/h5&gt;
&lt;p&gt;Advanced RAG引入一些改进来解决Navie RAG存在的一些问题，主要聚焦在提升检索质量，一般使用pre-retrieval和post-retrieval两种策略。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pre-retrieval：在这个阶段，主要目标是优化索引结构以及初始query。
&lt;ul&gt;
&lt;li&gt;优化索引：常用的策略有增强数据细粒度，优化索引结构，添加元数据，对齐优化，混合检索。&lt;/li&gt;
&lt;li&gt;优化初始query：常用的策略有query transformation，query expansion等。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;post-retrieval：当相关内容已经被检索后，将其与初始query有效结合是至关重要的一步。post-retrieval过程中主要的方法包括：chunks重排，chunks内容压缩。
&lt;ul&gt;
&lt;li&gt;chunks重排：调整被检索到的内容（chunks）在最终prompt中的位置，让更相关的chunks排在prompt的边缘（非中间，中间更容易被llm忽略），这个策略在LlamaIndex，LangChain，HayStack中均有使用。&lt;/li&gt;
&lt;li&gt;chunks内容压缩：将所有检索到的内容（chunks）全部输入llm容易导致信息过载（因为会包含很多无关或者冗余的信息），对此，chunks内容压缩主要聚焦在选择重要信息，缩短检索内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;13-modular-rag&#34;&gt;1.3 Modular RAG&lt;/h5&gt;
&lt;p&gt;模块化RAG相比前两种范式提供更好的适应性和多功能性。其往往结合不同的策略来优化其组成部分，比如：添加一个搜索模块，通过微调精进检索器等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入新模块：搜索模块（Search Module）可以用于搜索外部资源（搜索引擎、数据库、知识图谱），使用LLM生成的搜索指令和查询语句处理；RAG-Fusion用于处理传统搜索的限制问题，使用multi-query策略将用户query从不同角度扩充；记忆模块（Memory Module）用于提升LLM的记忆来指导检索；路由模块（Routing）等等&lt;/li&gt;
&lt;li&gt;引入新模式：Rewrite-Retrieve-Read模型通过引入rewriting module和一个语言模型反馈机制来更新rewriting model，提升性能；Generate-Read，Recite-Read等等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://tqzhong.github.io/my-blog/my-blog/images/2025-01-08-retrieval-augmented-generation/2025-01-08-image2.png&#34; alt=&#34;three rag types&#34;  /&gt;
&lt;/p&gt;
&lt;div align=&#39;center&#39; style=&#34;color: #999999&#34;&gt;图2: 三类不同RAG模型流程示意图&lt;/div&gt;
&lt;h4 id=&#34;2-retrieval-part&#34;&gt;2. Retrieval Part&lt;/h4&gt;
&lt;h5 id=&#34;21-检索资源&#34;&gt;2.1 检索资源&lt;/h5&gt;
&lt;p&gt;从检索内容的数据上来看包含以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无结构化数据：文本，语料库，例如Wikipedia Dump，HotpotQA，DRP；多语种文本，特别领域文本等&lt;/li&gt;
&lt;li&gt;半结构化数据：PDF，这种数据包含文本和表格，对于RAG系统而言处理起来更具挑战，一般会用到LLM生成Text-2-SQL指令查询表格中的数据，工作如TableGPT等。&lt;/li&gt;
&lt;li&gt;结构化数据：知识图谱，工作如KnowledGPT，G-Retriever等。&lt;/li&gt;
&lt;li&gt;LLMs生成内容&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从检索的粒度来看，包含以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于文本，检索粒度涵盖：Token，短语，句子，Chunks，文章&lt;/li&gt;
&lt;li&gt;对于知识图谱，检索粒度包含：实体，三元组，子图&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;22-索引的优化&#34;&gt;2.2 索引的优化&lt;/h5&gt;
&lt;p&gt;在索引这一环节，文章将被处理，分割并转变成向量表征被存储在向量数据库中。索引结构的质量决定着在检索过程中能否获取正确的内容。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>强化学习笔记</title>
      <link>https://tqzhong.github.io/my-blog/posts/2024-11-21-reinforcement-learning/</link>
      <pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2024-11-21-reinforcement-learning/</guid>
      <description>&lt;h3 id=&#34;1-基本概念公式&#34;&gt;1. 基本概念，公式&lt;/h3&gt;
&lt;p&gt;策略$\pi$，状态$s\in\mathcal S$，动作$a\in\mathcal A$，奖励$r\in\mathcal R$&lt;/p&gt;
&lt;p&gt;转移函数$P$给出当采取行动$a$从状态$s$转移到$s^\prime$，同时获得奖励$r$的概率&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$P(s^\prime,r\vert s,a)=\mathbb P[S_{t+1}=s^\prime,R_{t+1}=r\vert S_t=s,A_t=a]$$
&lt;/div&gt;
&lt;p&gt;状态转移函数$P^a_{ss^\prime}$&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$P^a_{ss^\prime}=P(s^\prime\vert s,a)=\mathbb P[S_{t+1}=s^\prime|S_t=s,A_t=a]=\sum_{r\in\mathcal R}P(s^\prime,r\vert s,a)$$
&lt;/div&gt;
&lt;p&gt;奖励函数$R$预测给定状态和动作后的下一个奖励值&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$R(s,a)=\mathbb E[R_{t+1}\vert S_t=s,A_t=a]=\sum_{r\in\mathcal R}r\sum_{s^\prime\in\mathcal S}P(s^\prime,r\vert s,a)$$
&lt;/div&gt;
&lt;!-- $$R(s)=\mathbb E[R_{t+1}\vert S_t=s]$$ --&gt;
&lt;p&gt;策略$\pi$给出在状态$s$下会采取何种行动，分为两种&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;确定性：$\pi(s)=a$&lt;/li&gt;
&lt;li&gt;随机性：$\pi(a\vert s)=\mathbb P_\pi[A=a\vert S=s]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;回报$G_t$，即未来的奖励之和，其中$\gamma\in[0,1]$为惩罚因子&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$G_t=R_{t+1}+\gamma R_{t+2}+\dots=\sum_{k=0}^\infty \gamma^k R_{t+k+1}$$
&lt;/div&gt;
&lt;p&gt;状态价值函数$V_\pi(s)$给出在状态$s$下的期望回报&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$V_\pi(s)=\mathbb E_\pi[G_t\vert S_t=s]$$
&lt;/div&gt;
&lt;p&gt;动作价值函数$Q_\pi(s,a)$给出在状态$s$下采取动作$a$的期望回报&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$Q_\pi(s,a)=\mathbb E_\pi[G_t\vert S_t=s, A_t=a]$$
&lt;/div&gt;
&lt;p&gt;状态价值和动作价值的关系&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$V_\pi(s)=\sum_{a\in\mathcal A}Q_\pi(s,a)\pi(a|s)=\mathbb E_{a\sim\pi}Q_\pi(s,a)$$
&lt;/div&gt;
优势函数$A_\pi(s,a)$定义为动作价值与状态价值的差
&lt;div class=&#34;scroll-container&#34;&gt;
$$A_\pi(s,a)=Q_\pi(s,a)-V_\pi(s)$$
&lt;/div&gt;
最优价值函数定义为在最优策略下的价值函数，即能够产生最大回报
&lt;div class=&#34;scroll-container&#34;&gt;
$$V_*(s)=\max_\pi V_\pi(s)\\
Q_*(s,a)=\max_\pi Q_\pi(s,a)$$
&lt;/div&gt;
&lt;p&gt;最优策略定义为实现最优价值的策略，即对任意状态$s$都有$V_\pi(s)\ge V_{\pi^\prime}(s)$，最优策略可能有多个，都将其表示为$\pi_*(s)$&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$\pi_*=\arg\max_\pi V_\pi(s)\\
\pi_*=\arg\max_\pi Q_\pi(s,a)$$
&lt;/div&gt;
&lt;p&gt;因此，以下关系是成立的&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$V_{\pi_*}(s)=V_*(s)\\
Q_{\pi_*}(s,a)=Q_*(s,a)$$
&lt;/div&gt;
&lt;h3 id=&#34;2-马尔可夫过程mdps&#34;&gt;2. 马尔可夫过程（MDPs）&lt;/h3&gt;
&lt;p&gt;几乎所有RL问题都可以划在马尔可夫过程内，马尔可夫过程内的所有状态都有同一个特性，即未来的状态只取决于当下的状态，与历史状态无关
$$\mathbb P[S_{t+1}\vert S_t]=\mathbb P[S_{t+1}\vert S_1,\dots S_t]$$
一个马尔可夫决策过程包含五个元素$\mathcal M=\langle \mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma\rangle$，对应的符号与基本符号含义相同&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Deepspeed多机多卡训练&amp;代码细节</title>
      <link>https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/2024-10-30-deepspeed/</guid>
      <description>&lt;p&gt;本次使用的是多台8卡1080Ti服务器进行DeepSpeed多机多卡实验。&lt;/p&gt;
&lt;h3 id=&#34;supervised-finetuning&#34;&gt;Supervised finetuning&lt;/h3&gt;
&lt;p&gt;首先在主节点克隆&lt;a href=&#34;https://github.com/microsoft/DeepSpeedExamples&#34; class=&#34;entityLink&#34;&gt;deepspeed-chat&lt;/a&gt;仓库。&lt;/p&gt;
&lt;p&gt;使用的主要环境：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;1.13.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install datasets
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install sentencepiece
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;protobuf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;3.20.3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install accelerate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;deepspeed&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;0.10.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;transformers&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;4.44.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install tensorboard
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install &lt;span class=&#34;nv&#34;&gt;numpy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;1.26.4
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;deepspeed安装需要有nvcc，开始这些1080Ti服务器没有nvcc，所以先装了这个：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt update
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt install nvidia-cuda-toolkit
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;之后先跑通单节点，我用的是&lt;code&gt;step1_supervised_finetuning/training_scripts/opt/single_node/run_1.3b.sh&lt;/code&gt;，因为当时考虑1080Ti显存较小，不过后来发现原仓库里的bash脚本都差不多，就是改了模型路径。&lt;/p&gt;
&lt;p&gt;跑通单节点也花了不少时间，最开始是模型和数据集的问题，因为服务器本地连接不到hf，所以下载了opt-1.3b模型到主节点，数据集部分也无法访问hf，是从hf上下载了&lt;code&gt;synthetic-instruct-gptj-pairwise&lt;/code&gt;数据集，两个文件保存在主节点：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;datasets
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    └── synthetic-instruct-gptj-pairwise
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        ├── dataset_infos.json
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        └── train-00000-of-00001-1e5d57b93c448e7a.parquet
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;在&lt;code&gt;dschat/utils/data/raw_datasets.py&lt;/code&gt;的数据集类&lt;code&gt;PromptRawDataset&lt;/code&gt;上也做了对应修改:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;PromptRawDataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;object&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;local_rank&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dataset_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_path&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;local_rank&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;local_rank&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&amp;#39;原始数据的读取，这里根据自己数据集作相应修改&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_datasets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_dataset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;parquet&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_files&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dataset_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;到这里，数据集模型以及环境都差不多了，在单节点上启动训练脚本，发现optimizer有报错，原因是原训练主函数使用的是&lt;code&gt;FusedAdam&lt;/code&gt;，可能是g++环境匹配存在问题，这个最终没解决就没管了，直接把optimizer换成&lt;code&gt;AdamW&lt;/code&gt;就跑通了。查了一下&lt;code&gt;FusedAdam&lt;/code&gt;在需要大量计算资源的场景下有一定优势。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>大模型post-training方法</title>
      <link>https://tqzhong.github.io/my-blog/posts/llm-post-training/</link>
      <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://tqzhong.github.io/my-blog/posts/llm-post-training/</guid>
      <description>&lt;!--tips:--&gt;
&lt;!--公式块里，如果加了class=scroll-container(滚轮滑块防止单行公式太长)，大小于号注意要与后面的字符隔开一个空格，否则无法正常编译--&gt;
&lt;!--常规公式块里，换行要用\\\\而不是\\否则无法正常编译，但是在{cases}环境里，换行用\\即可（目前发现这个，后续有其他再补充）--&gt;
&lt;h3 id=&#34;1-dpo&#34;&gt;1. DPO&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html&#34; class=&#34;entityLink&#34;&gt;Rafailov et al. (2023)&lt;/a&gt;基于RLHF中PPO的优化式推导出最优奖励函数表达式：$r(x, y)=\beta log\frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}+\beta logZ(x)$，将该奖励函数表达式带入BT-model得到DPO的损失函数表达式：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\mathcal L_{DPO}(\pi_\theta;\pi_{ref})=-\mathbb E_{(x, y_w, y_l)\sim\mathcal D}[log\ \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)}-\beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]
$$
&lt;/div&gt;
&lt;h3 id=&#34;2-simple-dpo&#34;&gt;2. Simple-DPO&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.14734&#34; class=&#34;entityLink&#34;&gt;Meng et al. (2024)&lt;/a&gt;考虑到DPO的奖励函数有以下两个缺点：1）训练DPO时需要一个额外的reference 模型，增大训练开销；2）DPO的优化式和inference阶段存在差异。具体来讲，inference阶段时需要优化最大平均对数似然：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
p_\theta(y|x)=\frac{1}{|y|}log\ \pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i})
$$
&lt;/div&gt;
&lt;p&gt;因此Simple-DPO考虑将奖励函数表达式改为：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
r_{SimPO}(x, y)=\frac{\beta}{|y|}log\ \pi_\theta(y|x)=\frac{\beta}{|y|}\sum_{i=1}^{|y|}log\ \pi_\theta(y_i|x,y_{&lt; i})
$$
&lt;/div&gt;
&lt;p&gt;此外，为了进一步强化模型对winning response的拟合，弱化对losing response的拟合，作者在BT公式中引入超参数$\gamma \ (\gamma&amp;gt;0)$，表达式如下：&lt;/p&gt;
&lt;div class=&#34;scroll-container&#34;&gt;
$$
p(y_w&gt;y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma)
$$
&lt;/div&gt;
从而，Simple-DPO的优化函数：
&lt;div class=&#34;scroll-container&#34;&gt;
$$
\mathcal L_{SimPO}(\pi_\theta)=-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}[log\ \sigma(\frac{\beta}{|y_w|}log\ \pi_\theta(y_w|x)-\frac{\beta}{|y_l|}log\ \pi_\theta(y_l|x)-\gamma)]
$$
&lt;/div&gt;
&lt;h3 id=&#34;3-kto&#34;&gt;3. KTO&lt;/h3&gt;
&lt;p&gt;KTO loss (&lt;a href=&#34;https://arxiv.org/abs/2402.01306&#34; class=&#34;entityLink&#34;&gt;Ethayarajh et al. (2024)&lt;/a&gt;)与DPO相比，不需要为每个prompt配对提供偏好回答和拒绝回答。它仅需要一个答案，并给出这个答案一个标签来指示该答案的质量是正面还是负面的。KTO不需要偏好回答的数量与拒绝回答的数量相同，简化数据的准备流程。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
