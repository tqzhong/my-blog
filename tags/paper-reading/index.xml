<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Paper Reading on Rs' Log</title><link>https://tqzhong.github.io/my-blog/tags/paper-reading/</link><description>Recent content in Paper Reading on Rs' Log</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 29 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://tqzhong.github.io/my-blog/tags/paper-reading/index.xml" rel="self" type="application/rss+xml"/><item><title>DeepSeek-V3技术报告解读</title><link>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-01-29-deepseek-v3/</guid><description>&lt;h3 id="1-摘要">1. 摘要&lt;/h3>
&lt;p>DeepSeek-V3，是一个Mixture-of-Experts（MoE）结构的大语言模型，参数量671B，其中每个token激活的参数量为37B。DeepSeek-V3主要采用Multi-head Latent Attention（MLA）和DeepSeekMoE结构，此外为了expert负载均衡引入了auxiliary-loss-free策略，为了更强的模型性能采用了multi-token prediction（MTP）训练策略。DeepSeek-V3预训练预料一共14.8T个token，并采用SFT和RL进一步对齐增强模型性能。DeepSeek-V3完整的训练一共仅需要2.788M H800 GPU hours。项目链接：&lt;a href="https://github.com/deepseek-ai/DeepSeek-V3" class="entityLink">DeepSeek-V3&lt;/a>&lt;/p>
&lt;h3 id="2-deepseek-v3模型结构">2. DeepSeek-V3模型结构&lt;/h3>
&lt;h4 id="21-basic-architecture">2.1 Basic Architecture&lt;/h4>
&lt;p>&lt;img loading="lazy" src="https://tqzhong.github.io/my-blog/my-blog/images/2025-01-29-deepseek-v3/2025-01-29-image2.jpg" alt="deepseek-v3-architecture" />
&lt;/p>
&lt;div align='center' style="color: #999999">图1: DeepSeek-V3基础结构图&lt;/div>
&lt;p>DeepSeek-V3基本结构基于Transformer模型，为了高效推理并降低训练成本，DeepSeek-V3采用了DeepSeek-V2中的MLA和DeepSeekMoE结构。并给予DeepSeek-V2，团队添加了一个auxiliary-loss-free的专家负载均衡策略。图1为MLA和DeepSeekMoE的结构示意图。&lt;/p>
&lt;h5 id="211-multi-head-latent-attention">2.1.1 Multi-Head Latent Attention&lt;/h5>
&lt;p>定义$d$为词嵌入向量维度，$n_h$为注意力头数目，$d_h$为每个注意力头的维度，$\bold{h}_t\in\mathbb R^d$表示给定注意力层的第$t$个token的注意力输入向量。MLA的关键在于在推理阶段使用low-rank joint compression技术来减少KV-Cache所占用的存储量：&lt;/p>
&lt;div class="scroll-container">
$$
\textcolor{blue}{\bold{c}_t^{KV}}=W^{DKV}\bold{h}_t,\\
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\left[\mathbf{k}_{t,1}; \mathbf{k}^C_{t,2}; \dots; \mathbf{k}^C_{t,n_h} \right] = \mathbf{k}^C_t = W^{UK} \mathbf{c}^{KV}_t,
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\textcolor{blue}{\mathbf{k}^R_t} = \mathrm{RoPE}(W^{KR} \mathbf{h}_t),
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\mathbf{k}_{t,i} = \left[\mathbf{k}^C_{t,i}; \mathbf{k}^R_t \right],
$$
&lt;/div>
&lt;div class="scroll-container">
$$
\left[\mathbf{v}^C_{t,1}; \mathbf{v}^C_{t,2}; \dots; \mathbf{v}^C_{t,n_h} \right] = \mathbf{v}^C_t = W^{UV} \mathbf{c}^{KV}_t.
$$
&lt;/div>
&lt;p>其中$\bold{c}_t^{KV}\in\mathbb R^{d_c}$代表key和value压缩后的隐藏向量；$d_c(\ll d_n n_h)$表明key和value的压缩维度，$W^{DKV}\in\mathbb R^{d_c\times d}$为下投影矩阵，$W^{UK},W^{UV}\in\mathbb R^{d_hn_h\times d_c}$为key和value的上投影矩阵。$W^{KR}\in\mathbb R^{d_h^R\times d}$用于生成carry RoPE key向量的矩阵。在MLA中，只有标蓝的向量（$\textcolor{blue}{\bold{c}_t^{KV}}$和$\textcolor{blue}{\bold{k}_t^R}$）需要在推理阶段存储（相比Multi-Head Attention的KV-Cache开销小很多）。&lt;/p></description></item><item><title>DeepSeek-R1技术报告解读</title><link>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://tqzhong.github.io/my-blog/posts/2025-01-27-deepseek-r1/</guid><description>&lt;h3 id="1-摘要">1. 摘要&lt;/h3>
&lt;p>本次更新开源了DeepSeek-R1-Zero和DeepSeek-R1两个新旗舰reasoning模型，主要使用large-scale reinforcement learning且不需要SFT即完成训练，为开源社区给出了一个完全新颖且行之有效的reasoning LLM训练方案。其中DeepSeek-R1在reasoning任务上和OpenAI-o1-1217性能相当。除此之外，团队还开源了不同size的稠密模型（1.5B,7B,8B,14B,32B,70B），这些小模型是基于Qwen和Llama开源模型通过蒸馏DeepSeek-R1得到。&lt;/p>
&lt;h3 id="2-主要贡献">2. 主要贡献&lt;/h3>
&lt;p>新的后训练范式：在Base Model上直接使用Large-Scale RL&lt;/p>
&lt;ul>
&lt;li>不使用SFT而直接基于base model做RL，让模型能够探索CoT来解决复杂问题。其中得到的DeepSeek-R1-Zero模型展现出了&lt;strong>自我验证，反思，生成长的CoT&lt;/strong>的能力。&lt;/li>
&lt;li>团队给出了DeepSeek-R1的详细训练pipeline，该pipeline包含两段RL阶段，分别用于提升reasoning能力和用于提升通用能力；以及包含两段SFT阶段，分别为模型获取reasoning和non-reasoning能力提供冷启动。&lt;/li>
&lt;/ul>
&lt;p>蒸馏：小模型也可以很强大&lt;/p>
&lt;ul>
&lt;li>开源了多个size的基于Qwen2.5和Llama3系列模型使用DeepSeek-R1蒸馏得到的小模型，并且均在reasoning任务上展现了比同size最强开源模型更强的能力。在AIME2024、MATH-500、LiveCodeBench等基准上取得很高成绩。&lt;/li>
&lt;/ul>
&lt;h3 id="3-方法">3. 方法&lt;/h3>
&lt;h4 id="31-deepseek-r1-zero-reinforcement-learning-on-the-base-model">3.1 DeepSeek-R1-Zero: Reinforcement Learning on the Base Model&lt;/h4>
&lt;p>DeepSeek-R1-Zero模型不实用任何有监督数据，不做SFT，仅使用纯粹的强化学习过程让模型自我进化。&lt;/p>
&lt;h5 id="311-reinforcement-learning-algorithm">3.1.1 Reinforcement Learning Algorithm&lt;/h5>
&lt;p>团队采用Croup Relative Policy Optimization（GRPO）强化学习算法。使critic model和policy model具有相同模型大小，具体来说，对每个问题$q$，GRPO从旧策略$\pi_{\theta_{old}}$采样一组输出${o_1,o_2,\cdots,o_G}$，然后使用如下优化目标优化策略模型$\pi_\theta$：&lt;/p>
&lt;div class="scroll-container">
$$
\begin{align*}
\mathcal{J}_{\text{GRPO}}(\theta) &amp;= \mathbb{E}\left[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O|q)\right]\\
&amp;=\frac{1}{G} \sum_{i=1}^G \left(
\min \left(
\frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)} A_i,
\text{clip}\left( \frac{\pi_{\theta}(o_i | q)}{\pi_{\theta_{\text{old}}}(o_i | q)}, 1 - \epsilon, 1 + \epsilon \right) A_i
\right)
- \beta D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}})
\right),
\end{align*}
$$
&lt;/div>
&lt;div class="scroll-container">
$$
D_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) =
\frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)}
- \log \frac{\pi_{\text{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1,
$$
&lt;/div>
&lt;div class="scroll-container">
$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}.
$$
&lt;/div>
&lt;p>其中$\epsilon$和$\beta$为超参数，$A_i$为advantage，使用每个输出对应的奖励${r_1,r_2,\cdots,r_G}$计算得到。&lt;/p></description></item></channel></rss>